{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper reproduction: Bringing At-home Pediatric Sleep Apnea Tesing Closer to Reality: A Multi-Modal Transformer Approach\n",
    "\n",
    "```\n",
    "Aaron Schlesinger and Dave Pankros\n",
    "{aschle2, pankros2}@illinois.edu\n",
    "Team ID: 3\n",
    "```\n",
    "\n",
    "This notebook is a draft submission of our paper reproduction for CS 598 - Deep Learning for healthcare. In it, we aim to reproduce the findings in the paper _Bringing At-home Pediatric Sleep Apnea Testing Closer to Reality: A Multi-Modal Transformer Approach_ $^1$.\n",
    "\n",
    "The code herein is functional, with some limitations around data, discussed below. The public GitHub repository in which we do our work and from which we derived this notebook is located at [github.com/arschles/UIUC-CS598-DLH-Project](https://github.com/arschles/UIUC-CS598-DLH-Project). The code therein is organized as a standard Python project, and the repository contains comprehensive instructions on running it.\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "Our work to get to this point has been extensive and varied. In the subsequent sections, you will see extensive prose and code detailing the work we've done, but below is a high-level summary, in bulleted form:\n",
    "\n",
    "- We have acquired a subset of the data used in the paper\n",
    "- We have completed data preprocessing and data loading\n",
    "- We have ensured the model trains sufficiently\n",
    "- We have done standard evaluations of our trained model\n",
    "\n",
    "From here, we plan to acquire as much additional data as we can (this is not under our control, as described below), clean up the code, perform ablation studies, and generate graphs and other visuals as necessary to support and illustrate our evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# License\n",
    "The data used by this dataset is subject to a license.  While we are not sharing the original data in its raw form, we are sharing derivations of that data like pre-trained model checkpoints. To facilitate the goals of this assignment and class and to honor the spirit of the [license](https://physionet.org/content/nch-sleep/view-license/3.1.0/), we require that by opening and running this notebook you agree that you:\n",
    "\n",
    "1. Will not attempt to reverse engineer the data into a form other than provided,\n",
    "2. Will not attempt to identify or de-anonymize any individual or institution in the dataset,\n",
    "3. Will not share or re-use the data in any form,\n",
    "4. Will not use the data for any purpose other than this assignment, and\n",
    "5. Maintain a up-to-date cerification in human research subject protection and HIPAA regulations.\n",
    "\n",
    "The data license is discussed in more detail [below](#data-licensing-note)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "Much effort went into making the [original paper repository](https://github.com/healthylaife/Pediatric-Apnea-Detection) runnable.  The repository was lacking any documentation including even python version and package versions.  It was built exclusively to be run on Windows and lacked any attempt at cross-platform support.  We ran into several cases where the code, as supplied, could never have been run in the provided state.  In one instance, for example, the arguments to a function were illegal and, after reaching out to the author and receiving no reply, we used our best judgment at a solution.\n",
    "\n",
    "With that in mind, we are attempting to reproduce results consistent with the original paper, but there may be differences due to these changes or other instances of errors or omissions that did not cause a code failure and that may have been too subtle to be caught in our initial passes over the code.  These differences will, inevitably, cause deviations between our results and the results presented in the paper.\n",
    "\n",
    "We have, however, undertaken in [our fork of the original code](https://github.com/arschles/UIUC-CS598-DLH-Project/tree/main/original) to provide more information to aid reproducibility (including a `requirements.txt` file and other detailed dependency information), made the code cross-platform where it was not, and provide for information about how to preprocess and run the code using standard tools like bash and make.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Sleep apnea in children is a major health problem that affects between one to five percent of US children, but differs from sleep apnea in adults in its clinical causes and characteristics.  Thus, previously-created methods for detecting adult sleep apnea may be ineffective at detecting pediatric sleep apnea.\n",
    "\n",
    "## Background\n",
    "While there are numerous testing tools and algorithmic methods for detecting adult sleep apnea, the same tools and methods are unavailable for pediatric sleep apnea (PSA) due to these differences. Detecting pediatric sleep apnea more quickly and easily can lead to earlier clinical intervention in the child's care and ultimately prevent the wide variety of health issues commonly caused by Obstructive Sleep Apnea (OSA).  Polysomnography (PSG) is the standard method for formal sleep apnea diagnosis, but is generally performed in a dedicated facility where a patient can be monitored overnight. Polysomnography involves collecting various continuous-time signals, including electroencephalogram (EEG), electrooculogram (EOG), electrocardiogram (ECG), pulse oximetry (SpO2), end-tidal carbon dioxide (ETCO2), respiratory inductance plethysmography (RIP), nasal airflow, and oral airflow.  While effective, PSG is, however, complex, costly and requires a dedicated sleep lab.  \n",
    "\n",
    "### State of the Art\n",
    "Current methods target adults and, for reasons stated earlier, are ineffective at diagnosing PSA in childen. Very little work has been done in the scope of pediatric sleep apnea. In general, full Polysomnography data is hard to find and thus, much research has focused on determining the Apnea-Hypopnea Index (AHI) from ECG and SpO2 signals.   \n",
    "\n",
    "While transformers are used commonly in general deep learning models, they are much less prevalent in the detection of sleep apnea.  Two studies described in this paper used transformers to determine sleep stages (one in adults, one in children), while another used a hybrid CNN/transformer model of obstructive sleep apnea (OSA) detection.  \n",
    "\n",
    "## Paper\n",
    "The paper proposes to study the gaps in Obstructive Sleep Apnea Hypopnea Syndrome (OSAHS) in children vis-a-vis adults. The paper then suggests a custom transformer-based method and data representation for PSA detection, and identifies the polysomography modalities that most closely correlate to OSAHS in children.\n",
    "\n",
    "The results presented in the paper portray state-of-the-art results. \n",
    "![Figure 2](./images/figure_2.png)\n",
    "\n",
    "If diagnosing pediatric sleep apnea can be done with low-cost, consumer hardware, then the costs of pediatric sleep apena diagnoses decrease and childrens' health improves.  Additionally, lowering the cost of dianoses would also enable access for underserved, including rural, populations with limited access to sleep labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility\n",
    "We will generally investigate whether we can beat state-of-the-art results from Polysomnography (PSG) studies in pediatric sleep apnea detection using the transformer-based model proposed in the paper and discussed above. More specifically, we will focus on testing the following hypotheses:\n",
    "\n",
    "1. Whether the proposed model can achieve results from signals more easily collected than PSG. As in the paper, we will focus on ECG and $SpO_2$ signals, and\n",
    "2. Whether the results support this method being effective as a dedicated method for in-lab sleep studies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "In this section, we detail how we acquire, process and load the relevant data, and how we train and evaluate the model given those data.  We will exlain the procedure and code in this notebook with the understanding that we did not process the data, train the model, or evaluate the model using this notebook.  True reproduction of our environment is explained in the Repository Setup and Evaluation section, immediately following.\n",
    "\n",
    "## Environment\n",
    "The original project lacked any definition of python and dependency version requirements.  We documented the complete list of python packages and versions necessary to run our repository in our [environment.yaml file](https://raw.githubusercontent.com/arschles/UIUC-CS598-DLH-Project/main/environment.yml).  For convenience, the primary requirements we chose were:\n",
    "- python 3.11\n",
    "- tensorflow 2.15\n",
    "- keras 2.16\n",
    "- numpy 1.26\n",
    "- pandas 2.2\n",
    "- scipy 1.12\n",
    "\n",
    "# Repository Setup\n",
    "\n",
    "We originally started with the [original research code](https://github.com/healthylaife/Pediatric-Apnea-Detection), copied it into this project's [repository](https://github.com/arschles/UIUC-CS598-DLH-Project), and began making changes. As detailed elsewhere, much work was done to get the code to a runnable state, then more was done to make it performant, able to load models from checkpoints, and so on.\n",
    "\n",
    "When we got to a good state in the repository, we moved code into this notebook and modified it as necessary to work in this environment. Some of those modifications include the following:\n",
    "\n",
    "- Intra-project imports don't work so we had to remove them and linearize the intra-project dependency graph herein\n",
    "- Everything is in a global namespace, so we had to fix the naming conflicts that arose herein\n",
    "- Testing and training code is combined in the repository, but it made more sense to split the two pieces herein, so we had to extract large segments of code common to both operations\n",
    "\n",
    "Despite the differences between code in this notebook and code in the repository, we achieve the same result in both.\n",
    "\n",
    "If you'd like to run the code in the repository, please see the [`original/HOW_TO_RUN.md` file](https://github.com/arschles/UIUC-CS598-DLH-Project/blob/main/original/HOW_TO_RUN.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "##  Data\n",
    "\n",
    "### Data Licensing Note\n",
    "\n",
    "**Due to the license required to access the data, we cannot provide the raw dataset with this notebook because doing so would be a violation of terms 3 and 4 of the [PhysioNetCredential Health Data License 1.5.0](https://physionet.org/content/nch-sleep/view-license/3.1.0/).**\n",
    "\n",
    "Because we cannot supply original raw data, we will discuss the processing of the data and the relevant processing code with example output, but the provided code **will not actually process data** within this notebook.  Since it is not subject to the aforementioned license, we have stored preprocessed data elsewhere and will load it in relevant sections of this notebook. From that point, further processing will be performed in this notebook.\n",
    "\n",
    ">Before we proceed, we want to stress that this preprocessed data -- and the model checkpoint data we'll discuss later -- is derived from licensed data for which one must pass a training course to access. This notebook includes instructions and code to access these resources. **By proceeding past this section, you must agree to adhere to the requirements outlined in the license section above**.\n",
    "\n",
    "### Source(s)\n",
    "The original paper used Nationwide Childrenâ€™s Hospital (NCH) Sleep Data Bank (Lee et al., 2022), and Childhood Adenotonsillectomy Trial (CHAT) dataset (Marcus et al., 2013; Redline et al., 2011).  Each of these datasets are collected from actual sleep studies, anonymized and made available for research.  At the time of this writing, we have been unable to gain approval to access the CHAT dataset so are unable to use it.  We currently have access to the [NCH dataset through physionet.org](https://physionet.org/content/nch-sleep/3.1.0/).  While we have been downloading this data for over a week, download speeds are capped at around 600KB/s.  We have currently downloaded around 400GB of 2.1TB total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset summary\n",
    "Since the paper provides extensive statistics on the data used in this study, we have not undertaken to perform our own calculations. The paper-provided measures are illustrated below.\n",
    "\n",
    "### Demographics of the Datasets\n",
    "\n",
    "The dataset are summarized in this section by patient demographics, in charted and tabular format.\n",
    "\n",
    "![Data Demographics](./images/data/demographics.png)\n",
    "\n",
    "|          |                         |     NCH      |    CHAT     |\n",
    "|---------:|:------------------------|:------------:|:-----------:|\n",
    "|          | Number of Patients      |       3673   | 453         |\n",
    "|          | Number of Sleep Studies |     3984     |     453     |\n",
    "|  **Sex** |                         |              |             |\n",
    "|          | Male                    |     2068     |     219     |\n",
    "|          | Female                  |     1604     |     234     |\n",
    "| **Race** |                         |              |             |\n",
    "|          | Asian                   |      93      |      8      |\n",
    "|          | Black                   |     738      |     252     |\n",
    "|          | White                   |     2433     |     161     |\n",
    "|          | Other                   |     409      |     32      |\n",
    "|          | **Age (years/mean)**    | \\[0-30\\]/8.8 | \\[5-9\\]/6.5 |\n",
    "\n",
    "\n",
    "### Data Statistics \n",
    "\n",
    "The data are summarized in this section by the various collected signals, in both charted and tabular format.\n",
    "\n",
    "![Data Signals](./images/data/signals.png)\n",
    "\n",
    "\n",
    "\n",
    "|                Event |   NCH   | CHAT  |\n",
    "|---------------------:|:-------:|:-----:|\n",
    "|  Oxygen Desaturation | 215280  | 65006 |\n",
    "|       Oximeter Event | 161641  | 9,864 |\n",
    "|          EEG arousal | 146052  |  --   |\n",
    "|   Respiratory Events |         |       |\n",
    "|             Hypopnea |  14522  | 15871 |\n",
    "| Obstructive Hypopnea |  42179  |  --   |\n",
    "|    Obstructive apnea |  15782  | 7075  |\n",
    "|        Central apnea |  6938   | 3656  |\n",
    "|          Mixed apnea |  2650   |  --   |\n",
    "|         Sleep Stages |         |       |\n",
    "|                 Wake | 665676  | 10282 |\n",
    "|                   N1 | 128410  | 13578 |\n",
    "|                   N2 | 1383765 | 19985 |\n",
    "|                   N3 | 875486  | 9981  |\n",
    "|                  REM | 611320  | 3283  |    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "### Data Normalization: Interpolation, Resampling and Tokenization\n",
    "The raw data are categories roughly as follows:\n",
    "\n",
    "1. Regular time-series.\n",
    "1. irregular time-teries, and\n",
    "1. tabular data.\n",
    "\n",
    "Additionally, the time-series data may be provided in various frequencies.  To merge all the different data types into a coherent dataset suitable for training and testing, the following steps must be performed:\n",
    "    \n",
    "1. Each irregular time series must be interpolated to convert it into a regular time series.\n",
    "1. All the time series data must be resampled into a uniform frequency, $f_{sampling}$, for all sleep studies and modalities.\n",
    "1. The tabular data is added to the time series as a constant signal (i.e. repeated tokens)\n",
    "1. The combined data is split into $i$ equal-length tokens of time $S$, where each modality consists of $S * f_{sampling}$ data points\n",
    "    \n",
    "These data can then be split and passed to the model for training and testing.\n",
    "\n",
    "### Splitting\n",
    "\n",
    "This paper utilizes a custom stratified $k$-fold cross validation to achieve the following:\n",
    "\n",
    "1. Ensure that an equal number of patients are assigned to each fold, and\n",
    "1. normalize the number of positive samples in each fold.\n",
    "\n",
    "Pseudocode for this method is as follows:\n",
    "\n",
    "![Algorithm](./images/algorithm_1.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Apnea-Hypopnea index\n",
    "\n",
    "The [Apnea-Hypopnea index (AHI)](https://www.sleepfoundation.org/sleep-apnea/ahi) is an important quantification of the severity of sleep apnea. Its derivation for a single night of sleep is as follows:\n",
    "\n",
    "$$\n",
    "\\frac {N_{apneic} + N_{hypopneic}} {H}\n",
    "$$\n",
    "\n",
    "Where the following are defined:\n",
    "\n",
    "- $N_{apneic}$ is the number of apneic events (instances where the patient stops breathing),\n",
    "- $N_{hypopneic}$ is the number of hypopneic events (instances where the airflow is blocked and the patient's breathing becomes more shallow), and\n",
    "- $H$ is the total number of hours the patient slept that night\n",
    "\n",
    "Since it can be derived from an entire night of sleep, the AHI is a very useful measure to summarize, with relatively modest loss of information, a person's apnea/hypopnea activity in a given night of sleep.\n",
    "\n",
    "Below we show some setup code followed by the code to create a TSV (tab-separated file) file containing AHI measures for a given sleep study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and configuration\n",
    "\n",
    "First, we provide some setup and configuration code. These configuration settings will be valid for this section as well as the remainder of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# raw data is required for both the preprocessing and data loading step.\n",
    "# since raw data is licensed, we do not, in this notebook, do these steps\n",
    "# by default.\n",
    "# \n",
    "# if you have access to raw data, download it to your machine, set this \n",
    "# variable to True and set PHYSIONET_ROOT to the location on disk of your data's\n",
    "# physionet.org directory\n",
    "SHOULD_PREPROCESS = False\n",
    "PHYSIONET_ROOT=\"/root/data/physionet.org\"\n",
    "# We have preprocessed the data and hosted it at the below URL. Do not change this value.\n",
    "# \n",
    "# Data will remain present at this URL through May 31, 2024\n",
    "PREPROCESS_URL=\"https://dlhproject.sfo3.cdn.digitaloceanspaces.com/nch_30x64.npz\"\n",
    "\n",
    "# Flag to determine whether to test from a pretrained model checkpoint.\n",
    "# set to False if you want to test from the model that is trained in this notebook, \n",
    "# or True if you want to download the checkpoint and test with that.\n",
    "# \n",
    "# NOTE: the model will train regardless of the value to which you set this \n",
    "# constant. it only affects how testing is done\n",
    "# \n",
    "# If you want to skip training, set the SKIP_TRAINING flag below to True\n",
    "TEST_FROM_CHECKPOINT = True\n",
    "CHECKPOINT_URL=\"https://dlhproject.sfo3.cdn.digitaloceanspaces.com/weights-2024-14-14.zip\"\n",
    "\n",
    "# this is handy to turn off the training process. it should only be\n",
    "# set to true if TEST_FROM_CHECKPOINT is also set to True\n",
    "SKIP_TRAINING = False\n",
    "\n",
    "if SKIP_TRAINING and not TEST_FROM_CHECKPOINT:\n",
    "    print(\n",
    "        \"WARNING: SKIP_TRAINING is on and TEST_FROM_CHECKPOINT is off. \"\n",
    "        \"This configuration will result a completely untrained model, and your \"\n",
    "        \"evaluation metrics will probably be horrible.\"\n",
    "    )\n",
    "\n",
    "#####\n",
    "# do not change anything under this line\n",
    "#####\n",
    "AHI_OUT_PATH = os.path.join(PHYSIONET_ROOT, 'AHI.csv')\n",
    "PREPROCESS_OUT_PATH = os.path.join(PHYSIONET_ROOT, 'nch_30x64.npz')\n",
    "NCH_DATA_ROOT = os.path.join(PHYSIONET_ROOT, 'files', 'nch-sleep', '3.1.0')\n",
    "HEALTH_DATA_ROOT = os.path.join(NCH_DATA_ROOT, 'Health_Data')\n",
    "SLEEP_DATA_ROOT = os.path.join(NCH_DATA_ROOT, 'Sleep_Data')\n",
    "MODEL_OUT_PATH = os.path.join('original', 'weights', 'semscnn_ecgspo2', 'f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AHI Calculation\n",
    "\n",
    "Next, after setup code is complete, we compute AHI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "APNEA_EVENT_DICT = {\n",
    "    \"Obstructive Apnea\": 2,\n",
    "    \"Central Apnea\": 2,\n",
    "    \"Mixed Apnea\": 2,\n",
    "    \"apnea\": 2,\n",
    "    \"obstructive apnea\": 2,\n",
    "    \"central apnea\": 2,\n",
    "    \"apnea\": 2,\n",
    "    \"Apnea\": 2,\n",
    "}\n",
    "\n",
    "HYPOPNEA_EVENT_DICT = {\n",
    "    \"Obstructive Hypopnea\": 1,\n",
    "    \"Hypopnea\": 1,\n",
    "    \"hypopnea\": 1,\n",
    "    \"Mixed Hypopnea\": 1,\n",
    "    \"Central Hypopnea\": 1,\n",
    "}\n",
    "\n",
    "def _num_sleep_hours(\n",
    "    sleep_study_metadata: pd.DataFrame,\n",
    "    pat_id: int,\n",
    "    study_id: int,\n",
    ") -> int:\n",
    "    ssm = sleep_study_metadata\n",
    "    sleep_duration_df = ssm.loc[\n",
    "        (ssm[\"STUDY_PAT_ID\"] == pat_id) &\n",
    "        (ssm[\"SLEEP_STUDY_ID\"] == study_id)\n",
    "    ]\n",
    "    assert len(sleep_duration_df) == 1, (\n",
    "        f'expected just 1 study with patient {pat_id} and study '\n",
    "        f'{study_id}, but got {len(sleep_duration_df)} instead'\n",
    "    )\n",
    "    sleep_duration_datetime = datetime.strptime(\n",
    "        str(\n",
    "            sleep_duration_df[\n",
    "                \"SLEEP_STUDY_DURATION_DATETIME\"\n",
    "            ].iloc[0]\n",
    "        ).strip(),\n",
    "        \"%H:%M:%S\"\n",
    "    )\n",
    "    return sleep_duration_datetime.hour\n",
    "\n",
    "\n",
    "def _ahi_for_study(\n",
    "    sleep_study_metadata: pd.DataFrame,\n",
    "    sleep_study: pd.DataFrame,\n",
    "    pat_id: int,\n",
    "    study_id: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculate the apnea-hypopnea index (AHI) for a given sleep study.\n",
    "    All apnea and hypopnea events will be counted from the sleep_study\n",
    "    DataFrame, and then divided by the total sleep duration, which\n",
    "    will be fetched from the sleep_study_metadata DataFrame. The result will\n",
    "    be returned as a float.\n",
    "\n",
    "    For more on AHI, see the following link:\n",
    "\n",
    "    https://www.sleepfoundation.org/sleep-apnea/ahi\n",
    "\n",
    "    :param sleep_study_metadata\n",
    "        the DataFrame that has at least the following columns in order\n",
    "        from left to right:\n",
    "        STUDY_PAT_ID,\n",
    "        SLEEP_STUDY_ID,\n",
    "        SLEEP_STUDY_START_DATETIME,\n",
    "        SLEEP_STUDY_DURATION_DATETIME\n",
    "    :param sleep_study\n",
    "        the data from the sleep study in which we're interested\n",
    "    :param pat_id\n",
    "        the ID of the patient on whom the given study was done\n",
    "    :param study_id\n",
    "        the ID of the study\n",
    "    \n",
    "    :return\n",
    "        the AHI for the given study, as a float value\n",
    "    \"\"\"\n",
    "\n",
    "    # example tab-separated (TSV) file:\n",
    "    # \n",
    "    # onset duration description\n",
    "    # 29766.7421875\t11.0546875\tObstructive Hypopnea\n",
    "\n",
    "    df = sleep_study\n",
    "    hypopnea_keys = set(HYPOPNEA_EVENT_DICT.keys())\n",
    "    apnea_keys = set(APNEA_EVENT_DICT.keys())\n",
    "\n",
    "    hypopnea_events = df.loc[df[\"description\"].isin(hypopnea_keys)]\n",
    "    apnea_events = df.loc[df[\"description\"].isin(apnea_keys)]\n",
    "    total_num_events = len(hypopnea_events) + len(apnea_events)\n",
    "    sleep_hours = float(_num_sleep_hours(\n",
    "        sleep_study_metadata,\n",
    "        pat_id,\n",
    "        study_id,\n",
    "    ))\n",
    "    return float(total_num_events) / sleep_hours\n",
    "\n",
    "\n",
    "def _parse_ss_tsv_filename(filename: str) -> tuple[int, int]:\n",
    "    '''\n",
    "    given a sleep study filename like `10048_24622.tsv`, that represents\n",
    "    <patient_id>_<sleep_study_id>.tsv, return a 2-tuple containing\n",
    "    the patient ID in element 1 and sleep study ID in element 2\n",
    "    '''\n",
    "    if not filename.endswith(\".tsv\"):\n",
    "        raise FileNotFoundError(\n",
    "            f\"expected {filename} to end with .tsv but it didn't\"\n",
    "        )\n",
    "\n",
    "    underscore_spl = filename.split(\"/\")[-1][:-4].split(\"_\")\n",
    "    if len(underscore_spl) != 2:\n",
    "        raise FileNotFoundError(f'malformed filename {filename}')\n",
    "    [pat_id, study_id] = underscore_spl\n",
    "    return (int(pat_id), int(study_id))\n",
    "\n",
    "\n",
    "def _write_tsv(out_filename: str, data: list[tuple[str, str, float]]):\n",
    "    with open(out_filename, 'w', newline='') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter=',')\n",
    "        # in ./preprocessing.py, we need to have at least 'Study'\n",
    "        # and 'AHI'. Since they chose PascalCase, I extended that usage\n",
    "        # to patient ID.\n",
    "        writer.writerow((\"PatID\", \"Study\", \"AHI\"))\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def calculate_ahi(\n",
    "    sleep_study_metadata_file: str,\n",
    "    sleep_study_root: str,\n",
    "    out_file: str,\n",
    ") -> None:\n",
    "    '''\n",
    "    Calculate the AHI values from a sleep study's metadata file and all the \n",
    "    sleep measurements in a given directory. See the _ahi_for_study function\n",
    "    for an overview of how the metadata file and sleep measurement files should\n",
    "    be structured.\n",
    "\n",
    "    :param sleep_study_metadata_file - the metadata file summarizing the sleep\n",
    "        study\n",
    "    :param sleep_study_root - the root directory containing all the individual\n",
    "        sleep study measures\n",
    "    \n",
    "    :return out_file - the name of the file to which to write the AHI values\n",
    "        in TSV format\n",
    "\n",
    "    '''\n",
    "    metadata_df = pd.read_csv(\n",
    "        sleep_study_metadata_file,\n",
    "        sep=\",\"\n",
    "    )\n",
    "\n",
    "    tsv_files = [\n",
    "        f for f in os.listdir(sleep_study_root)\n",
    "        if f.endswith(\".tsv\")\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        f\"creating AHI from {len(tsv_files)} sleep studies in \"\n",
    "        f\"{sleep_study_root} and outputting the AHI results to {out_file}\"\n",
    "    )\n",
    "\n",
    "    # each tuple is (patient_id, study_id, AHI)\n",
    "    results: list[tuple[str, str, float]] = []\n",
    "    for tsv_file in tsv_files:\n",
    "        filename = os.path.join(sleep_study_root, tsv_file)\n",
    "        pat_id, study_id = _parse_ss_tsv_filename(filename)\n",
    "        sleep_study_df = pd.read_csv(\n",
    "            filename,\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "        ahi = _ahi_for_study(\n",
    "            metadata_df,\n",
    "            sleep_study_df,\n",
    "            pat_id,\n",
    "            study_id,\n",
    "        )\n",
    "        results.append((pat_id, study_id, ahi))\n",
    "    _write_tsv(out_file, results)\n",
    "\n",
    "\n",
    "def generate_ahi_file(data_root: str, out_file: str) -> None:\n",
    "    sleep_study_metadata_file = os.path.join(\n",
    "        data_root,\n",
    "        \"files\",\n",
    "        \"nch-sleep\",\n",
    "        \"3.1.0\",\n",
    "        \"Health_Data\",\n",
    "        \"SLEEP_STUDY.csv\"\n",
    "    )\n",
    "    sleep_study_root = os.path.join(\n",
    "        data_root,\n",
    "        \"files\",\n",
    "        \"nch-sleep\",\n",
    "        \"3.1.0\",\n",
    "        \"Sleep_Data\"\n",
    "    )\n",
    "\n",
    "    calculate_ahi(\n",
    "        sleep_study_metadata_file,\n",
    "        sleep_study_root,\n",
    "        out_file=out_file,\n",
    "    )\n",
    "\n",
    "if SHOULD_PREPROCESS:\n",
    "    print(\n",
    "        f\"SHOULD_PREPROCESS is true, so generating AHI file from data at \"\n",
    "        f\"{PHYSIONET_ROOT} and outputting to {AHI_OUT_PATH}\"\n",
    "    )\n",
    "    generate_ahi_file(PHYSIONET_ROOT, AHI_OUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing code\n",
    "\n",
    "With the AHI values calculated and saved to a TSV file, we can move onto data preprocessing. Roughly speaking, the goals of preprocessing are as follows for each sleep study:\n",
    "\n",
    "- Filter out all studies for patients whose AHI is lower than a threshold\n",
    "- Collate and pad (as necessary) the relevant sleep events in the study\n",
    "- Resample samples from raw data as necessary\n",
    "    - The motivation behind, and method for Resampling was discussed above\n",
    "- Write results to a [compressed numpy representation](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sleep study loading and manipulation logic\n",
    "\n",
    "First, we have foundational logic for loading sleep study and health data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def init_study_list():\n",
    "    return [x[:-4] for x in os.listdir(SLEEP_DATA_ROOT) if x.endswith('edf')]\n",
    "\n",
    "def init_age_file():\n",
    "    new_fn = 'age_file.csv'\n",
    "    age_path = os.path.join(HEALTH_DATA_ROOT, 'SLEEP_STUDY.csv')\n",
    "\n",
    "    df = pd.read_csv(age_path, sep=',', dtype='str')\n",
    "    df['FILE_NAME'] = df[\"STUDY_PAT_ID\"].str.cat(df[\"SLEEP_STUDY_ID\"], sep='_')\n",
    "\n",
    "    df.to_csv(new_fn, columns=[\"FILE_NAME\", \"AGE_AT_SLEEP_STUDY_DAYS\"], index=False)\n",
    "    return os.path.abspath(new_fn)\n",
    "\n",
    "def load_health_info(name: str, convert_datetime: bool = True):\n",
    "    assert type(name) == str\n",
    "\n",
    "    path = os.path.join(HEALTH_DATA_ROOT, name)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    if convert_datetime:\n",
    "        if name == DEMOGRAPHIC:\n",
    "            df['BIRTH_DATE'] = pd.to_datetime(df['BIRTH_DATE'], infer_datetime_format=True)\n",
    "        elif name == DIAGNOSIS:\n",
    "            df['DX_START_DATETIME'] = pd.to_datetime(df['DX_START_DATETIME'], infer_datetime_format=True)\n",
    "            df['DX_END_DATETIME'] = pd.to_datetime(df['DX_END_DATETIME'], infer_datetime_format=True)\n",
    "        elif name == ENCOUNTER:\n",
    "            df['ENCOUNTER_DATE'] = pd.to_datetime(df['ENCOUNTER_DATE'], infer_datetime_format=True)\n",
    "            df['VISIT_START_DATETIME'] = pd.to_datetime(df['VISIT_START_DATETIME'], infer_datetime_format=True)\n",
    "            df['VISIT_END_DATETIME'] = pd.to_datetime(df['VISIT_END_DATETIME'], infer_datetime_format=True)\n",
    "            df['ADT_ARRIVAL_DATETIME'] = pd.to_datetime(df['ADT_ARRIVAL_DATETIME'], infer_datetime_format=True)\n",
    "            df['ED_DEPARTURE_DATETIME'] = pd.to_datetime(df['ED_DEPARTURE_DATETIME'], infer_datetime_format=True)\n",
    "        elif name == MEASUREMENT:\n",
    "            df['MEAS_RECORDED_DATETIME'] = pd.to_datetime(df['MEAS_RECORDED_DATETIME'], infer_datetime_format=True)\n",
    "        elif name == MEDICATION:\n",
    "            df['MED_START_DATETIME'] = pd.to_datetime(df['MED_START_DATETIME'], infer_datetime_format=True)\n",
    "            df['MED_END_DATETIME'] = pd.to_datetime(df['MED_END_DATETIME'], infer_datetime_format=True)\n",
    "            df['MED_ORDER_DATETIME'] = pd.to_datetime(df['MED_ORDER_DATETIME'], infer_datetime_format=True)\n",
    "            df['MED_TAKEN_DATETIME'] = pd.to_datetime(df['MED_TAKEN_DATETIME'], infer_datetime_format=True)\n",
    "        elif name == PROCEDURE:\n",
    "            df['PROCEDURE_DATETIME'] = pd.to_datetime(df['PROCEDURE_DATETIME'], infer_datetime_format=True)\n",
    "        elif name == PROCEDURE_SURG_HX:\n",
    "            df['PROC_NOTED_DATE'] = pd.to_datetime(df['PROC_NOTED_DATE'], infer_datetime_format=True)\n",
    "            df['PROC_START_TIME'] = pd.to_datetime(df['PROC_START_TIME'], infer_datetime_format=True)\n",
    "            df['PROC_END_TIME'] = pd.to_datetime(df['PROC_END_TIME'], infer_datetime_format=True)\n",
    "        elif name == SLEEP_STUDY_NAME:\n",
    "            df['SLEEP_STUDY_START_DATETIME'] = pd.to_datetime(df['SLEEP_STUDY_START_DATETIME'])\n",
    "\n",
    "    return df\n",
    "\n",
    "if SHOULD_PREPROCESS:\n",
    "    print(f'SHOULD_PREPROCESS is true, so loading sleep study information into memory')\n",
    "    ss_study_list = init_study_list()\n",
    "    age_fn = init_age_file()\n",
    "    SLEEP_STUDY = load_health_info(\"SLEEP_STUDY.csv\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing code\n",
    "\n",
    "Next, the actual preprocessing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os.path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import mne\n",
    "import numpy as np\n",
    "from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n",
    "from biosppy.signals import tools as st\n",
    "from mne import make_fixed_length_events\n",
    "from scipy.interpolate import splev, splrep\n",
    "from itertools import compress\n",
    "\n",
    "mne.set_log_file('log.txt', overwrite=False)\n",
    "\n",
    "CHUNK_DURATION = 30.0\n",
    "FREQ = 64.0\n",
    "\n",
    "POS_EVENT_DICT: dict[str, int] = {\n",
    "    \"Obstructive Hypopnea\": 1,\n",
    "    \"Hypopnea\": 1,\n",
    "    \"hypopnea\": 1,\n",
    "    \"Mixed Hypopnea\": 1,\n",
    "    \"Central Hypopnea\": 1,\n",
    "\n",
    "    \"Obstructive Apnea\": 2,\n",
    "    \"Central Apnea\": 2,\n",
    "    \"Mixed Apnea\": 2,\n",
    "    \"apnea\": 2,\n",
    "    \"obstructive apnea\": 2,\n",
    "    \"central apnea\": 2,\n",
    "    \"Apnea\": 2,\n",
    "}\n",
    "\n",
    "WAKE_DICT: dict[str, int] = {\n",
    "    \"Sleep stage W\": 10\n",
    "}\n",
    "\n",
    "########################################## Annotation Modifier functions ##########################################\n",
    "def identity(df):\n",
    "    return df\n",
    "\n",
    "\n",
    "def apnea2bad(df):\n",
    "    df = df.replace(r'.*pnea.*', 'badevent', regex=True)\n",
    "    print(\"bad replaced!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def wake2bad(df):\n",
    "    return df.replace(\"Sleep stage W\", 'badevent')\n",
    "\n",
    "\n",
    "def change_duration(df, label_dict=POS_EVENT_DICT, duration=CHUNK_DURATION):\n",
    "    for key in label_dict:\n",
    "        df.loc[df.description == key, 'duration'] = duration\n",
    "    print(\"change duration!\")\n",
    "    return df\n",
    "\n",
    "def preprocess(i, annotation_modifier, out_dir, ahi_dict):\n",
    "    is_apnea_available, is_hypopnea_available = True, True\n",
    "    study = ss.data.study_list[i]\n",
    "\n",
    "    # print(f\"loading study {study}\")\n",
    "    raw = ss.data.load_study(study, annotation_modifier, verbose=True)\n",
    "\n",
    "    ########################################   CHECK CRITERIA FOR SS   #################################################\n",
    "    if not all([name in raw.ch_names for name in channels]):\n",
    "        print(\"study \" + str(study) + \" skipped since insufficient channels\", file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "    ahi_value = ahi_dict.get(study,None)\n",
    "    if ahi_value is None:\n",
    "        print(ahi_dict)\n",
    "        print(\"study \" + str(study) + \" skipped since AHI is MISSING.  Is AHI.csv out of date?\", file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "    if ahi_value < THRESHOLD:\n",
    "        print(\"study \" + str(study) + \" skipped since low AHI ---  AHI = \" + str(ahi_value), file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        apnea_events, event_ids = mne.events_from_annotations(\n",
    "            raw,\n",
    "            event_id=POS_EVENT_DICT,\n",
    "            chunk_duration=1.0,\n",
    "            verbose=None\n",
    "        )\n",
    "        # print('|')\n",
    "    except ValueError:\n",
    "        print(\"No Chunk found!\", file=sys.stderr)\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "    ########################################   CHECK CRITERIA FOR SS   #################################################\n",
    "    print(str(i) + \"---\" + str(datetime.now().time().strftime(\"%H:%M:%S\")) + ' --- Processing %d' % i)\n",
    "\n",
    "    try:\n",
    "        apnea_events, event_ids = mne.events_from_annotations(\n",
    "            raw,\n",
    "            event_id=APNEA_EVENT_DICT,\n",
    "            chunk_duration=1.0,\n",
    "            verbose=None\n",
    "        )\n",
    "    except ValueError:\n",
    "        is_apnea_available = False\n",
    "\n",
    "    try:\n",
    "        hypopnea_events, event_ids = mne.events_from_annotations(\n",
    "            raw,\n",
    "            event_id=HYPOPNEA_EVENT_DICT,\n",
    "            chunk_duration=1.0,\n",
    "            verbose=None\n",
    "        )\n",
    "    except ValueError:\n",
    "        is_hypopnea_available = False\n",
    "\n",
    "    wake_events, event_ids = mne.events_from_annotations(\n",
    "        raw,\n",
    "        event_id=WAKE_DICT,\n",
    "        chunk_duration=1.0,\n",
    "        verbose=None\n",
    "    )\n",
    "    ####################################################################################################################\n",
    "    sfreq = raw.info['sfreq']\n",
    "    tmax = CHUNK_DURATION - 1. / sfreq\n",
    "\n",
    "    raw = raw.pick_channels(channels, ordered=True)\n",
    "    fixed_events = make_fixed_length_events(\n",
    "        raw,\n",
    "        id=0,\n",
    "        duration=CHUNK_DURATION,\n",
    "        overlap=0.\n",
    "    )\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        fixed_events,\n",
    "        event_id=[0],\n",
    "        tmin=0,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        preload=True,\n",
    "        proj=False,\n",
    "        verbose=None\n",
    "    )\n",
    "    epochs.load_data()\n",
    "    if sfreq != FREQ:\n",
    "        epochs = epochs.resample(FREQ, npad='auto', n_jobs=4, verbose=None)\n",
    "    data = epochs.get_data()\n",
    "    ####################################################################################################################\n",
    "    if is_apnea_available:\n",
    "        apnea_events_set = set((apnea_events[:, 0] / sfreq).astype(int))\n",
    "    if is_hypopnea_available:\n",
    "        hypopnea_events_set = set((hypopnea_events[:, 0] / sfreq).astype(int))\n",
    "    wake_events_set = set((wake_events[:, 0] / sfreq).astype(int))\n",
    "\n",
    "    starts = (epochs.events[:, 0] / sfreq).astype(int)\n",
    "\n",
    "    labels_apnea = []\n",
    "    labels_hypopnea = []\n",
    "    labels_wake = []\n",
    "    total_apnea_event_second = 0\n",
    "    total_hypopnea_event_second = 0\n",
    "\n",
    "    for seq in range(data.shape[0]):\n",
    "        epoch_set = set(range(starts[seq], starts[seq] + int(CHUNK_DURATION)))\n",
    "        if is_apnea_available:\n",
    "            apnea_seconds = len(apnea_events_set.intersection(epoch_set))\n",
    "            total_apnea_event_second += apnea_seconds\n",
    "            labels_apnea.append(apnea_seconds)\n",
    "        else:\n",
    "            labels_apnea.append(0)\n",
    "\n",
    "        if is_hypopnea_available:\n",
    "            hypopnea_seconds = len(hypopnea_events_set.intersection(epoch_set))\n",
    "            total_hypopnea_event_second += hypopnea_seconds\n",
    "            labels_hypopnea.append(hypopnea_seconds)\n",
    "        else:\n",
    "            labels_hypopnea.append(0)\n",
    "\n",
    "        labels_wake.append(len(wake_events_set.intersection(epoch_set)) == 0)\n",
    "    ####################################################################################################################\n",
    "    print(study + \"    HAMED    \" + str(len(labels_wake) - sum(labels_wake)))\n",
    "    data = data[labels_wake, :, :]\n",
    "    labels_apnea = list(compress(labels_apnea, labels_wake))\n",
    "    labels_hypopnea = list(compress(labels_hypopnea, labels_wake))\n",
    "\n",
    "    out_name = study + \"_\" + str(total_apnea_event_second) + \"_\" + str(total_hypopnea_event_second)\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    # print(f\"Saving {study} to {out_path}.npz\")\n",
    "    np.savez_compressed(out_path, data=data, labels_apnea=labels_apnea, labels_hypopnea=labels_hypopnea)\n",
    "\n",
    "    return data.shape[0]\n",
    "\n",
    "def run_preprocess(\n",
    "    ahi_path: str,\n",
    "    out_folder: str,\n",
    "    n_studies: int = 3984,\n",
    "    n_workers: int = 3\n",
    "):\n",
    "    if not os.path.exists(ahi_path):\n",
    "        return FileNotFoundError(f'AHI file {ahi_path} was not found!')\n",
    "\n",
    "    ahi = pd.read_csv(ahi_path)\n",
    "    # filename is <patient_id>_<study>\n",
    "    filenames = ahi['PatID'].astype(str) + '_' + ahi['Study'].astype(str)\n",
    "    # ahi_dict = dict(zip(ahi.Study, ahi.AHI))\n",
    "    ahi_dict = dict(zip(filenames, ahi['AHI']))\n",
    "\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "\n",
    "    if n_workers < 2:\n",
    "        for idx in range(n_studies):\n",
    "            preprocess(\n",
    "                ahi_path=idx,\n",
    "                out_folder=identity,\n",
    "                n_studies=out_folder,\n",
    "                n_workers=ahi_dict\n",
    "            )\n",
    "    else:\n",
    "        with concurrent.futures.ThreadPoolExecutor(\n",
    "            max_workers=n_workers\n",
    "        ) as executor:\n",
    "            executor.map(\n",
    "                preprocess,\n",
    "                range(n_studies),\n",
    "                [identity] * n_studies,\n",
    "                [out_folder] * n_studies,\n",
    "                [ahi_dict] * n_studies\n",
    "            )\n",
    "\n",
    "if SHOULD_PREPROCESS:\n",
    "    print(f'SHOULD_PREPROCESS is true, so running preprocessing logic')\n",
    "    run_preprocess(AHI_OUT_PATH, PREPROCESS_OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "After data are preprocessed, we are left with a compressed numpy array file, also called an `npz` file. At this point, we are ready to take the final step before model training - data loading.\n",
    "\n",
    "The data loader code requires familiar collation and padding logic shown below. Notably, we make use of [TensorFlow's ragged tensors](https://www.tensorflow.org/guide/ragged_tensor) to simply and easily handle the task of padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy.typing as npt\n",
    "import tensorflow as tf\n",
    "import tensorflow.ragged\n",
    "\n",
    "def max_dimensions(\n",
    "        lst: list[Any],\n",
    "        level: int=0,\n",
    "        max_dims: list[int] | None = None\n",
    ") -> tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Finds the maximum dimension for each level of a nested list structure, and return\n",
    "    each dimension ordered in a tuple.\n",
    "\n",
    "    For example, given a nested list like the following:\n",
    "\n",
    "    [[1, 2, 3], [1, 2]]\n",
    "\n",
    "    ... return the tuple (2, 3), since the first dimension has 2 elements and then\n",
    "    second dimension has max(2, 3) elements.\n",
    "\n",
    "    :param lst: The list for which to get dimensions\n",
    "    :param level: INTERNAL USE ONLY (the dimension we are processing)\n",
    "    :param max_dims: INTERNAL USE ONLY (the current array of maximums)\n",
    "    :return: a tuple of sizes, similar to torch.Tensor.shape()\n",
    "    \"\"\"\n",
    "    if max_dims is None:\n",
    "        max_dims = []\n",
    "\n",
    "    # Extend the max_dims list if this is the deepest level we've encountered so far\n",
    "    if level >= len(max_dims):\n",
    "        max_dims.append(len(lst))\n",
    "    else:\n",
    "        max_dims[level] = max(max_dims[level], len(lst))\n",
    "\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            # Recursively process each sublist\n",
    "            max_dimensions(item, level + 1, max_dims)\n",
    "\n",
    "    return tuple(max_dims)\n",
    "\n",
    "def pad_lists(\n",
    "    lst: list[Any],\n",
    "    pad_with: int = 0\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Given a ragged nested list structure `lst` (i.e. where the length\n",
    "    in each dimension is not uniform), return a new list with all\n",
    "    levels of the list padded to the maximal length of any list in that \n",
    "    dimension. Padding elements will have the same value as given in \n",
    "    `pad_with`\n",
    "\n",
    "    For example, the return value of `pad_lists([[1], [1, 2]], 0)` will\n",
    "    be `[[1, 0], [1, 2]]`\n",
    "\n",
    "    :param lst: the list to pad, if it is ragged. if it's not, this function\n",
    "        is a no-op\n",
    "    :param pad_with: the value to use for padding\n",
    "    \"\"\"\n",
    "    # max_dims[0] is the number of elements (either lists or ints) we \n",
    "    # need in this dimension\n",
    "    assert type(lst) is list\n",
    "    rt = tensorflow.ragged.constant(lst)\n",
    "    return rt.to_tensor(pad_with).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, with padding and collating handled, data loading code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from typing import Any\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import resample\n",
    "from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n",
    "from biosppy.signals import tools as st\n",
    "from scipy.interpolate import splev, splrep\n",
    "\n",
    "# \"EOG LOC-M2\",  # 0\n",
    "# \"EOG ROC-M1\",  # 1\n",
    "# \"EEG C3-M2\",  # 2\n",
    "# \"EEG C4-M1\",  # 3\n",
    "# \"ECG EKG2-EKG\",  # 4\n",
    "#\n",
    "# \"RESP PTAF\",  # 5\n",
    "# \"RESP AIRFLOW\",  # 6\n",
    "# \"RESP THORACIC\",  # 7\n",
    "# \"RESP ABDOMINAL\",  # 8\n",
    "# \"SPO2\",  # 9\n",
    "# \"CAPNO\",  # 10\n",
    "\n",
    "######### ADDED IN THIS STEP #########\n",
    "# RRI #11\n",
    "# Ramp #12\n",
    "# Demo #13\n",
    "\n",
    "\n",
    "SIGS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "s_count = len(SIGS)\n",
    "\n",
    "THRESHOLD = 3\n",
    "FREQ = 64\n",
    "EPOCH_DURATION = 30\n",
    "ECG_SIG = 4\n",
    "\n",
    "AHI_PATH = AHI_OUT_PATH\n",
    "OUT_PATH = PREPROCESS_OUT_PATH\n",
    "PATH=os.path.join(PHYSIONET_ROOT, \"nch_30x64\")\n",
    "\n",
    "\n",
    "def extract_rri(signal, ir, CHUNK_DURATION):\n",
    "    tm = np.arange(0, CHUNK_DURATION, step=1 / float(ir))  # TIME METRIC FOR INTERPOLATION\n",
    "\n",
    "    # print('filtering', signal, FREQ)\n",
    "    # TODO: Temporarily bypassed until we know how we want to handle this\n",
    "    # filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\", order=int(0.3 * FREQ),\n",
    "    #                                   frequency=[3, 45], sampling_rate=FREQ, )\n",
    "    filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\", order=int(0.3 * FREQ),\n",
    "                                      frequency=[3, 30], sampling_rate=FREQ, )\n",
    "    (rpeaks,) = hamilton_segmenter(signal=filtered, sampling_rate=FREQ)\n",
    "    (rpeaks,) = correct_rpeaks(signal=filtered, rpeaks=rpeaks, sampling_rate=FREQ, tol=0.05)\n",
    "\n",
    "    if 4 < len(rpeaks) < 200:  # and np.max(signal) < 0.0015 and np.min(signal) > -0.0015:\n",
    "        rri_tm, rri_signal = rpeaks[1:] / float(FREQ), np.diff(rpeaks) / float(FREQ)\n",
    "        ampl_tm, ampl_signal = rpeaks / float(FREQ), signal[rpeaks]\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=1)\n",
    "        amp_interp_signal = splev(tm, splrep(ampl_tm, ampl_signal, k=3), ext=1)\n",
    "\n",
    "        return np.clip(rri_interp_signal, 0, 2), np.clip(amp_interp_signal, -0.001, 0.002)\n",
    "    else:\n",
    "        return np.zeros((FREQ * EPOCH_DURATION)), np.zeros((FREQ * EPOCH_DURATION))\n",
    "\n",
    "\n",
    "def load_data(path: str) -> tuple[list[Any], list[Any], list[Any]]:\n",
    "    '''\n",
    "    given a path to the directory of sleep studies\n",
    "    (i.e. DATA_ROOT/physionet.org/nch_30x64), load and process all sleep \n",
    "    studies, then return them\n",
    "    '''\n",
    "    # demo = pd.read_csv(\"../misc/result.csv\") # TODO\n",
    "\n",
    "    ahi = pd.read_csv(AHI_PATH)\n",
    "    filename = ahi.PatID.astype(str) + '_' + ahi.Study.astype(str)\n",
    "    ahi_dict = dict(zip(filename, ahi.AHI))\n",
    "    root_dir = os.path.expanduser(path)\n",
    "    file_list = os.listdir(root_dir)\n",
    "    length = len(file_list)\n",
    "\n",
    "    print(f\"Using AHI from {AHI_PATH}\")\n",
    "    print(f\"Using npz files from {root_dir}\")\n",
    "    print(f\"Files {file_list}\")\n",
    "\n",
    "    study_event_counts = {}\n",
    "    apnea_event_counts = {}\n",
    "    hypopnea_event_counts = {}\n",
    "    ######################################## Count the respiratory events ###########################################\n",
    "    for i in range(length):\n",
    "        # skip directories\n",
    "        if os.path.isdir(file_list[i]):\n",
    "            continue\n",
    "\n",
    "        # print(f\"Processing {file_list[i]}\")\n",
    "        try:\n",
    "            # parts = file_list[i].split(\"_\")\n",
    "            # parts[0] should be nch\n",
    "\n",
    "            patient_id = (file_list[i].split(\"_\")[0])\n",
    "            study_id = (file_list[i].split(\"_\")[1])\n",
    "            apnea_count = int((file_list[i].split(\"_\")[2]))\n",
    "            hypopnea_count = int((file_list[i].split(\"_\")[3]).split(\".\")[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Filename mismatch. Skipping {file_list[i]} ({e})\", file=sys.stderr)\n",
    "            continue\n",
    "        filename = f\"{patient_id}_{study_id}\"\n",
    "        ahi_value = ahi_dict.get(filename, None)\n",
    "        if ahi_value is None:\n",
    "            print(f\"Sleep study {filename} is not found in AHI.csv.  Skipping {file_list[i]}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if ahi_value > THRESHOLD:\n",
    "                apnea_event_counts[patient_id] = apnea_event_counts.get(patient_id, 0) + apnea_count\n",
    "                hypopnea_event_counts[patient_id] = hypopnea_event_counts.get(patient_id, 0) + hypopnea_count\n",
    "                study_event_counts[patient_id] = study_event_counts.get(patient_id, 0) + apnea_count + hypopnea_count\n",
    "        except Exception as e:\n",
    "            print(f\"File structure problem.  Skipping {file_list[i]} ({e})\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "    apnea_event_counts = sorted(apnea_event_counts.items(), key=lambda item: item[1])\n",
    "    hypopnea_event_counts = sorted(hypopnea_event_counts.items(), key=lambda item: item[1])\n",
    "    study_event_counts = sorted(study_event_counts.items(), key=lambda item: item[1])\n",
    "\n",
    "    ################################### Fold the data based on number of respiratory events #########################\n",
    "    folds = []\n",
    "    for i in range(5):\n",
    "        folds.append(study_event_counts[i::5])\n",
    "\n",
    "    # print('FOLDS:', folds)\n",
    "\n",
    "    x = []\n",
    "    y_apnea = []\n",
    "    y_hypopnea = []\n",
    "    counter = 0\n",
    "    for idx, fold in enumerate(folds):\n",
    "        first = True\n",
    "        aggregated_data = None\n",
    "        aggregated_label_apnea = None\n",
    "        aggregated_label_hypopnea = None\n",
    "        for patient in fold:\n",
    "            counter += 1\n",
    "            # print(counter)\n",
    "            glob_path = os.path.join(PATH, patient[0] + \"_*\")\n",
    "            # print(\"glob path\", glob_path)\n",
    "            for study in glob.glob(glob_path):\n",
    "                study_data = np.load(study)\n",
    "\n",
    "                signals = study_data['data']\n",
    "                labels_apnea = study_data['labels_apnea']\n",
    "                labels_hypopnea = study_data['labels_hypopnea']\n",
    "\n",
    "                identifier = study.split(os.path.sep)[-1].split('_')[0] + \"_\" + study.split(os.path.sep)[-1].split('_')[1]\n",
    "                # print(identifier)\n",
    "                # demo_arr = demo[demo['id'] == identifier].drop(columns=['id']).to_numpy().squeeze() # TODO\n",
    "\n",
    "                y_c = labels_apnea + labels_hypopnea\n",
    "                neg_samples = np.where(y_c == 0)[0]\n",
    "                pos_samples = list(np.where(y_c > 0)[0])\n",
    "                ratio = len(pos_samples) / len(neg_samples)\n",
    "                neg_survived = []\n",
    "                for s in range(len(neg_samples)):\n",
    "                    if random.random() < ratio:\n",
    "                        neg_survived.append(neg_samples[s])\n",
    "                samples = neg_survived + pos_samples\n",
    "                signals = signals[samples, :, :]\n",
    "                labels_apnea = labels_apnea[samples]\n",
    "                labels_hypopnea = labels_hypopnea[samples]\n",
    "\n",
    "                data = np.zeros((signals.shape[0], EPOCH_DURATION * FREQ, s_count + 3))\n",
    "                for i in range(signals.shape[0]):  # for each epoch\n",
    "                    # data[i, :len(demo_arr), -1] = demo_arr TODO\n",
    "                    data[i, :, -2], data[i, :, -3] = extract_rri(signals[i, ECG_SIG, :], FREQ, float(EPOCH_DURATION))\n",
    "                    for j in range(s_count):  # for each signal\n",
    "                        data[i, :, j] = resample(signals[i, SIGS[j], :], EPOCH_DURATION * FREQ)\n",
    "\n",
    "                if first:\n",
    "                    aggregated_data = data\n",
    "                    aggregated_label_apnea = labels_apnea\n",
    "                    aggregated_label_hypopnea = labels_hypopnea\n",
    "                    first = False\n",
    "                else:\n",
    "                    aggregated_data = np.concatenate((aggregated_data, data), axis=0)\n",
    "                    aggregated_label_apnea = np.concatenate((aggregated_label_apnea, labels_apnea), axis=0)\n",
    "                    aggregated_label_hypopnea = np.concatenate((aggregated_label_hypopnea, labels_hypopnea), axis=0)\n",
    "\n",
    "        if aggregated_data is not None:\n",
    "            x.append(aggregated_data.tolist())\n",
    "        if aggregated_label_apnea is not None:\n",
    "            y_apnea.append(aggregated_label_apnea.tolist())\n",
    "        if aggregated_label_hypopnea is not None:\n",
    "            y_hypopnea.append(aggregated_label_hypopnea.tolist())\n",
    "\n",
    "    return x, y_apnea, y_hypopnea\n",
    "\n",
    "def list_lengths(lst):\n",
    "    \"\"\"\n",
    "    Gets all the individual lengths of a list\n",
    "    :param lst:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(lst, list):\n",
    "        # For each item in the list, recursively process it if it's a list\n",
    "        # Otherwise, the item itself is not counted and is represented as None for non-list items\n",
    "        sublengths = [list_lengths(item) for item in lst]\n",
    "        if len([*filter(lambda v: v is not None, sublengths)]) == 0:\n",
    "            return len(lst)\n",
    "        # Instead of returning None for non-list items, you could choose to omit them or handle differently\n",
    "        return len(lst), sublengths  # Return the length of the current list and the structure\n",
    "    # Return None or some indication for non-list items, if needed\n",
    "    return None\n",
    "\n",
    "def run_load_data():\n",
    "    raw_data_root = os.path.join(PHYSIONET_ROOT, \"nch_30x64\")\n",
    "    x, y_apnea, y_hypopnea = load_data(raw_data_root)\n",
    "\n",
    "    # these output the maximum size for dimension.\n",
    "    # If we're going to make this a consistent size without truncating,\n",
    "    # this is the size to make it\n",
    "    print(f\"Padded X.shape:{max_dimensions(x)}\")\n",
    "    x_norm = pad_lists(x, 0)\n",
    "\n",
    "    print(f\"Padded Y_a shape: {max_dimensions(y_apnea)}\")\n",
    "    y_apnea_norm = pad_lists(y_apnea, 0)\n",
    "\n",
    "    print(f\"Padded Y_h.shape:{max_dimensions(y_hypopnea)}\")\n",
    "    y_hypopnea_norm = pad_lists(y_hypopnea, 0)\n",
    "\n",
    "    print(f\"Saving to {OUT_PATH}\")\n",
    "    np.savez_compressed(\n",
    "        OUT_PATH,\n",
    "        x=x_norm,\n",
    "        y_apnea=y_apnea_norm,\n",
    "        y_hypopnea=y_hypopnea_norm,\n",
    "    )\n",
    "\n",
    "if SHOULD_PREPROCESS:\n",
    "    print(f'SHOULD_PREPROCESS set to true')\n",
    "    print(f'PREPROCESS_OUT_PATH={PREPROCESS_OUT_PATH}')\n",
    "    run_load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "#   Model\n",
    "\n",
    "As discussed above, the model this paper proposes is based on the [transformer architecture](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)). While this architecture has led to excellent results in NLP and other applications, it has, to our knowledge, not been studied in the context of sleep apnea. It is implemented (although broken, as we discuss elsewhere in this notebook) in the paper's [accompanying open-source repository on GitHub](https://github.com/healthylaife/Pediatric-Apnea-Detection).\n",
    "\n",
    "Like many other transformer-based architectures, this model has several other components, illustrated below.\n",
    "\n",
    "![model architecture](./images/model_arch.png)\n",
    "\n",
    "As seen in this architecture, inputs, which are primarily pre-processed signals data, are first fed in parallel through a variety of 1-D convolutional layers, partially concatenated to fewer parallel \"tracks\", and then those results are passed through an activation layer. Next, all tracks are concatenated prior to being passed into the transformer, which includes at least one multi-headed attention mechanism and one dense layer. Finally, the output of the transformer is passed through a normalization layer, pooling layer, fully-connected layer (labeled \"Multi-layer perceptron\" in the illustration) and a final activation function.\n",
    "\n",
    "As mentioned in previous sections, we provide a pre-trained model because we are unable to share raw, preprocessed, or loaded data in any form due to licensing issues. Model code is shown in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessed data loading\n",
    "\n",
    "All code prior to this section relies on raw data being present. After running that code, a single `npz` (numpy compressed file, discussed above) is written to disk. Below is code to ensure that `npz` file is in place and, if not, download it if the `SHOULD_PREPROCESS` flag is set to `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed file found at expected location /root/data/physionet.org/nch_30x64.npz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(PREPROCESS_OUT_PATH):\n",
    "    # if the file exists, just move on regardless of whether it was generated\n",
    "    # in this notebook or downloaded\n",
    "    print(f'preprocessed file found at expected location {PREPROCESS_OUT_PATH}')\n",
    "elif SHOULD_PREPROCESS:\n",
    "    # otherwise, the file was not found but the preprocessing flag was set,\n",
    "    # so this notebook should have processed it.\n",
    "    raise FileNotFoundError(\n",
    "        f'ERROR: preprocessed data at path {PREPROCESS_OUT_PATH} does '\n",
    "        'not exist. please check that preprocessing succeeded'\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f'preprocessed file {PREPROCESS_OUT_PATH} does not exist,'\n",
    "        f'downloading from {PREPROCESS_URL}'\n",
    "    )\n",
    "    # just a simple function to report the progress of the download operation. \n",
    "    # such reporting may be nice since this file could take a long time to download\n",
    "    # on slow connections.\n",
    "    def dl_report_hook(blcks_sofar: int, blck_size: int, tot_file_size: int) -> None:\n",
    "        print(\n",
    "            f'({blcks_sofar} blocks downloaded), '\n",
    "            f'{blcks_sofar*blck_size} of {tot_file_size}'\n",
    "        )\n",
    "    urlretrieve(\n",
    "        url=PREPROCESS_URL,\n",
    "        filename=PREPROCESS_OUT_PATH,\n",
    "        reporthook=dl_report_hook, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer layers\n",
    "\n",
    "Since model code is extensive, we split it up into two sections. We show the transformer model code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras import Model\n",
    "from keras.activations import sigmoid, relu\n",
    "from keras.layers import Dense, Dropout, Reshape, LayerNormalization, MultiHeadAttention, Add, Flatten, Input, Layer, \\\n",
    "    GlobalAveragePooling1D, AveragePooling1D, Concatenate, SeparableConvolution1D, Conv1D\n",
    "from keras.regularizers import L2\n",
    "\n",
    "\n",
    "class Patches(Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, input):\n",
    "        input = input[:, tf.newaxis, :, :]\n",
    "        batch_size = tf.shape(input)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=input,\n",
    "            sizes=[1, 1, self.patch_size, 1],\n",
    "            strides=[1, 1, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches,\n",
    "                             [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "\n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches, projection_dim, l2_weight):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.l2_weight = l2_weight\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(units=projection_dim, kernel_regularizer=L2(l2_weight),\n",
    "                                bias_regularizer=L2(l2_weight))\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) # + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate, l2_weight):\n",
    "    for _, units in enumerate(hidden_units):\n",
    "        x = Dense(units, activation=None, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight))(x)\n",
    "        x = tf.nn.gelu(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_transformer_model(input_shape, num_patches,\n",
    "                             projection_dim, transformer_layers,\n",
    "                             num_heads, transformer_units, mlp_head_units,\n",
    "                             num_classes, drop_out, reg, l2_weight, demographic=False):\n",
    "    if reg:\n",
    "        activation = None\n",
    "    else:\n",
    "        activation = 'sigmoid'\n",
    "    inputs = Input(shape=input_shape)\n",
    "    patch_size = input_shape[0] / num_patches\n",
    "    if demographic:\n",
    "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                                             beta_initializer=\"glorot_uniform\",\n",
    "                                                             gamma_initializer=\"glorot_uniform\")(inputs[:,:,:-1])\n",
    "        demo = inputs[:, :12, -1]\n",
    "\n",
    "    else:\n",
    "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                                             beta_initializer=\"glorot_uniform\",\n",
    "                                                             gamma_initializer=\"glorot_uniform\")(inputs)\n",
    "\n",
    "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
    "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
    "    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n",
    "            bias_regularizer=L2(l2_weight))(x1, x1)\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    #x = Concatenate()([x, demo])\n",
    "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
    "\n",
    "    logits = Dense(num_classes, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n",
    "                   activation=activation)(features)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "\n",
    "\n",
    "def create_hybrid_transformer_model(input_shape):\n",
    "    transformer_units =  [32,32]\n",
    "    transformer_layers = 2\n",
    "    num_heads = 4\n",
    "    l2_weight = 0.001\n",
    "    drop_out= 0.25\n",
    "    mlp_head_units = [256, 128]\n",
    "    num_patches=30\n",
    "    projection_dim=  32\n",
    "\n",
    "    # Conv1D(32...\n",
    "    input1 = Input(shape=input_shape)\n",
    "    conv11 = Conv1D(16, 256)(input1) #13\n",
    "    conv12 = Conv1D(16, 256)(input1) #13\n",
    "    conv13 = Conv1D(16, 256)(input1) #13\n",
    "\n",
    "    pwconv1 = SeparableConvolution1D(32, 1)(input1)\n",
    "    pwconv2 = SeparableConvolution1D(32, 1)(pwconv1)\n",
    "\n",
    "    conv21 = Conv1D(16, 256)(conv11) # 7\n",
    "    conv22 = Conv1D(16, 256)(conv12) # 7\n",
    "    conv23 = Conv1D(16, 256)(conv13) # 7\n",
    "\n",
    "    concat = keras.layers.concatenate([conv21, conv22, conv23], axis=-1)\n",
    "    concat = Dense(64, activation=relu)(concat) #192\n",
    "    concat = Dense(64, activation=sigmoid)(concat) #192\n",
    "    concat = SeparableConvolution1D(32,1)(concat)\n",
    "    concat = keras.layers.concatenate([concat, pwconv2], axis=1)\n",
    "\n",
    "    ####################################################################################################################\n",
    "    patch_size = input_shape[0] / num_patches\n",
    "\n",
    "    normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                                             beta_initializer=\"glorot_uniform\",\n",
    "                                                             gamma_initializer=\"glorot_uniform\")(concat)\n",
    "\n",
    "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
    "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
    "    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n",
    "            bias_regularizer=L2(l2_weight))(x1, x1)\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    #x = Concatenate()([x, demo])\n",
    "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
    "\n",
    "    logits = Dense(1, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n",
    "                   activation='sigmoid')(features)\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    model = Model(inputs=input1, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting layers\n",
    "\n",
    "As we show above, several layers surround the transformer layers. These include several 1-D convolutions, activations, concatenations, normalizations, and fully-connected layers. Code for these layers is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Input, Model\n",
    "from keras.layers import Dense, Flatten, MaxPooling2D, Conv2D, BatchNormalization, LSTM, Bidirectional, Permute, \\\n",
    "    Reshape, GRU, Conv1D, MaxPooling1D, Activation, Dropout, GlobalAveragePooling1D, multiply, MultiHeadAttention, Add, \\\n",
    "    LayerNormalization, SeparableConvolution1D\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.regularizers import l2\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    for i in range(5): # 10\n",
    "        model.add(Conv1D(45, 32, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(relu))\n",
    "        model.add(MaxPooling1D())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    for i in range(2): #4\n",
    "        model.add(Dense(512))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(relu))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnnlstm_model(input_a_shape, weight=1e-3):\n",
    "    cnn_filters = 32 # 128\n",
    "    cnn_kernel_size = 4 # 4\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                              beta_initializer=\"glorot_uniform\",\n",
    "                                              gamma_initializer=\"glorot_uniform\")(input1)\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(input1)\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = LSTM(32, return_sequences=True)(x1) #256\n",
    "    x1 = LSTM(32, return_sequences=True)(x1) #256\n",
    "    x1 = LSTM(32)(x1) #256\n",
    "    x1 = Flatten()(x1)\n",
    "\n",
    "    x1 = Dense(32, activation='relu')(x1) #64\n",
    "    x1 = Dense(32, activation='relu')(x1) #64\n",
    "    outputs = Dense(1, activation='sigmoid')(x1)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_semscnn_model(input_a_shape):\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    # input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "    #                                           beta_initializer=\"glorot_uniform\",\n",
    "    #                                           gamma_initializer=\"glorot_uniform\")(input1)\n",
    "    x1 = Conv1D(45, 32, strides=1)(input1) #kernel_size=11\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    squeeze = Flatten()(x1)\n",
    "    excitation = Dense(128, activation='relu')(squeeze)\n",
    "    excitation = Dense(64, activation='relu')(excitation)\n",
    "    logits = Dense(1, activation='sigmoid')(excitation)\n",
    "    model = Model(inputs=input1, outputs=logits)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "\n",
    "    \"cnn\": create_cnn_model((60 * 32, 3)),\n",
    "    \"sem-mscnn\": create_semscnn_model((60 * 32, 3)),\n",
    "    \"cnn-lstm\": create_cnnlstm_model((60 * 32, 3)),\n",
    "    \"hybrid\": create_hybrid_transformer_model((60 * 32, 3)),\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(config):\n",
    "    if config[\"model_name\"].split('_')[0] == \"Transformer\":\n",
    "        return create_transformer_model(input_shape=(60 * 32, len(config[\"channels\"])),\n",
    "                                        num_patches=config[\"num_patches\"], projection_dim=config[\"transformer_units\"],\n",
    "                                        transformer_layers=config[\"transformer_layers\"], num_heads=config[\"num_heads\"],\n",
    "                                        transformer_units=[config[\"transformer_units\"] * 2,\n",
    "                                                           config[\"transformer_units\"]],\n",
    "                                        mlp_head_units=[256, 128], num_classes=1, drop_out=config[\"drop_out_rate\"],\n",
    "                                        reg=config[\"regression\"], l2_weight=config[\"regularization_weight\"])\n",
    "    else:\n",
    "        return model_dict.get(config[\"model_name\"].split('_')[0])\n",
    "\n",
    "def run_model():\n",
    "    config = {\n",
    "        \"model_name\": \"hybrid\",\n",
    "        \"regression\": False,\n",
    "\n",
    "        \"transformer_layers\": 4,  # best 5\n",
    "        \"drop_out_rate\": 0.25,\n",
    "        \"num_patches\": 20,  # best\n",
    "        \"transformer_units\": 32,  # best 32\n",
    "        \"regularization_weight\": 0.001,  # best 0.001\n",
    "        \"num_heads\": 4,\n",
    "        \"epochs\": 100,  # best\n",
    "        \"channels\": [14, 18, 19, 20],\n",
    "    }\n",
    "    model = get_model(config)\n",
    "    model.build(input_shape=(1, 60 * 32, 10))\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "We now have preprocessed, loaded data and a model architecture, so we're ready to train a model. As discussed previously, we do not provide raw data for licensing reasons. Instead, we provide the following derivations of those raw data, on a limited basis:\n",
    "\n",
    "- Preprocessed data, and \n",
    "- Pre-trained model weights\n",
    "\n",
    "To minimize resource requirements and runtime when running this notebook in this section, we will show training on a limited subset of the preprocessed data and limited number of epochs. We will also take steps to minimize resource requirements and runtime in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting code\n",
    "\n",
    "Prior to building our training loop, we need to write some utility code to allow us to manipulate training data as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "\n",
    "def _replace_final_dim(orig_x: npt.NDArray, final_dim_size: int) -> npt.NDArray:\n",
    "    '''\n",
    "    Given an NDArray `orig_x`, return a new NDArray of the same shape,\n",
    "    except with an additional dimension filled with zeros and of size\n",
    "    `final_dim_size`.\n",
    "    '''\n",
    "    orig_x_shape = orig_x.shape\n",
    "    x_transform = np.zeros(\n",
    "        (*orig_x_shape[:-1], final_dim_size)\n",
    "    )\n",
    "    assert x_transform.shape[:-1] == orig_x.shape[:-1]\n",
    "    return x_transform\n",
    "\n",
    "def transform_for_channels(x: npt.NDArray, channels: list[int]) -> npt.NDArray:\n",
    "    '''\n",
    "    Returns an NDArray whose shape is identical to that of the `x` parameter,\n",
    "    except the final dimension is of size `len(channels)`\n",
    "\n",
    "    Generally speaking, x has a shape similar to \n",
    "    `(NUM_FOLDS, 530, 1920, NUM_CHANNELS)`. In this array, each fold thus\n",
    "    has shape (530, 1920, NUM_CHANNELS).\n",
    "    \n",
    "    Since we're trying to only extract `num_channels` out into the last \n",
    "    dimension, we need to create an NDArray whose final dimension matches\n",
    "    that number of channels.\n",
    "     \n",
    "    The value returned from this function, which we'll call `x_transform`,\n",
    "    is compatible with this number of channels since its last dimension is \n",
    "    the number of channels we're trying to extract.\n",
    "    '''\n",
    "\n",
    "    num_channels = len(channels)\n",
    "    assert (len(x.shape)) == 4\n",
    "    x_transform = _replace_final_dim(\n",
    "        orig_x=x,\n",
    "        final_dim_size=num_channels,\n",
    "    )\n",
    "    return x_transform\n",
    "\n",
    "def concat_all_folds(orig: npt.NDArray, except_fold: int) -> npt.NDArray:\n",
    "    assert(except_fold) >= 0\n",
    "    assert len(orig.shape) > 0\n",
    "    # how to initialize our concatenated array:\n",
    "    # \n",
    "    # - if except_fold is 0, initialize with index 1\n",
    "    # - if except_fold is 1, initialize with index 0\n",
    "    # \n",
    "    # in either case, we want to start at index 2 since we skip one of the \n",
    "    # previous indices (0 and 1) and choose to start with the other.\n",
    "    concat = orig[1] if except_fold == 0 else orig[0]\n",
    "    for i in range(2, orig.shape[0]):\n",
    "        if i == except_fold:\n",
    "            continue\n",
    "        concat = np.concatenate((concat, orig[i]))\n",
    "    return concat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Next, the actual training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running training...\n",
      "----------\n",
      "training channel ECGSPO2...\n",
      "training with config {'data_path': '/root/data/physionet.org/nch_30x64.npz', 'model_path': 'original/weights/semscnn_ecgspo2/f', 'model_name': 'sem-mscnn_ECGSPO2', 'regression': False, 'transformer_layers': 5, 'drop_out_rate': 0.25, 'num_patches': 30, 'transformer_units': 32, 'regularization_weight': 0.001, 'num_heads': 4, 'epochs': 2, 'channels': [11, 12, 9]}, fold=0\n",
      "x=(5, 1836, 1920, 14), y_apnea=(5, 1836), y_hypopnea=(5, 1836)\n",
      "Extracting channels [11, 12, 9]\n",
      "iterating over range(0, 5) fold(s)\n",
      "Epoch 1/2\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.5972 - precision: 0.2472 - recall: 0.0121 - val_loss: 9.5304 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 21s 2s/step - loss: 0.5907 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 5.6446 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "saving model for fold 0 to original/weights/semscnn_ecgspo2/f/0\n",
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/0/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.5884 - precision: 0.2975 - recall: 0.0886 - val_loss: 13.9062 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 21s 2s/step - loss: 0.5776 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 10.6658 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "saving model for fold 1 to original/weights/semscnn_ecgspo2/f/1\n",
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.6213 - precision: 0.3626 - recall: 0.0222 - val_loss: 21.3808 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.6013 - precision: 0.5000 - recall: 0.0054 - val_loss: 16.4141 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "saving model for fold 2 to original/weights/semscnn_ecgspo2/f/2\n",
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - 18s 2s/step - loss: 0.5861 - precision: 0.3000 - recall: 0.0022 - val_loss: 5.0613 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 17s 2s/step - loss: 0.5725 - precision: 0.6667 - recall: 0.0015 - val_loss: 4.7401 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "saving model for fold 3 to original/weights/semscnn_ecgspo2/f/3\n",
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/3/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10/10 [==============================] - 19s 2s/step - loss: 0.5232 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.6198 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 15s 1s/step - loss: 0.5158 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.5871 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 0.0010\n",
      "saving model for fold 4 to original/weights/semscnn_ecgspo2/f/4\n",
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: original/weights/semscnn_ecgspo2/f/4/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Any\n",
    "import keras\n",
    "import keras.metrics\n",
    "import numpy as np\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "THRESHOLD = 1\n",
    "FOLD = 5\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50 and (epoch - 1) % 5 == 0:\n",
    "        lr *= 0.5\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train(\n",
    "        config,\n",
    "        fold: int | None = None,\n",
    "        force_retrain: bool = False\n",
    "):\n",
    "    print(f'training with config {config}, fold={fold}')\n",
    "\n",
    "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
    "\n",
    "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
    "    print(\n",
    "        f'x={x.shape}, y_apnea={y_apnea.shape}, y_hypopnea={y_hypopnea.shape}'\n",
    "    )\n",
    "    y = y_apnea + y_hypopnea\n",
    "    ########################################################################################\n",
    "    # Channel selection\n",
    "    \n",
    "    chans = config[\"channels\"]\n",
    "    x_transform = transform_for_channels(x=x, channels=chans)\n",
    "    print(f'Extracting channels {chans}')\n",
    "    max_fold = min(FOLD, x_transform.shape[0])\n",
    "    if max_fold < x_transform.shape[0]:\n",
    "        print(\n",
    "            f'WARNING: only looking at the first {max_fold} of '\n",
    "            f'{x_transform.shape[0]} total folds in X'\n",
    "        )\n",
    "    # for i in range(FOLD):\n",
    "    for i in range(max_fold):\n",
    "        x_transform[i], y[i] = shuffle(x_transform[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        if config[\"regression\"]:\n",
    "            y[i] = np.sqrt(y[i])\n",
    "            y[i][y[i] != 0] += 2\n",
    "        else:\n",
    "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "        \n",
    "        replace = x[i][:, :, chans]\n",
    "        \n",
    "        x_transform[i] = replace  # CHANNEL SELECTION\n",
    "\n",
    "    ########################################################################################\n",
    "    #\n",
    "    # The original code for this is taken from the following link:\n",
    "    # \n",
    "    # https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/6dc5ec87ef17810c461d4738dd4f46240816999c/train.py#L39-L48\n",
    "    # \n",
    "    # I (Aaron) think that in the inner loop, they're just trying to create\n",
    "    # one big NDArray with the concatenation of all the folds except for the \n",
    "    # one on which they're currently on in the outer loop.\n",
    "    # \n",
    "    # Then, they train on the concatenated array. In other words, the outer\n",
    "    # loop behaves similarly to epochs, with a small twist.\n",
    "    # \n",
    "    # They used to have the logic to do this inside the outer loop,\n",
    "    # but I pulled it out.\n",
    "    # \n",
    "    # also note, the folds selection (commented below) didn't work because \n",
    "    # they pass fold=0 into this function, which results in no training \n",
    "    # whatsoever.\n",
    "    folds = range(max_fold)\n",
    "    # folds = range(FOLD) if fold is None else range(fold)\n",
    "    print(f'iterating over {folds} fold(s)')\n",
    "    for fold in folds:\n",
    "        base_model_path = config[\"model_path\"]\n",
    "        model_path = f\"{base_model_path}/{str(fold)}\"\n",
    "        if (\n",
    "            os.path.exists(model_path) and \n",
    "            not force_retrain\n",
    "        ):\n",
    "            print(\n",
    "                f'Training fold {fold}: force_retrain==False and '\n",
    "                f'{model_path} already exists, skipping.'\n",
    "            )\n",
    "            continue\n",
    "        x_train = concat_all_folds(orig=x_transform, except_fold=fold)\n",
    "        y_train = concat_all_folds(orig=y, except_fold=fold)\n",
    "\n",
    "        model = get_model(config)\n",
    "        if config[\"regression\"]:\n",
    "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
    "            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        else:\n",
    "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy(),\n",
    "                          metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        model.fit(x=x_train, y=y_train, batch_size=512, epochs=config[\"epochs\"], validation_split=0.1,\n",
    "                  callbacks=[early_stopper, lr_scheduler])\n",
    "        ################################################################################################################\n",
    "        print(f\"saving model for fold {fold} to {model_path}\")\n",
    "        model.save(model_path)\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "\n",
    "def train_age_seperated(config):\n",
    "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
    "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
    "    y = y_apnea + y_hypopnea\n",
    "    ########################################################################################\n",
    "    for i in range(10):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        if config[\"regression\"]:\n",
    "            y[i] = np.sqrt(y[i])\n",
    "            y[i][y[i] != 0] += 2\n",
    "        else:\n",
    "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]  # CHANNEL SELECTION\n",
    "\n",
    "    ########################################################################################\n",
    "    first = True\n",
    "    for i in range(10):\n",
    "        if first:\n",
    "            x_train = x[i]\n",
    "            y_train = y[i]\n",
    "            first = False\n",
    "        else:\n",
    "            x_train = np.concatenate((x_train, x[i]))\n",
    "            y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "    model = get_model(config)\n",
    "    if config[\"regression\"]:\n",
    "        model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
    "        early_stopper = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model.compile(optimizer=\"adam\", loss=BinaryCrossentropy(),\n",
    "                      metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "        early_stopper = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=512,\n",
    "        epochs=config[\"epochs\"],\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopper, lr_scheduler]\n",
    "    )\n",
    "    ################################################################################################################\n",
    "    model.save(config[\"model_path\"] + str(0))\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "# the original repository indicated that the best value for this constant\n",
    "# is 200. we are setting it to a much lower number so we can illustrate\n",
    "# training working without requiring a large amount of computing power\n",
    "# or running for an excessively long time.\n",
    "# \n",
    "# feel free to increase this value, but be mindful that if you do, we make\n",
    "# no guarantees about required resources or expected runtime.\n",
    "NUM_EPOCHS=2\n",
    "def build_configs() -> dict[str, dict[str, Any]]:\n",
    "    sig_dict = {\"EOG\": [0, 1],\n",
    "                \"EEG\": [2, 3],\n",
    "                \"RESP\": [5, 6],\n",
    "                \"SPO2\": [9],\n",
    "                \"CO2\": [10],\n",
    "                \"ECG\": [11, 12],\n",
    "                \"DEMO\": [13],\n",
    "                }\n",
    "\n",
    "    channel_list = [\n",
    "        [\"ECG\", \"SPO2\"],\n",
    "    ]\n",
    "    configs: dict[str, dict[str, Any]] = {}\n",
    "    for ch in channel_list:\n",
    "            chs = []\n",
    "            chstr = \"\"\n",
    "            for name in ch:\n",
    "                chstr += name\n",
    "                chs = chs + sig_dict[name]\n",
    "            if chstr in configs:\n",
    "                raise ValueError(\n",
    "                    f\"tried to create a config for channel {chstr}, \"\n",
    "                    \"but one already existed\"\n",
    "                )\n",
    "            configs[chstr] = {\n",
    "                \"data_path\": PREPROCESS_OUT_PATH,\n",
    "                \"model_path\": MODEL_OUT_PATH,\n",
    "                \"model_name\": \"sem-mscnn_\" + chstr,\n",
    "                \"regression\": False,\n",
    "\n",
    "                \"transformer_layers\": 5,  # best 5\n",
    "                \"drop_out_rate\": 0.25,  # best 0.25\n",
    "                \"num_patches\": 30,  # best 30 TBD\n",
    "                \"transformer_units\": 32,  # best 32\n",
    "                \"regularization_weight\": 0.001,  # best 0.001\n",
    "                \"num_heads\": 4,\n",
    "                \"epochs\": NUM_EPOCHS,\n",
    "                \"channels\": chs,\n",
    "            }\n",
    "    return configs\n",
    "\n",
    "def run_training():\n",
    "\n",
    "    configs = build_configs()\n",
    "\n",
    "    for chstr, config in configs.items():\n",
    "        print(\n",
    "            f\"----------\\n\"\n",
    "            f\"training channel {chstr}...\"\n",
    "        )\n",
    "        train(config=config, fold=0, force_retrain=True)\n",
    "        print(\"done.\")\n",
    "\n",
    "if SKIP_TRAINING:\n",
    "    print(\"SKIP_TRAINING is set to True, so not running training loop\")\n",
    "else:\n",
    "    print('running training...')\n",
    "    run_training()\n",
    "    print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "In the above training loop, the number of epochs was set at 200.  A Keras [`EarlyStopping`](https://keras.io/api/callbacks/early_stopping/) callback was used to bailout from training if the value loss does not improve for 10 epochs.  Thus, while the *maximum* epochs were 200, the *actual* epochs used to train a model were typically around 70.\n",
    "\n",
    "The starting learning rate is `0.0010`, and learning rates on an after epoch 52 were reduced using a Keras [`LearningRateScheduler`](https://keras.io/api/callbacks/learning_rate_scheduler/).  At epoch 52 and every 5 epochs thereafter, the learning rate is halved.  Thus, the learning rate schedule looks like the following after the first 3 adjustments:\n",
    "\n",
    "| Epoch range | Learning Rate |\n",
    "| -- | -- |\n",
    "| 1-51 | 0.0010 |\n",
    "| 52-56 | 0.0005 |\n",
    "| 57-62 | 0.00025 |\n",
    "\n",
    "The subsequent adjustments continue in this pattern as expected. This learning rate decay, along with the `EarlyStopping` bailout, undoubtedly contributed to the limited number of epochs actually used.\n",
    "\n",
    "### Other Hyperparameters Used\n",
    "\n",
    "In addition to the above hyperparameter settings, we also did training+testing runs with the following combinations:\n",
    "\n",
    ">In all cases, the LR Decay was used with the `LearningRateScheduler` as above.\n",
    "\n",
    "| Num. Epochs | Starting Learning Rate | LR Decay | Additional Notes |\n",
    "| -- | -- | -- | -- |\n",
    "| 2 | | | Primarily used for local testing on limited computational resources |\n",
    "| 50 | | | Done on an 8-core CPU on a DigitalOcean Droplet VM |\n",
    "| 100 | | | Performed both on an 8-core CPU and on an NVIDIA H-100 GPU, both cases on virtualized cloud systems |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Computational Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Model checkpoint generation was performed on several hardware configurations, including the following:\n",
    "\n",
    "### Windows 11 PC\n",
    "\n",
    "In this configuration, training was done on Ubuntu running inside a [Windows Subsystem of Linux](https://learn.microsoft.com/en-us/windows/wsl/about) virtual machine (VM). This setup had the following hardware specifications:\n",
    "\n",
    "- Hardware CPU: AMD Ryzen 7 3800X\n",
    "    - 8 physical cores, up to 4.2 GHz\n",
    "    - 16 virtual processors\n",
    "- Hardware GPU: NVIDIA RTX 4070Ti\n",
    "    - 12GB of GPU RAM\n",
    "    - Up to 44GB shared RAM\n",
    "- Hardware RAM: 64GB\n",
    "- Storage: 4TB SSD\n",
    "\n",
    "### DigitalOcean VM\n",
    "\n",
    "In this configuration, training was done on Ubuntu running on a [DigitalOcean](https://www.digitalocean.com) cloud VM (called a \"droplet\"). Since this is a cloud-based VM, there are relatively few guarantees about the underlying hardware. This setup had the following known specification:\n",
    "\n",
    "- Virtual CPU: 8-core generic AMD\n",
    "- Virtual GPU: None\n",
    "- RAM: 32GB\n",
    "- Storage: 400GB SSD\n",
    "\n",
    "### Paperspace VM\n",
    "\n",
    "In this configuration, training was done on Ubuntu running on a [Paperspace](https://www.paperspace.com) AI/ML-optimized cloud VM. Since this is a cloud-based VM, there are relatively few guarantees made about the underlying hardware, similar to the DigitalOcean case above. This setup had the following known specification:\n",
    "\n",
    "- Virtual CPU: 8-core generic\n",
    "- Virtual GPU: NVIDIA P4000\n",
    "    - 8GB GPU RAM\n",
    "- RAM: 30GB\n",
    "- Storage: 1TB SSD\n",
    "\n",
    "### Overview of Computations\n",
    "\n",
    "Generally speaking, computations were performed on a 500GB subset of the NCH dataset.  This consumed the nearly entire system RAM in all the above hardware configurations during preprocessing. Similarly, in the GPU-enabled setups above, all the GPU-dedicated RAM was used during model training.\n",
    "\n",
    "In the Windows setup, each epoch of each fold lasted approximately 2 seconds.  The total combined training and testing time per model, including data loading and model saving was approximately 14 minutes.  For the total 63 ablations computed that equates to approximately 882 minutes or 14.7 hours of total compute time, most of which was nearly 100% GPU-bound.\n",
    "\n",
    "In the DigitalOcean setup described above, training and testing took significantly longer, and in the Paperspace setup, it took moderately longer, although we did not do an apples-to-apples comparison. Further, since the latter two configurations were cloud-based systems, we're unable to guarantee exclusive access to the underlying hardware and thus unable to guarantee a stable experiment in those cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results and evaluations\n",
    "\n",
    "In a traditional machine learning pipeline, we would train a model and then immediately evaluate it on a test data set. As with the last section, however, we take steps to minimize resource requirements and runtime. To that end, we have run training on the full dataset and saved the resulting model's weights (which you can find in our repository's [`original/weights`](https://github.com/arschles/UIUC-CS598-DLH-Project/tree/main/original/weights) directory).\n",
    "\n",
    "In this section, however, we've built the option to instantiate our model from those weights, then evaluate it. The results of this evaluation determine how well we were able to reproduce the results claimed in the original paper.\n",
    "\n",
    "The code shown in this section tests our preloaded model. As with the previous section, testing and evaluation code is extensive, so we split it into two separate sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics reporting and calculation\n",
    "\n",
    "Below, we have utility code for metrics collection and reporting. This code will be used by subsequent evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, average_precision_score, roc_auc_score\n",
    "\n",
    "\n",
    "class FromLogitsMixin:\n",
    "    def __init__(self, from_logits=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if self.from_logits:\n",
    "            y_pred = tf.nn.sigmoid(y_pred)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class AUC(FromLogitsMixin, tf.metrics.AUC):\n",
    "    ...\n",
    "\n",
    "\n",
    "class BinaryAccuracy(FromLogitsMixin, tf.metrics.BinaryAccuracy):\n",
    "    ...\n",
    "\n",
    "\n",
    "class TruePositives(FromLogitsMixin, tf.metrics.TruePositives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class FalsePositives(FromLogitsMixin, tf.metrics.FalsePositives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class TrueNegatives(FromLogitsMixin, tf.metrics.TrueNegatives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class FalseNegatives(FromLogitsMixin, tf.metrics.FalseNegatives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Precision(FromLogitsMixin, tf.metrics.Precision):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Recall(FromLogitsMixin, tf.metrics.Recall):\n",
    "    ...\n",
    "\n",
    "\n",
    "class F1Score(FromLogitsMixin, tfa.metrics.F1Score):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Result:\n",
    "    def __init__(self):\n",
    "        self.accuracy_list = []\n",
    "        self.sensitivity_list = []\n",
    "        self.specificity_list = []\n",
    "        self.f1_list = []\n",
    "        self.auroc_list = []\n",
    "        self.auprc_list = []\n",
    "        self.precision_list = []\n",
    "\n",
    "    def add(self, y_test, y_predict, y_score):\n",
    "        C = confusion_matrix(y_test, y_predict, labels=(1, 0))\n",
    "        TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "\n",
    "        acc, sn, sp, pr = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP), 1. * TP / (\n",
    "                TP + FP)\n",
    "        acc = 1. * (TP + TN) / (TP + TN + FP + FN)\n",
    "        sn = 1. * TP / (TP + FN)\n",
    "        sp = 1. * TN / (TN + FP)\n",
    "        pr = 1. * TP / (TP + FP) if TP + FP != 0 else 0 # define precision to be zeero if there are NO positive predictions\n",
    "        f1 = f1_score(y_test, y_predict)\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "        auprc = average_precision_score(y_test, y_score)\n",
    "\n",
    "        self.accuracy_list.append(acc * 100)\n",
    "        self.precision_list.append(pr * 100)\n",
    "        self.sensitivity_list.append(sn * 100)\n",
    "        self.specificity_list.append(sp * 100)\n",
    "        self.f1_list.append(f1 * 100)\n",
    "        self.auroc_list.append(auc * 100)\n",
    "        self.auprc_list.append(auprc * 100)\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        out_str = \"=========================================================================== \\n\"\n",
    "        out_str += str(self.accuracy_list) + \" \\n\"\n",
    "        out_str += str(self.precision_list) + \" \\n\"\n",
    "        out_str += str(self.sensitivity_list) + \" \\n\"\n",
    "        out_str += str(self.specificity_list) + \" \\n\"\n",
    "        out_str += str(self.f1_list) + \" \\n\"\n",
    "        out_str += str(self.auroc_list) + \" \\n\"\n",
    "        out_str += str(self.auprc_list) + \" \\n\"\n",
    "        out_str += str(\"Accuracy: %.2f -+ %.3f\" % (np.mean(self.accuracy_list), np.std(self.accuracy_list))) + \" \\n\"\n",
    "        out_str += str(\"Precision: %.2f -+ %.3f\" % (np.mean(self.precision_list), np.std(self.precision_list))) + \" \\n\"\n",
    "        out_str += str(\n",
    "            \"Recall: %.2f -+ %.3f\" % (np.mean(self.sensitivity_list), np.std(self.sensitivity_list))) + \" \\n\"\n",
    "        out_str += str(\n",
    "            \"Specifity: %.2f -+ %.3f\" % (np.mean(self.specificity_list), np.std(self.specificity_list))) + \" \\n\"\n",
    "        out_str += str(\"F1: %.2f -+ %.3f\" % (np.mean(self.f1_list), np.std(self.f1_list))) + \" \\n\"\n",
    "        out_str += str(\"AUROC: %.2f -+ %.3f\" % (np.mean(self.auroc_list), np.std(self.auroc_list))) + \" \\n\"\n",
    "        out_str += str(\"AUPRC: %.2f -+ %.3f\" % (np.mean(self.auprc_list), np.std(self.auprc_list))) + \" \\n\"\n",
    "\n",
    "        out_str += str(\"$ %.1f \\pm %.1f$\" % (np.mean(self.accuracy_list), np.std(self.accuracy_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.precision_list), np.std(self.precision_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.sensitivity_list), np.std(self.sensitivity_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.f1_list), np.std(self.f1_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.auroc_list), np.std(self.auroc_list))) + \"& \"\n",
    "\n",
    "        return out_str\n",
    "\n",
    "    def print(self):\n",
    "        print(self.get())\n",
    "\n",
    "    def save(self, path, config):\n",
    "        enclosing_dir = os.path.dirname(path)\n",
    "        if not os.path.exists(enclosing_dir):\n",
    "            print(\n",
    "                f'directory {enclosing_dir} did not exist, creating it for '\n",
    "                'result saving'\n",
    "            )\n",
    "            os.makedirs(enclosing_dir)\n",
    "        \n",
    "        file = open(path, \"w+\")\n",
    "        file.write(str(config))\n",
    "        file.write(\"\\n\")\n",
    "        file.write(self.get())\n",
    "        file.flush()\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the pretrained model\n",
    "\n",
    "The `TEST_FROM_CHECKPOINT` flag indicates whether we should test from a model loaded from pretrained weights (`True`), or the model we just tested (`False`). If that flag is set to `True`, we expect a zip archive of the weights to be downloadable from the value in `CHECKPOINT_URL`. The code below checks for the flag and downloads weights if necessary.\n",
    "\n",
    "The default configuration from the beginning of this notebook is set up to download a pre-trained model rather than using the model trained in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_FROM_CHECKPOINT is True, downloading https://dlhproject.sfo3.cdn.digitaloceanspaces.com/weights-2024-14-14.zip and saving to original/weights/semscnn_ecgspo2/f\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "\n",
    "\n",
    "if TEST_FROM_CHECKPOINT:\n",
    "    print(\n",
    "        f'TEST_FROM_CHECKPOINT is True, downloading {CHECKPOINT_URL} '\n",
    "        f'and saving to {MODEL_OUT_PATH}'\n",
    "    )\n",
    "    zip_path = \"./model.zip\"\n",
    "    # first download the zip to the current working dir\n",
    "    urlretrieve(CHECKPOINT_URL, zip_path)\n",
    "    if not os.path.exists(MODEL_OUT_PATH):\n",
    "        print(f'{MODEL_OUT_PATH} did not exist, creating')\n",
    "        os.makedirs(MODEL_OUT_PATH)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('original')\n",
    "    os.remove(zip_path)\n",
    "\n",
    "else:\n",
    "    if not os.path.exists(MODEL_OUT_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"TEST_FROM_CHECKPOINT is False, but {MODEL_OUT_PATH} was not \"\n",
    "            f\"found. please make sure model training succeeded\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics calculation\n",
    "\n",
    "The below code evaluates the model and reports it using the above reporting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T15:21:22.529526Z",
     "start_time": "2024-04-24T15:21:05.010656Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_configs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 146\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m data, x_test, y_test, model, predict, y_score, y_predict\n\u001b[0;32m--> 146\u001b[0m configs \u001b[38;5;241m=\u001b[39m build_configs()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chstr, config \u001b[38;5;129;01min\u001b[39;00m configs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting on channel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchstr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_configs' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "THRESHOLD = 1\n",
    "FOLD = 5\n",
    "\n",
    "def add_noise_to_data(a):\n",
    "    \"\"\"\n",
    "    Stand-in function for adding noise to data.\n",
    "\n",
    "    This function is used in the codebase, hidden behind a\n",
    "    config flag, and not defined anywhere in the original research\n",
    "    repository, so we have defined it here as the identity function,\n",
    "    just so things run properly.\n",
    "    \"\"\"\n",
    "    print(\"WARNING: 'test_noise_snr' config option is a no-op\")\n",
    "    return a\n",
    "\n",
    "\n",
    "def test(config: dict[str, str], fold=None):\n",
    "    data_path = config['data_path']\n",
    "    print(f'testing from {data_path}')\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "    ############################################################################\n",
    "    x, y_apnea, y_hypopnea = data[\"x\"], data[\"y_apnea\"], data[\"y_hypopnea\"]\n",
    "    x_transform = transform_for_channels(x=x, channels=config[\"channels\"])\n",
    "    y = y_apnea + y_hypopnea\n",
    "\n",
    "    max_fold = min(FOLD, x_transform.shape[0])\n",
    "    if max_fold < x_transform.shape[0]:\n",
    "        print(\n",
    "            f'WARNING: only looking at the first {max_fold} of '\n",
    "            f'{x_transform.shape[0]} total folds in X'\n",
    "        )\n",
    "\n",
    "    # for i in range(FOLD):\n",
    "    for i in range(max_fold):\n",
    "        x_transform[i], y[i] = shuffle(x_transform[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "        x_transform[i] = x[i][:, :, config[\"channels\"]]\n",
    "    ############################################################################\n",
    "    result = Result()\n",
    "    # folds = range(FOLD) if fold is None else [fold]\n",
    "    folds = range(max_fold)\n",
    "    for fold in folds:\n",
    "        base_model_path = config[\"model_path\"]\n",
    "        model_path = f\"{base_model_path}/{str(fold)}\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(\n",
    "                f\"WARNING: model path {model_path} does not exist, skipping!\"\n",
    "            )\n",
    "            continue\n",
    "        x_test = x_transform[fold]\n",
    "        # NOTE: this config key is not set in both `main_chat.py` and\n",
    "        # `main_nch.py`. if it were, the code under this `if` would fail\n",
    "        # because there is no `add_noise_to_data` function in this repository.\n",
    "        if config.get(\"test_noise_snr\"):\n",
    "           x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
    "\n",
    "        y_test = y[\n",
    "            fold\n",
    "        ]  # For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\n",
    "        model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "        predict = model.predict(x_test)\n",
    "        y_score = predict\n",
    "        y_predict = np.where(\n",
    "            predict > 0.5, 1, 0\n",
    "        )  # For MultiClass np.argmax(y_score, axis=-1)\n",
    "\n",
    "        result.add(y_test, y_predict, y_score)\n",
    "\n",
    "    print(\n",
    "        '\\n----------\\n'\n",
    "        'results:\\n'\n",
    "    )\n",
    "    result.print()\n",
    "    model_name = config[\"model_name\"]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\") # Format the date and time as a string \"YYYYMMDD-HH:mm\"\n",
    "    results_file = os.path.join('results', f\"{model_name}-{timestamp}.txt\")\n",
    "    print(\n",
    "        f'done, saving to {results_file}\\n'\n",
    "        '----------\\n'\n",
    "    )\n",
    "\n",
    "    result.save(path=results_file, config=config)\n",
    "\n",
    "    del data, x_test, y_test, model, predict, y_score, y_predict\n",
    "\n",
    "\n",
    "def test_age_seperated(config):\n",
    "    x = []\n",
    "    y_apnea = []\n",
    "    y_hypopnea = []\n",
    "    for i in range(10):\n",
    "        data = np.load(config[\"data_path\"] + str(i) + \".npz\", allow_pickle=True)\n",
    "        x.append(data[\"x\"])\n",
    "        y_apnea.append(data[\"y_apnea\"])\n",
    "        y_hypopnea.append(data[\"y_hypopnea\"])\n",
    "    ############################################################################\n",
    "    y = np.array(y_apnea) + np.array(y_hypopnea)\n",
    "    for i in range(10):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]\n",
    "    ############################################################################\n",
    "    result = Result()\n",
    "\n",
    "    for fold in range(10):\n",
    "        x_test = x[fold]\n",
    "        if config.get(\"test_noise_snr\"):\n",
    "            x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
    "\n",
    "        y_test = y[\n",
    "            fold\n",
    "        ]  # For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\n",
    "\n",
    "        model = tf.keras.models.load_model(config[\"model_path\"] + str(0), compile=False)\n",
    "\n",
    "        predict = model.predict(x_test)\n",
    "        y_score = predict\n",
    "        y_predict = np.where(\n",
    "            predict > 0.5, 1, 0\n",
    "        )  # For MultiClass np.argmax(y_score, axis=-1)\n",
    "\n",
    "        result.add(y_test, y_predict, y_score)\n",
    "\n",
    "    result.print()\n",
    "    save_file = os.path.join('results', config[\"model_name\"], \".txt\")\n",
    "    result.save(save_file, config)\n",
    "    print(f\"results saved to {save_file}\")\n",
    "\n",
    "\n",
    "    del data, x_test, y_test, model, predict, y_score, y_predict\n",
    "\n",
    "configs = build_configs()\n",
    "for chstr, config in configs.items():\n",
    "\n",
    "    print(f'testing on channel {chstr}')\n",
    "    test(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Table of Results\n",
    "| CO2 | ECG | EEG | EOG | RESP | SPO2 | Mean F1 | StdDev F1 | Mean AUROC | StdDev AUROC |\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "| X | | | | | | 72.00 | 4.922 | 76.21 | 5.277 |\n",
    "| | X | | | | | 73.61 | 2.884 | 82.13 | 2.299 |\n",
    "| | | X | | | | 72.16 | 2.844 | 79.56 | 2.664 |\n",
    "| | | | X | | | 75.62 | 2.737 | 82.17 | 2.165 |\n",
    "| | | | | X | | 74.54 | 4.417 | 83.57 | 3.399 |\n",
    "| | | | | | X | 80.80 | 3.011 | 89.27 | 2.300 |\n",
    "| X | X | | | | | 73.80 | 4.433 | 81.44 | 4.658 |\n",
    "| X | | X | | | | 72.79 | 3.511 | 79.51 | 3.396 |\n",
    "| | X | X | | | | 74.56 | 2.322 | 83.08 | 1.852 |\n",
    "| X | | | X | | | 74.33 | 3.536 | 81.40 | 3.701 |\n",
    "| | X | | X | | | 76.07 | 2.281 | 84.39 | 1.976 |\n",
    "| | | X | X | | | 76.84 | 2.440 | 83.51 | 2.162 |\n",
    "| | | X | | X | | 76.28 | 4.883 | 85.38 | 2.884 |\n",
    "| | | X | | | X | 81.08 | 2.699 | 89.43 | 2.242 |\n",
    "| | | | X | X | | 77.63 | 4.331 | 87.19 | 1.894 |\n",
    "| | | | X | | X | 81.00 | 2.347 | 89.55 | 1.932 |\n",
    "| X | | | | X | | 72.08 | 5.568 | 79.59 | 5.236 |\n",
    "| | X | | | X | | 77.33 | 3.228 | 86.06 | 2.001 |\n",
    "| X | | | | | X | 81.38 | 2.867 | 89.59 | 2.099 |\n",
    "| | X | | | | X | 81.80 | 2.435 | 89.66 | 1.994 |\n",
    "| | | | | X | X | 80.83 | 2.626 | 89.04 | 2.036 |\n",
    "| X | X | X | X | X | X | 79.18 | 3.553 | 87.48 | 3.070 |\n",
    "\n",
    "These results, while not a decimal-for-decimal duplication are, overall, very close to the paper's results and particularly consistent with the trends shown in the original paper, as demonstrated by the following comparison graph.\n",
    "\n",
    "![Result Comparison](original/results/plots/eval.png)\n",
    "\n",
    ">Code to generate this the the below data visualizations is elided from this notebook for clarity and brevity purposes, and can be found in our repository at [`original/eval/plot.py`](https://github.com/arschles/UIUC-CS598-DLH-Project/blob/main/original/eval/plot.py). We have provided rendered graph images instead.\n",
    "\n",
    "In this figure, our results are shown in shades of orange with the original paper's results shown in shades of blue.\n",
    "\n",
    "While the mean F1 score and the mean AUROC score are close, our results differ more substantially in the standard deviation of each value.\n",
    "\n",
    "Some key differences account for the differences in the final calculated statistics, however.  Chiefly among these, the size of the data set used for training differed substantially.  The original paper used the entire NCH dataset, but due to bandwidth limitations on [physionet.org](https://physionet.org) (the repository for these data), we were only able to use a roughly 500GB subset of the data, which was selected by the first files the `wget` command (the tool used to download these data) fetched.\n",
    "\n",
    "Thus, we do not truly know if this was a random sample, a chronological sample, or if our dataset was skewed by some underlying sampling bias inadvertently introduced by the wget command.  The quality of the results overall seems to suggest it was random, but without much more analysis of the included data against the broader dataset, that question remains open.\n",
    "\n",
    "It is logical to conclude that a larger overall training set would make each fold proportionally larger and, thus, lead to more consistent results across each combination of folds and, in turn, lead to a smaller standard deviation for F1 and AUROC scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Results Beyond the Original Paper\n",
    "\n",
    "### More Ablations\n",
    "\n",
    "We undertook to complete nearly all possible ablations, including those not present in the paper.  The paper included combinations of one signal, combinations of two signals, and all six signals.  We ran additional ablations on combinations of three, four, and five signals.  Our results are shown in the table as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "| CO2 | ECG | EEG | EOG | RESP | SPO2 | Mean F1 | StdDev F1 | Mean AUROC | StdDev AUROC |\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "| X | X | X | | | | 74.49 | 3.478 | 82.04 | 4.030 |\n",
    "| X | X | | X | | | 74.44 | 3.343 | 82.29 | 3.989 |\n",
    "| X | | X | X | | | 74.22 | 3.359 | 80.93 | 3.569 |\n",
    "| | X | X | X | | | 76.65 | 2.167 | 84.78 | 1.793 |\n",
    "| X | | X | | X | | 76.37 | 4.227 | 83.98 | 3.874 |\n",
    "| | X | X | | X | | 77.79 | 2.897 | 86.49 | 1.804 |\n",
    "| X | | X | | | X | 81.60 | 2.916 | 89.71 | 2.094 |\n",
    "| | X | X | | | X | 81.88 | 2.279 | 89.87 | 1.831 |\n",
    "| | | X | X | X | | 78.76 | 2.905 | 87.45 | 1.610 |\n",
    "| | | X | X | | X | 81.14 | 2.704 | 89.55 | 1.911 |\n",
    "| X | | | X | X | | 76.19 | 3.918 | 84.50 | 2.507 |\n",
    "| | X | | X | X | | 78.36 | 2.863 | 86.82 | 1.608 |\n",
    "| X | | | X | | X | 81.19 | 3.142 | 89.48 | 2.236 |\n",
    "| | X | | X | | X | 81.60 | 2.146 | 89.82 | 1.627 |\n",
    "| X | X | | | X | | 74.29 | 4.232 | 82.43 | 3.876 |\n",
    "| X | X | | | | X | 81.62 | 3.072 | 89.62 | 2.180 |\n",
    "| | | X | | X | X | 81.00 | 2.366 | 89.36 | 1.729 |\n",
    "| | | | X | X | X | 80.84 | 2.865 | 89.15 | 1.924 |\n",
    "| X | | | | X | X | 80.42 | 3.499 | 88.67 | 2.585 |\n",
    "| | X | | | X | X | 81.90 | 2.420 | 89.61 | 1.869 |\n",
    "| X | X | X | X | | | 74.73 | 3.362 | 82.74 | 3.694 |\n",
    "| X | X | X | | X | | 75.89 | 3.345 | 84.19 | 2.690 |\n",
    "| X | X | X | | | X | 80.99 | 2.992 | 88.71 | 2.356 |\n",
    "| X | | X | X | X | | 75.50 | 3.927 | 84.15 | 2.685 |\n",
    "| | X | X | X | X | | 78.58 | 2.619 | 87.31 | 1.711 |\n",
    "| X | | X | X | | X | 81.22 | 3.136 | 89.54 | 2.013 |\n",
    "| | X | X | X | | X | 81.68 | 2.455 | 89.85 | 1.945 |\n",
    "| X | X | | X | X | | 75.83 | 4.202 | 83.62 | 4.358 |\n",
    "| X | X | | X | | X | 81.14 | 3.400 | 89.08 | 2.620 |\n",
    "| X | | X | | X | X | 80.60 | 3.470 | 88.52 | 3.042 |\n",
    "| | X | X | | X | X | 81.49 | 2.473 | 89.42 | 2.118 |\n",
    "| | | X | X | X | X | 80.83 | 2.413 | 89.08 | 1.986 |\n",
    "| X | | | X | X | X | 80.57 | 3.141 | 88.57 | 2.620 |\n",
    "| | X | | X | X | X | 81.57 | 2.468 | 89.71 | 1.879 |\n",
    "| X | X | | | X | X | 81.11 | 2.550 | 89.06 | 2.070 |\n",
    "| X | X | X | X | X | | 75.00 | 3.049 | 83.11 | 2.490 |\n",
    "| X | X | X | X | | X | 80.76 | 2.680 | 88.90 | 1.927 |\n",
    "| X | X | X | | X | X | 79.72 | 3.984 | 87.97 | 2.775 |\n",
    "| X | | X | X | X | X | 80.60 | 3.278 | 88.38 | 2.851 |\n",
    "| | X | X | X | X | X | 80.72 | 2.090 | 89.13 | 1.657 |\n",
    "| X | X | | X | X | X | 80.70 | 3.014 | 88.86 | 2.190 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When combined with our previous results (those also presented in the original paper), a clear trend is formed when all the signal combinations are plotted in a sorted-order with single signals, then two signals, then three, and so on.  \n",
    "\n",
    "![All Ablations](original/results/plots/all_ablations.png)\n",
    "\n",
    "While some signals tend to be more clearly correlated with higher scores, the overall trend is that as the count of signals increases, the overall result is better. \n",
    "\n",
    "### Score Correlation of Individual Signals\n",
    "\n",
    "Seeing the overall trends and that some signals were more \"helpful\" than others led us to ask whether there are signal combinations that perform better than the one selected in the paper (SPO2 and ECG).\n",
    "\n",
    "To answer this question, we undertook to determine a Spearman rank correlation coefficient between the F1 and AUROC scores and the signals.  This to the following results:\n",
    "\n",
    "![Correlation Results Heatmap](original/results/plots/correlation_hm.png)\n",
    " \n",
    "![P-Value Results Heatmap](original/results/plots/p_value_hm.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Based on these results and in terms of mean F1 score, SPO2 has the strongest correlation to a high score (correlation coefficient: 0.87) with ECG being a distant second (correlation coefficient: 0.09) and EOG being third (correlation coefficient: 0.04).  The remaining signals demonstrate correlation with *lower* scores.  AUROC scores follow the same pattern with slightly different correlation coefficients.    \n",
    "\n",
    "We can also see that the results of the SPO2 correlation are definitely non-random based on the p-score in that case of 0 for both F1 and AUROC.  CO2 is definitely non-random for AUROC and for F1 it slightly exceeds the standard significantly-significant threshold of 0.05.  CO2 is likely non-random due to the low p-value, despite that it does not meet the commonly accepted threshold.  The other signals cannot disprove the null-hypothesis.\n",
    "\n",
    "Overall, we can see that the original paper's selection of SPO2 and ECG captured the two largest contributors to high-scores.  Our results showed that a combination of ECG, EEG, and SPO2 yielded the best results for F1 scores (81.88), but based on the p-value of the EEG signal for F1 scores (0.98), we refrain from assuming that this is would be true if trained on the entire NCH dataset due to the high likelihood that the EEG contribution is random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "Since we were unable to acquire all the data used in the paper, and thus were not able to exactly reproduce the results outlined in the paper, we cannot assert with absolute certainty that this research is reproducible. However, the work we did with the data we were able to acquire indicates the paper would be reproducible in its entirety, were we to have access to the entire dataset.\n",
    "\n",
    "Further, the 63 total ablations (over 40 of which were beyond the scope of the paper) and additional statistical analyses we detailed in the previous section suggests the original research from the paper is sound.\n",
    "\n",
    "As a result, we _believe_ the research described in this paper is reproducible but cannot claim to know it is with complete confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What was easy and what was hard\n",
    "\n",
    "There was a substantial amount of open-source code written specifically to reproduce the findings in this paper. This code helped us make fast progress toward reproducing this research.\n",
    "\n",
    "On the other hand, however, we did encounter a significant number of non-trivial issues in the process of reproducing this paper, but were ultimately able to surmount them. Some of these issues have been mentioned or implied previously, but are summarized as follows:\n",
    "\n",
    "- Data necessary to reproduce this paper in its entirety are either impossible or very difficult, practically speaking, to acquire in a reasonable amount of time (less than a few weeks)\n",
    "- Most of the code is undocumented\n",
    "- Important parts of the codebase are non-functionan, with any Python interpreter with which we're familiar\n",
    "- Parts of their codebase do not match exactly the architecture discussed in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for improvement\n",
    "\n",
    "We estimate that data acquisition problems are outside the authors' control, so we focus our suggestions for improvement herein on code quality. Below are the highest-impact tasks we would suggest the authors complete to improve their code and overall open-source repository for future contributors:\n",
    "\n",
    "- Add clear documentation to each public Python function, including descriptions of parameters and return values\n",
    "- Add [Python type hints](https://docs.python.org/3/library/typing.html) to at least the function parameters and return values of public functions\n",
    "- Add complete information about the expected runtime environment, including required Python version(s) and dependency versions (including transitive dependencies)\n",
    "    - We used [Conda](https://conda.io/projects/conda/en/latest/index.html) to manage this information and would suggest the authors use this system as well, if they don't already have a preferred system.\n",
    "- Do an audit to ensure that model architecture and evaluations match those described in the paper\n",
    "- Add at least minimal testing to ensure all code runs successfully. Ideas for tests include:\n",
    "    - Construct the model, load it from pretrained weights, and do several inferences. Use this test to ensure the model runs with some minimal quality bar\n",
    "    - Train the model for a small number of epochs to make sure loss is established and gradient descent/backpropagation works properly and loss begins decreasing\n",
    "    - Preprocess and load one sleep study to ensure preprocessing/data loading code runs properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-draft Plans for improvement \n",
    "\n",
    "Between the draft due date and the final project due date, we planned to improve this notebook along several axes:\n",
    "\n",
    "- Include more evaluations and visual aids to illustrate them (i.e. charts, graphs, etc...)\n",
    "- Run at least one of the ablation studies from the paper and determine whether we reach the same conclusion as in the paper\n",
    "\n",
    "We believe we met and surpassed these goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "Fayyaz H, Strang A, Beheshti R. Bringing At-home Pediatric Sleep Apnea Testing Closer to Reality: A Multi-modal Transformer Approach. Proc Mach Learn Res. 2023;219:167-185.\n",
    "\n",
    "Marcus, Carole L et al. â€œA randomized trial of adenotonsillectomy for childhood sleep apnea.â€ The New England journal of medicine vol. 368,25 (2013): 2366-76. doi:10.1056/NEJMoa1215881\n",
    "\n",
    "Lee H, Li B, DeForte S, et al. A large collection of real-world pediatric sleep studies. Sci Data. 2022;9(1):421. Published 2022 Jul 19. doi:10.1038/s41597-022-01545-6"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5",
     "timestamp": 1709153069464
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
