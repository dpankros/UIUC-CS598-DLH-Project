{"cells":[{"cell_type":"markdown","metadata":{"id":"j01aH0PR4Sg-"},"source":["# Before you use this template\n","\n","This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n","\n","---\n","\n","# FAQ and Attentions\n","* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n","* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n","* any report must have run-able codes and necessary annotations (in text and code comments).\n","* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n","must be within 8 min, otherwise, you may get penalty on the grade.\n","  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n","  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n","  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n","* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n","* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n","* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."]},{"cell_type":"markdown","metadata":{"id":"dlv6knX04FiY"},"source":["# Mount Notebook to Google Drive\n","Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n","\n","Instruction: https://colab.research.google.com/notebooks/io.ipynb\n","\n","Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n","\n","Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sfk8Zrul_E8V"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"MQ0sNuMePBXx"},"source":["# Introduction\n","This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n","\n","*   Background of the problem\n","  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n","  * what is the importance/meaning of solving the problem\n","  * what is the difficulty of the problem\n","  * the state of the art methods and effectiveness.\n","*   Paper explanation\n","  * what did the paper propose\n","  * what is the innovations of the method\n","  * how well the proposed method work (in its own metrics)\n","  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ABD4VhFZbehA"},"outputs":[],"source":["# code comment is used as inline annotations for your coding"]},{"cell_type":"markdown","metadata":{"id":"uygL9tTPSVHB"},"source":["# Scope of Reproducibility:\n","\n","List hypotheses from the paper you will test and the corresponding experiments you will run.\n","\n","\n","1.   Hypothesis 1: xxxxxxx\n","2.   Hypothesis 2: xxxxxxx\n","\n","You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n","\n","![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"]},{"cell_type":"markdown","metadata":{"id":"LM4WUjz64C3B"},"source":["\n","You can also use code to display images, see the code below.\n","\n","The images must be saved in Google Drive first.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rRksCB1vbYwJ"},"outputs":[{"data":{"text/plain":["'\\nif you want to use an image outside this notebook for explanaition,\\nyou can upload it to your google drive and show it with OpenCV or matplotlib\\n'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# no code is required for this section\n","'''\n","if you want to use an image outside this notebook for explanaition,\n","you can upload it to your google drive and show it with OpenCV or matplotlib\n","'''\n","# mount this notebook to your google drive\n","# drive.mount('/content/gdrive')\n","\n","# define dirs to workspace and data\n","# img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n","\n","# import cv2\n","# img = cv2.imread(img_dir)\n","# cv2.imshow(\"Title\", img)\n"]},{"cell_type":"markdown","metadata":{"id":"xWAHJ_1CdtaA"},"source":["# Methodology\n","\n","This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n","\n","The methodology at least contains two subsections **data** and **model** in your experiment."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"yu61Jp1xrnKk"},"outputs":[],"source":["# import  packages you need\n","# import numpy as np\n","# from google.colab import drive\n"]},{"cell_type":"markdown","metadata":{"id":"2NbPHUTMbkD3"},"source":["#  Data\n","Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n","  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n","  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n","  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n","  * Illustration: printing results, plotting figures for illustration.\n","  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."]},{"cell_type":"markdown","metadata":{},"source":["## Additional files\n","\n","Consider looking at [`./main_chat.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/main_chat.py) and [`./main_nch.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/main_nch.py) to see how each of the CHAT and NCH sections fit together."]},{"cell_type":"markdown","metadata":{},"source":["## Read-only containers\n","\n","First, we define a helper class for storing immutable data."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from collections.abc import Mapping\n","\n","class ReadOnlyDict[K, V](Mapping[K, V]):\n","\n","    def __init__(self, data: dict[K, V]):\n","        self._data = data\n","\n","    def __getitem__(self, key: K) -> V: \n","        return self._data[key]\n","\n","    def __len__(self) -> int:\n","        return len(self._data)\n","\n","    def __iter__(self):\n","        return iter(self._data)\n","\n","    def as_dict(self):\n","        return self._data.copy()\n"]},{"cell_type":"markdown","metadata":{},"source":["## CHAT data\n","\n","This section contains code necessary for loading and processing CHAT data."]},{"cell_type":"markdown","metadata":{},"source":["### Constants\n","\n","Below are constants needed for the CHAT data processing. Taken from [`./data/chat/preprocessing.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/data/chat/preprocessing.py) in the paper's code repository."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["THRESHOLD = 3\n","NUM_WORKER = 1\n","# STUDY NUMBER\n","SN = 3984\n","FREQ = 128.0\n","EPOCH_LENGTH = 30.0\n","OUT_FOLDER = 'E:\\\\data_chat_baseline_30x128_test'\n","\n","channels: list[str] = [\n","    'E1',  # 0\n","    'E2',  # 1\n","    'F3',  # 2\n","    'F4',  # 3\n","    'C3',  # 4\n","    'C4',  # 5\n","    'M1',  # 6\n","    'M2',  # 7\n","    'O1',  # 8\n","    'O2',  # 9\n","    'ECG1',  # 10\n","    'ECG3',  # 11\n","\n","    'CANNULAFLOW',  # 12\n","    'AIRFLOW',  # 13\n","    'CHEST',  # 14\n","    'ABD',  # 15\n","    'SAO2',  # 16\n","    'CAP',  # 17\n","]\n","\n","APNEA_EVENT_DICT = ReadOnlyDict({\n","    \"Obstructive apnea\": 2,\n","    \"Central apnea\": 2,\n","})\n","\n","HYPOPNEA_EVENT_DICT = ReadOnlyDict({\n","    \"Hypopnea\": 1,\n","})\n","\n","POS_EVENT_DICT = ReadOnlyDict({\n","    \"Hypopnea\": 1,\n","    \"Obstructive apnea\": 2,\n","    \"Central apnea\": 2,\n","})\n","\n","NEG_EVENT_DICT = ReadOnlyDict({\n","    'Stage 1 sleep': 0,\n","    'Stage 2 sleep': 0,\n","    'Stage 3 sleep': 0,\n","    'REM sleep': 0,\n","})\n","\n","WAKE_DICT = ReadOnlyDict({\n","    \"Wake\": 10\n","})"]},{"cell_type":"markdown","metadata":{},"source":["### Remaining preprocessing code\n","\n","The rest of the pre-processing code is below. This code is also taken from [`./data/chat/preprocessing.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/data/chat/preprocessing.py)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import glob\n","import os\n","from datetime import datetime\n","from itertools import compress\n","from typing import Callable, Any\n","\n","import mne\n","import numpy as np\n","import pandas as pd\n","from mne import make_fixed_length_events\n","\n","mne.set_log_file('mne_log.txt', overwrite=False)\n","\n","\n","########################################## Annotation Modifier functions ##########################################\n","\n","def change_duration(\n","        df: pd.DataFrame,\n","        label_dict: ReadOnlyDict[str, int]=POS_EVENT_DICT,\n","        duration:float=EPOCH_LENGTH\n","    ):\n","    for key in label_dict:\n","        df.loc[df.description == key, 'duration'] = duration\n","    print(\"change duration!\")\n","    return df\n","\n","\n","def load_study_chat[T](\n","    edf_path: str,\n","    annotation_path: str,\n","    annotation_func: Callable[[T], T],\n","    preload: bool=False,\n","    exclude=[],\n","    verbose: str | bool='CRITICAL',\n","):\n","    raw = mne.io.edf.edf.RawEDF(input_fname=edf_path, exclude=exclude, preload=preload, verbose=verbose)\n","\n","    df: pd.DataFrame = annotation_func(pd.read_csv(annotation_path, sep='\\t'))\n","    annotations = mne.Annotations(df.onset, df.duration, df.description)  # ,orig_time=new_datetime)\n","\n","    raw.set_annotations(annotations)\n","\n","    raw.rename_channels({name: name.upper() for name in raw.info['ch_names']})\n","\n","    return raw\n","\n","\n","def preprocess[T](\n","    path: tuple[str, str],\n","    annotation_modifier: Callable[[T], T],\n","    out_dir: str\n","):\n","    is_apnea_available, is_hypopnea_available = True, True\n","    raw = load_study_chat(\n","        path[0],\n","        path[1],\n","        annotation_modifier,\n","        verbose=True\n","    )\n","\n","    ########################################   CHECK CRITERIA FOR SS   #################################################\n","    if not all([name in raw.ch_names for name in channels]):\n","        print([name in raw.ch_names for name in channels])\n","        print(\"study \" + os.path.basename(path[0]) + \" skipped since insufficient channels\")\n","        return 0\n","\n","    try:\n","        apnea_events, event_ids = mne.events_from_annotations(\n","            raw=raw,\n","            event_id=POS_EVENT_DICT,\n","            chunk_duration=1.0,\n","            verbose=None,\n","        )\n","    except ValueError:\n","        print(\"No Chunk found!\")\n","        return 0\n","    ########################################   CHECK CRITERIA FOR SS   #################################################\n","    print(str(datetime.now().time().strftime(\"%H:%M:%S\")) + ' --- Processing %s' % os.path.basename(path[0]))\n","\n","    try:\n","        apnea_events, event_ids = mne.events_from_annotations(\n","            raw=raw,\n","            event_id=APNEA_EVENT_DICT,\n","            chunk_duration=1.0,\n","            verbose=None,\n","        )\n","    except ValueError:\n","        is_apnea_available = False\n","\n","    try:\n","        hypopnea_events, event_ids = mne.events_from_annotations(\n","            raw=raw,\n","            event_id=HYPOPNEA_EVENT_DICT,\n","            chunk_duration=1.0,\n","            verbose=None,\n","        )\n","    except ValueError:\n","        is_hypopnea_available = False\n","\n","    wake_events, event_ids = mne.events_from_annotations(raw, event_id=WAKE_DICT, chunk_duration=1.0, verbose=None)\n","    ####################################################################################################################\n","    sfreq = raw.info['sfreq']\n","    tmax = EPOCH_LENGTH - 1. / sfreq\n","\n","    raw = raw.pick_channels(channels, ordered=True)\n","    fixed_events = make_fixed_length_events(raw, id=0, duration=EPOCH_LENGTH, overlap=0.)\n","    epochs = mne.Epochs(raw, fixed_events, event_id=[0], tmin=0, tmax=tmax, baseline=None, preload=True, proj=False, verbose=None)\n","    epochs.load_data()\n","    if sfreq != FREQ:\n","        epochs = epochs.resample(FREQ, npad='auto', n_jobs=8, verbose=None)\n","    data: np.ndarray[Any, Any] = epochs.get_data()\n","    ####################################################################################################################\n","    if is_apnea_available:\n","        apnea_events_set = set((apnea_events[:, 0] / sfreq).astype(int))\n","    if is_hypopnea_available:\n","        hypopnea_events_set = set((hypopnea_events[:, 0] / sfreq).astype(int))\n","    wake_events_set = set((wake_events[:, 0] / sfreq).astype(int))\n","\n","    starts = (epochs.events[:, 0] / sfreq).astype(int)\n","\n","    labels_apnea = []\n","    labels_hypopnea = []\n","    labels_not_awake = []\n","    total_apnea_event_second = 0\n","    total_hypopnea_event_second = 0\n","\n","    for seq in range(data.shape[0]):\n","        epoch_set = set(range(starts[seq], starts[seq] + int(EPOCH_LENGTH)))\n","        if is_apnea_available:\n","            apnea_seconds = len(apnea_events_set.intersection(epoch_set))\n","            total_apnea_event_second += apnea_seconds\n","            labels_apnea.append(apnea_seconds)\n","        else:\n","            labels_apnea.append(0)\n","\n","        if is_hypopnea_available:\n","            hypopnea_seconds = len(hypopnea_events_set.intersection(epoch_set))\n","            total_hypopnea_event_second += hypopnea_seconds\n","            labels_hypopnea.append(hypopnea_seconds)\n","        else:\n","            labels_hypopnea.append(0)\n","\n","        labels_not_awake.append(len(wake_events_set.intersection(epoch_set)) == 0)\n","    ####################################################################################################################\n","    data = data[labels_not_awake, :, :]\n","    labels_apnea = list(compress(labels_apnea, labels_not_awake))\n","    labels_hypopnea = list(compress(labels_hypopnea, labels_not_awake))\n","    ####################################################################################################################\n","\n","    new_data: np.ndarray[Any, Any] = np.zeros_like(data)\n","    for i in range(data.shape[0]):\n","\n","        new_data[i, 0, :] = data[i, 0, :] - data[i, 7, :]  # E1 - M2\n","        new_data[i, 1, :] = data[i, 1, :] - data[i, 6, :]  # E2 - M1\n","\n","        new_data[i, 2, :] = data[i, 2, :] - data[i, 7, :]  # F3 - M2\n","        new_data[i, 3, :] = data[i, 3, :] - data[i, 6, :]  # F4 - M1\n","        new_data[i, 4, :] = data[i, 4, :] - data[i, 7, :]  # C3 - M2\n","        new_data[i, 5, :] = data[i, 5, :] - data[i, 6, :]  # C4 - M1\n","        new_data[i, 6, :] = data[i, 8, :] - data[i, 7, :]  # O1 - M2\n","        new_data[i, 7, :] = data[i, 9, :] - data[i, 6, :]  # O2 - M1\n","\n","        new_data[i, 8, :] = data[i, 10,:] - data[i, 11,:]  # ECG\n","\n","        new_data[i, 9, :] = data[i, 12, :]  # CANULAFLOW\n","        new_data[i, 10, :] = data[i, 13, :]  # AIRFLOW\n","        new_data[i, 11, :] = data[i, 14, :]  # CHEST\n","        new_data[i, 12, :] = data[i, 15, :]  # ABD\n","        new_data[i, 13, :] = data[i, 16, :]  # SAO2\n","        new_data[i, 14, :] = data[i, 17, :]  # CAP\n","    data = new_data[:, :15, :]\n","    ####################################################################################################################\n","\n","    np.savez_compressed(\n","        file=(\n","            out_dir + '\\\\' + os.path.basename(path[0]) + \"_\" + \n","            str(total_apnea_event_second) + \"_\" + \n","            str(total_hypopnea_event_second)\n","        ),\n","        data=data,\n","        labels_apnea=labels_apnea,\n","        labels_hypopnea=labels_hypopnea,\n","    )\n","\n","    return data.shape[0]\n","\n","\n","root = \"./chat/polysomnography/edfs/baseline_test/\"\n","for edf_file in glob.glob(root + \"*.edf\"):\n","    annot_file = edf_file.replace(\".edf\", \"-nsrr.tsv\")\n","\n","    preprocess((edf_file, annot_file), lambda a: a, OUT_FOLDER)"]},{"cell_type":"markdown","metadata":{},"source":["### Loading data\n","\n","Below is the dataloader code. It is taken from [`./data/chat/dataloader.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/data/chat/dataloader.py)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import glob\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","from scipy.signal import resample\n","from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n","from biosppy.signals import tools as st\n","from scipy.interpolate import splev, splrep\n","\n","# 0- E1 - M2\n","# 1- E2 - M1\n","\n","# 2- F3 - M2\n","# 3- F4 - M1\n","# 4- C3 - M2\n","# 5- C4 - M1\n","# 6- O1 - M2\n","# 7- O2 - M1\n","\n","# 8- ECG3 - ECG1\n","\n","# 9- CANULAFLOW\n","# 10- AIRFLOW\n","# 11- CHEST\n","# 12- ABD\n","\n","# 13- SAO2\n","# 14- CAP\n","######### ADDED IN THIS STEP #########\n","# 15- RRI\n","# 16 Ramp\n","\n","SIGS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n","s_count = len(SIGS)\n","\n","THRESHOLD = 3\n","PATH = \"E:\\\\data_chat_baseline_30x128_test\"\n","FREQ = 128\n","EPOCH_LENGTH = 30\n","ECG_SIG = 8\n","OUT_PATH = \"E:\\\\data_chat_baseline_30x128_test\"\n","\n","\n","def extract_rri(signal, ir, CHUNK_DURATION):\n","    tm = np.arange(0, CHUNK_DURATION, step=1 / float(ir))  # TIME METRIC FOR INTERPOLATION\n","\n","    filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\", order=int(0.3 * FREQ),\n","                                      frequency=[3, 45], sampling_rate=FREQ, )\n","    (rpeaks,) = hamilton_segmenter(signal=filtered, sampling_rate=FREQ)\n","    (rpeaks,) = correct_rpeaks(signal=filtered, rpeaks=rpeaks, sampling_rate=FREQ, tol=0.05)\n","\n","    if 4 < len(rpeaks) < 200:  # and np.max(signal) < 0.0015 and np.min(signal) > -0.0015:\n","        rri_tm, rri_signal = rpeaks[1:] / float(FREQ), np.diff(rpeaks) / float(FREQ)\n","        ampl_tm, ampl_signal = rpeaks / float(FREQ), signal[rpeaks]\n","        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=1)\n","        amp_interp_signal = splev(tm, splrep(ampl_tm, ampl_signal, k=3), ext=1)\n","\n","        return np.clip(rri_interp_signal, 0, 2) * 100, np.clip(amp_interp_signal, -0.001, 0.002) * 10000, True\n","    else:\n","        return np.zeros((FREQ * EPOCH_LENGTH)), np.zeros((FREQ * EPOCH_LENGTH)), False\n","\n","\n","def load_data(path):\n","    # demo = pd.read_csv(\"../misc/result.csv\")\n","    # ahi = pd.read_csv(r\"C:\\Data\\AHI.csv\")\n","    # ahi_dict = dict(zip(ahi.Study, ahi.AHI))\n","    root_dir = os.path.expanduser(path)\n","    file_list = os.listdir(root_dir)\n","    length = len(file_list)\n","\n","    ################################### Fold the data based on number of respiratory events #########################\n","    study_event_counts = [i for i in range(0, length)]\n","    folds = []\n","    for i in range(5):\n","        folds.append(study_event_counts[i::5])\n","\n","    x = []\n","    y_apnea = []\n","    y_hypopnea = []\n","    counter = 0\n","    for idx, fold in enumerate(folds):\n","        first = True\n","        for patient in fold:\n","            rri_succ_counter = 0\n","            rri_fail_counter = 0\n","            counter += 1\n","            print(counter)\n","            # for study in glob.glob(PATH + patient[0] + \"_*\"):\n","            study_data = np.load(PATH + \"\\\\\" + file_list[patient - 1])\n","\n","            signals = study_data['data']\n","            labels_apnea = study_data['labels_apnea']\n","            labels_hypopnea = study_data['labels_hypopnea']\n","\n","            # identifier = study.split('\\\\')[-1].split('_')[0] + \"_\" + study.split('\\\\')[-1].split('_')[1]\n","            # demo_arr = demo[demo['id'] == identifier].drop(columns=['id']).to_numpy().squeeze()\n","\n","            y_c = labels_apnea + labels_hypopnea\n","            neg_samples = np.where(y_c == 0)[0]\n","            pos_samples = list(np.where(y_c > 0)[0])\n","            ratio = len(pos_samples) / len(neg_samples)\n","            neg_survived = []\n","            for s in range(len(neg_samples)):\n","                if random.random() < ratio:\n","                    neg_survived.append(neg_samples[s])\n","            samples = neg_survived + pos_samples\n","            signals = signals[samples, :, :]\n","            labels_apnea = labels_apnea[samples]\n","            labels_hypopnea = labels_hypopnea[samples]\n","\n","            data = np.zeros((signals.shape[0], EPOCH_LENGTH * FREQ, s_count + 2))\n","            for i in range(signals.shape[0]):  # for each epoch\n","                # data[i, :len(demo_arr), -3] = demo_arr\n","                data[i, :, -1], data[i, :, -2], status = extract_rri(signals[i, ECG_SIG, :], FREQ,\n","                                                                     float(EPOCH_LENGTH))\n","\n","                if status:\n","                    rri_succ_counter += 1\n","                else:\n","                    rri_fail_counter += 1\n","\n","                for j in range(s_count):  # for each signal\n","                    data[i, :, j] = signals[i, SIGS[j], :]\n","\n","            if first:\n","                aggregated_data = data\n","                aggregated_label_apnea = labels_apnea\n","                aggregated_label_hypopnea = labels_hypopnea\n","                first = False\n","            else:\n","                aggregated_data = np.concatenate((aggregated_data, data), axis=0)\n","                aggregated_label_apnea = np.concatenate((aggregated_label_apnea, labels_apnea), axis=0)\n","                aggregated_label_hypopnea = np.concatenate((aggregated_label_hypopnea, labels_hypopnea), axis=0)\n","            print(rri_succ_counter, rri_fail_counter)\n","\n","        x.append(aggregated_data)\n","        y_apnea.append(aggregated_label_apnea)\n","        y_hypopnea.append(aggregated_label_hypopnea)\n","\n","    return x, y_apnea, y_hypopnea\n","\n","\n","if __name__ == \"__main__\":\n","    x, y_apnea, y_hypopnea = load_data(PATH)\n","    for i in range(5):\n","        print(x[i].shape, y_apnea[i].shape, y_hypopnea[i].shape)\n","        np.savez_compressed(OUT_PATH + \"_\" + str(i), x=x[i], y_apnea=y_apnea[i], y_hypopnea=y_hypopnea[i])"]},{"cell_type":"markdown","metadata":{},"source":["## NCH Data\n","\n","This section contains preprocessing and data loading code for NCH data.\n","\n","It is taken from [`./data/nch`](https://github.com/healthylaife/Pediatric-Apnea-Detection/tree/main/data/nch) in the paper's repository."]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing\n","\n","The following is taken from [`./data/nch/preprocessing.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/data/nch/preprocessing.py)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import concurrent.futures\n","from datetime import datetime\n","import pandas as pd\n","import mne\n","import numpy as np\n","import scipy\n","from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n","from biosppy.signals import tools as st\n","from mne import make_fixed_length_events\n","from scipy.interpolate import splev, splrep\n","from itertools import compress\n","import sleep_study as ss\n","\n","THRESHOLD = 3\n","NUM_WORKER = 8\n","SN = 3984  # STUDY NUMBER\n","FREQ = 64.0\n","CHUNK_DURATION = 30.0\n","OUT_FOLDER = 'D:\\\\nch_30x64'\n","\n","# channels = [\n","#     \"EOG LOC-M2\",  # 0\n","#     \"EOG ROC-M1\",  # 1\n","#     \"EEG F3-M2\",  # 2\n","#     \"EEG F4-M1\",  # 3\n","#     \"EEG C3-M2\",  # 4\n","#     \"EEG C4-M1\",  # 5\n","#     \"EEG O1-M2\",  # 6\n","#     \"EEG O2-M1\",  # 7\n","#     \"EEG CZ-O1\",  # 8\n","#     \"ECG EKG2-EKG\",  # 9\n","#     \"RESP PTAF\",  # 10\n","#     \"RESP AIRFLOW\",  # 11\n","#     \"RESP THORACIC\",  # 12\n","#     \"RESP ABDOMINAL\",  # 13\n","#     \"SPO2\",  # 14\n","#     \"RATE\",  # 15\n","#     \"CAPNO\",  # 16\n","#     \"RESP RATE\",  # 17\n","# ]\n","\n","\n","channels = [\n","    \"EOG LOC-M2\",  # 0\n","    \"EOG ROC-M1\",  # 1\n","    \"EEG C3-M2\",  # 2\n","    \"EEG C4-M1\",  # 3\n","    \"ECG EKG2-EKG\",  # 4\n","\n","    \"RESP PTAF\",  # 5\n","    \"RESP AIRFLOW\",  # 6\n","    \"RESP THORACIC\",  # 7\n","    \"RESP ABDOMINAL\",  # 8\n","    \"SPO2\",  # 9\n","    \"CAPNO\",  # 10\n","]\n","\n","APNEA_EVENT_DICT = {\n","    \"Obstructive Apnea\": 2,\n","    \"Central Apnea\": 2,\n","    \"Mixed Apnea\": 2,\n","    \"apnea\": 2,\n","    \"obstructive apnea\": 2,\n","    \"central apnea\": 2,\n","    \"apnea\": 2,\n","    \"Apnea\": 2,\n","}\n","\n","HYPOPNEA_EVENT_DICT = {\n","    \"Obstructive Hypopnea\": 1,\n","    \"Hypopnea\": 1,\n","    \"hypopnea\": 1,\n","    \"Mixed Hypopnea\": 1,\n","    \"Central Hypopnea\": 1,\n","}\n","\n","POS_EVENT_DICT = {\n","    \"Obstructive Hypopnea\": 1,\n","    \"Hypopnea\": 1,\n","    \"hypopnea\": 1,\n","    \"Mixed Hypopnea\": 1,\n","    \"Central Hypopnea\": 1,\n","\n","    \"Obstructive Apnea\": 2,\n","    \"Central Apnea\": 2,\n","    \"Mixed Apnea\": 2,\n","    \"apnea\": 2,\n","    \"obstructive apnea\": 2,\n","    \"central apnea\": 2,\n","    \"Apnea\": 2,\n","}\n","\n","NEG_EVENT_DICT = {\n","    'Sleep stage N1': 0,\n","    'Sleep stage N2': 0,\n","    'Sleep stage N3': 0,\n","    'Sleep stage R': 0,\n","}\n","\n","WAKE_DICT = {\n","    \"Sleep stage W\": 10\n","}\n","\n","mne.set_log_file('log.txt', overwrite=False)\n","\n","########################################## Annotation Modifier functions ##########################################\n","def identity(df):\n","    return df\n","\n","\n","def apnea2bad(df):\n","    df = df.replace(r'.*pnea.*', 'badevent', regex=True)\n","    print(\"bad replaced!\")\n","    return df\n","\n","\n","def wake2bad(df):\n","    return df.replace(\"Sleep stage W\", 'badevent')\n","\n","\n","def change_duration(df, label_dict=POS_EVENT_DICT, duration=CHUNK_DURATION):\n","    for key in label_dict:\n","        df.loc[df.description == key, 'duration'] = duration\n","    print(\"change duration!\")\n","    return df\n","\n","def preprocess(i, annotation_modifier, out_dir, ahi_dict):\n","    is_apnea_available, is_hypopnea_available = True, True\n","    study = ss.data.study_list[i]\n","    raw = ss.data.load_study(study, annotation_modifier, verbose=True)\n","    ########################################   CHECK CRITERIA FOR SS   #################################################\n","    if not all([name in raw.ch_names for name in channels]):\n","        print(\"study \" + str(study) + \" skipped since insufficient channels\")\n","        return 0\n","\n","    if ahi_dict.get(study, 0) < THRESHOLD:\n","        print(\"study \" + str(study) + \" skipped since low AHI ---  AHI = \" + str(ahi_dict.get(study, 0)))\n","        return 0\n","\n","    try:\n","        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=POS_EVENT_DICT, chunk_duration=1.0,\n","                                                              verbose=None)\n","    except ValueError:\n","        print(\"No Chunk found!\")\n","        return 0\n","    ########################################   CHECK CRITERIA FOR SS   #################################################\n","    print(str(i) + \"---\" + str(datetime.now().time().strftime(\"%H:%M:%S\")) + ' --- Processing %d' % i)\n","\n","    try:\n","        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=APNEA_EVENT_DICT, chunk_duration=1.0,\n","                                                              verbose=None)\n","    except ValueError:\n","        is_apnea_available = False\n","\n","    try:\n","        hypopnea_events, event_ids = mne.events_from_annotations(raw, event_id=HYPOPNEA_EVENT_DICT, chunk_duration=1.0,\n","                                                                 verbose=None)\n","    except ValueError:\n","        is_hypopnea_available = False\n","\n","    wake_events, event_ids = mne.events_from_annotations(raw, event_id=WAKE_DICT, chunk_duration=1.0, verbose=None)\n","    ####################################################################################################################\n","    sfreq = raw.info['sfreq']\n","    tmax = CHUNK_DURATION - 1. / sfreq\n","\n","    raw = raw.pick_channels(channels, ordered=True)\n","    fixed_events = make_fixed_length_events(raw, id=0, duration=CHUNK_DURATION, overlap=0.)\n","    epochs = mne.Epochs(raw, fixed_events, event_id=[0], tmin=0, tmax=tmax, baseline=None, preload=True, proj=False, verbose=None)\n","    epochs.load_data()\n","    if sfreq != FREQ:\n","        epochs = epochs.resample(FREQ, npad='auto', n_jobs=4, verbose=None)\n","    data = epochs.get_data()\n","    ####################################################################################################################\n","    if is_apnea_available:\n","        apnea_events_set = set((apnea_events[:, 0] / sfreq).astype(int))\n","    if is_hypopnea_available:\n","        hypopnea_events_set = set((hypopnea_events[:, 0] / sfreq).astype(int))\n","    wake_events_set = set((wake_events[:, 0] / sfreq).astype(int))\n","\n","    starts = (epochs.events[:, 0] / sfreq).astype(int)\n","\n","    labels_apnea = []\n","    labels_hypopnea = []\n","    labels_wake = []\n","    total_apnea_event_second = 0\n","    total_hypopnea_event_second = 0\n","\n","    for seq in range(data.shape[0]):\n","        epoch_set = set(range(starts[seq], starts[seq] + int(CHUNK_DURATION)))\n","        if is_apnea_available:\n","            apnea_seconds = len(apnea_events_set.intersection(epoch_set))\n","            total_apnea_event_second += apnea_seconds\n","            labels_apnea.append(apnea_seconds)\n","        else:\n","            labels_apnea.append(0)\n","\n","        if is_hypopnea_available:\n","            hypopnea_seconds = len(hypopnea_events_set.intersection(epoch_set))\n","            total_hypopnea_event_second += hypopnea_seconds\n","            labels_hypopnea.append(hypopnea_seconds)\n","        else:\n","            labels_hypopnea.append(0)\n","\n","        labels_wake.append(len(wake_events_set.intersection(epoch_set)) == 0)\n","    ####################################################################################################################\n","    print(study + \"    HAMED    \" + str(len(labels_wake) - sum(labels_wake)))\n","    data = data[labels_wake, :, :]\n","    labels_apnea = list(compress(labels_apnea, labels_wake))\n","    labels_hypopnea = list(compress(labels_hypopnea, labels_wake))\n","\n","    np.savez_compressed(\n","        out_dir + '\\\\' + study + \"_\" + str(total_apnea_event_second) + \"_\" + str(total_hypopnea_event_second),\n","        data=data, labels_apnea=labels_apnea, labels_hypopnea=labels_hypopnea)\n","\n","    return data.shape[0]\n","\n","\n","if __name__ == \"__main__\":\n","    ahi = pd.read_csv(r\"D:\\Data\\AHI.csv\")\n","    ahi_dict = dict(zip(ahi.Study, ahi.AHI))\n","    ss.__init__()\n","\n","    if NUM_WORKER < 2:\n","        for idx in range(SN):\n","            preprocess(idx, identity, OUT_FOLDER, ahi_dict)\n","    else:\n","        with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_WORKER) as executor:\n","            executor.map(preprocess, range(SN), [identity] * SN, [OUT_FOLDER] * SN, [ahi_dict] * SN)"]},{"cell_type":"markdown","metadata":{},"source":["### Dataloader\n","\n","The following is taken from [`./data/nch/dataloader.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/data/nch/dataloader.py) in the repository."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import glob\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","from scipy.signal import resample\n","from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n","from biosppy.signals import tools as st\n","from scipy.interpolate import splev, splrep\n","\n","# \"EOG LOC-M2\",  # 0\n","# \"EOG ROC-M1\",  # 1\n","# \"EEG C3-M2\",  # 2\n","# \"EEG C4-M1\",  # 3\n","# \"ECG EKG2-EKG\",  # 4\n","#\n","# \"RESP PTAF\",  # 5\n","# \"RESP AIRFLOW\",  # 6\n","# \"RESP THORACIC\",  # 7\n","# \"RESP ABDOMINAL\",  # 8\n","# \"SPO2\",  # 9\n","# \"CAPNO\",  # 10\n","\n","######### ADDED IN THIS STEP #########\n","# RRI #11\n","# Ramp #12\n","# Demo #13\n","\n","\n","SIGS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","s_count = len(SIGS)\n","\n","THRESHOLD = 3\n","PATH = \"D:\\\\nch_30x64\\\\\"\n","FREQ = 64\n","EPOCH_DURATION = 30\n","ECG_SIG = 4\n","OUT_PATH = \"D:\\\\nch_30x64\"\n","\n","def extract_rri(signal, ir, CHUNK_DURATION):\n","    tm = np.arange(0, CHUNK_DURATION, step=1 / float(ir))  # TIME METRIC FOR INTERPOLATION\n","\n","    filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\", order=int(0.3 * FREQ),\n","                                      frequency=[3, 45], sampling_rate=FREQ, )\n","    (rpeaks,) = hamilton_segmenter(signal=filtered, sampling_rate=FREQ)\n","    (rpeaks,) = correct_rpeaks(signal=filtered, rpeaks=rpeaks, sampling_rate=FREQ, tol=0.05)\n","\n","    if 4 < len(rpeaks) < 200:  # and np.max(signal) < 0.0015 and np.min(signal) > -0.0015:\n","        rri_tm, rri_signal = rpeaks[1:] / float(FREQ), np.diff(rpeaks) / float(FREQ)\n","        ampl_tm, ampl_signal = rpeaks / float(FREQ), signal[rpeaks]\n","        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=1)\n","        amp_interp_signal = splev(tm, splrep(ampl_tm, ampl_signal, k=3), ext=1)\n","\n","        return np.clip(rri_interp_signal, 0, 2), np.clip(amp_interp_signal, -0.001, 0.002)\n","    else:\n","        return np.zeros((FREQ * EPOCH_DURATION)), np.zeros((FREQ * EPOCH_DURATION))\n","\n","\n","def load_data(path):\n","    # demo = pd.read_csv(\"../misc/result.csv\") # TODO\n","    ahi = pd.read_csv(r\"D:\\Data\\AHI.csv\")\n","    ahi_dict = dict(zip(ahi.Study, ahi.AHI))\n","    root_dir = os.path.expanduser(path)\n","    file_list = os.listdir(root_dir)\n","    length = len(file_list)\n","\n","    study_event_counts = {}\n","    apnea_event_counts = {}\n","    hypopnea_event_counts = {}\n","    ######################################## Count the respiratory events ###########################################\n","    for i in range(length):\n","        patient_id = (file_list[i].split(\"_\")[0])\n","        study_id = (file_list[i].split(\"_\")[1])\n","        apnea_count = int((file_list[i].split(\"_\")[2]))\n","        hypopnea_count = int((file_list[i].split(\"_\")[3]).split(\".\")[0])\n","\n","        if ahi_dict.get(patient_id + \"_\" + study_id, 0) > THRESHOLD:\n","            apnea_event_counts[patient_id] = apnea_event_counts.get(patient_id, 0) + apnea_count\n","            hypopnea_event_counts[patient_id] = hypopnea_event_counts.get(patient_id, 0) + hypopnea_count\n","            study_event_counts[patient_id] = study_event_counts.get(patient_id, 0) + apnea_count + hypopnea_count\n","        else:\n","            os.remove(PATH + file_list[i])\n","\n","    apnea_event_counts = sorted(apnea_event_counts.items(), key=lambda item: item[1])\n","    hypopnea_event_counts = sorted(hypopnea_event_counts.items(), key=lambda item: item[1])\n","    study_event_counts = sorted(study_event_counts.items(), key=lambda item: item[1])\n","\n","    ################################### Fold the data based on number of respiratory events #########################\n","    folds = []\n","    for i in range(5):\n","        folds.append(study_event_counts[i::5])\n","\n","    x = []\n","    y_apnea = []\n","    y_hypopnea = []\n","    counter = 0\n","    for idx, fold in enumerate(folds):\n","        first = True\n","        for patient in fold:\n","            counter += 1\n","            print(counter)\n","            for study in glob.glob(PATH + patient[0] + \"_*\"):\n","                study_data = np.load(study)\n","\n","                signals = study_data['data']\n","                labels_apnea = study_data['labels_apnea']\n","                labels_hypopnea = study_data['labels_hypopnea']\n","\n","                identifier = study.split('\\\\')[-1].split('_')[0] + \"_\" + study.split('\\\\')[-1].split('_')[1]\n","                # demo_arr = demo[demo['id'] == identifier].drop(columns=['id']).to_numpy().squeeze() # TODO\n","\n","                y_c = labels_apnea + labels_hypopnea\n","                neg_samples = np.where(y_c == 0)[0]\n","                pos_samples = list(np.where(y_c > 0)[0])\n","                ratio = len(pos_samples) / len(neg_samples)\n","                neg_survived = []\n","                for s in range(len(neg_samples)):\n","                    if random.random() < ratio:\n","                        neg_survived.append(neg_samples[s])\n","                samples = neg_survived + pos_samples\n","                signals = signals[samples, :, :]\n","                labels_apnea = labels_apnea[samples]\n","                labels_hypopnea = labels_hypopnea[samples]\n","\n","                data = np.zeros((signals.shape[0], EPOCH_DURATION * FREQ, s_count + 3))\n","                for i in range(signals.shape[0]):  # for each epoch\n","                    # data[i, :len(demo_arr), -1] = demo_arr TODO\n","                    data[i, :, -2], data[i, :, -3] = extract_rri(signals[i, ECG_SIG, :], FREQ, float(EPOCH_DURATION))\n","                    for j in range(s_count):  # for each signal\n","                        data[i, :, j] = resample(signals[i, SIGS[j], :], EPOCH_DURATION * FREQ)\n","\n","                if first:\n","                    aggregated_data = data\n","                    aggregated_label_apnea = labels_apnea\n","                    aggregated_label_hypopnea = labels_hypopnea\n","                    first = False\n","                else:\n","                    aggregated_data = np.concatenate((aggregated_data, data), axis=0)\n","                    aggregated_label_apnea = np.concatenate((aggregated_label_apnea, labels_apnea), axis=0)\n","                    aggregated_label_hypopnea = np.concatenate((aggregated_label_hypopnea, labels_hypopnea), axis=0)\n","\n","\n","        x.append(aggregated_data)\n","        y_apnea.append(aggregated_label_apnea)\n","        y_hypopnea.append(aggregated_label_hypopnea)\n","\n","    return x, y_apnea, y_hypopnea\n","\n","\n","if __name__ == \"__main__\":\n","    x, y_apnea, y_hypopnea = load_data(PATH)\n","    np.savez_compressed(OUT_PATH, x=x, y_apnea=y_apnea, y_hypopnea=y_hypopnea)"]},{"cell_type":"markdown","metadata":{},"source":["## Stock code\n","\n","The following code already existed in the notebook for loading data. It should be adapted to work with both CHAT and NCH datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZScZNbROw-N"},"outputs":[],"source":["# dir and function to load raw data\n","raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n","\n","def load_raw_data(raw_data_dir):\n","  # implement this function to load raw data to dataframe/numpy array/tensor\n","  return None\n","\n","raw_data = load_raw_data(raw_data_dir)\n","\n","# calculate statistics\n","def calculate_stats(raw_data):\n","  # implement this function to calculate the statistics\n","  # it is encouraged to print out the results\n","  return None\n","\n","# process raw data\n","def process_data(raw_data):\n","    # implement this function to process the data as you need\n","  return None\n","\n","processed_data = process_data(raw_data)\n","\n","''' you can load the processed data directly\n","processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n","def load_processed_data(raw_data_dir):\n","  pass\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"3muyDPFPbozY"},"source":["#   Model\n","The model includes the model definitation which usually is a class, model training, and other necessary parts.\n","  * Model architecture: layer number/size/type, activation function, etc\n","  * Training objectives: loss function, optimizer, weight of each loss term, etc\n","  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n","  * The code of model should have classes of the model, functions of model training, model validation, etc.\n","  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."]},{"cell_type":"markdown","metadata":{},"source":["## Transformers\n","\n","The below is from [`./models/transformer.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/models/transformer.py)."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _tf_keras\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/_tf_keras/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/_tf_keras/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/activations/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/activations/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gelu\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/activations/activations.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/backend/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m any_symbolic_tensors\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_keras_tensor\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/backend/common/keras_tensor.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.KerasTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKerasTensor\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/utils/tree.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_structures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[1;32m     14\u001b[0m     optree\u001b[38;5;241m.\u001b[39mregister_pytree_node(\n\u001b[1;32m     15\u001b[0m         ListWrapper,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: (x, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m metadata, children: ListWrapper(\u001b[38;5;28mlist\u001b[39m(children)),\n\u001b[1;32m     18\u001b[0m         namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.tree.is_nested\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_nested\u001b[39m(structure):\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"]}],"source":["import keras\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from keras import Model\n","from keras.activations import sigmoid, relu\n","from keras.layers import Dense, Dropout, Reshape, LayerNormalization, MultiHeadAttention, Add, Flatten, Input, Layer, \\\n","    GlobalAveragePooling1D, AveragePooling1D, Concatenate, SeparableConvolution1D, Conv1D\n","from keras.regularizers import L2\n","\n","\n","\n","class Patches(Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, input):\n","        input = input[:, tf.newaxis, :, :]\n","        batch_size = tf.shape(input)[0]\n","        patches = tf.image.extract_patches(\n","            images=input,\n","            sizes=[1, 1, self.patch_size, 1],\n","            strides=[1, 1, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(\n","            patches,\n","            [batch_size, -1, patch_dims]\n","        )\n","        return patches\n","\n","\n","class PatchEncoder(Layer):\n","    def __init__(self, num_patches, projection_dim, l2_weight):\n","        super(PatchEncoder, self).__init__()\n","        self.projection_dim = projection_dim\n","        self.l2_weight = l2_weight\n","        self.num_patches = num_patches\n","        self.projection = Dense(units=projection_dim, kernel_regularizer=L2(l2_weight),\n","                                bias_regularizer=L2(l2_weight))\n","        self.position_embedding = tf.keras.layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim)\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) # + self.position_embedding(positions)\n","        return encoded\n","\n","\n","def mlp(x, hidden_units, dropout_rate, l2_weight):\n","    for _, units in enumerate(hidden_units):\n","        x = Dense(units, activation=None, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight))(x)\n","        x = tf.nn.gelu(x)\n","        x = Dropout(dropout_rate)(x)\n","    return x\n","\n","\n","def create_transformer_model(\n","    input_shape,\n","    num_patches,\n","    projection_dim,\n","    transformer_layers: int,\n","    num_heads,\n","    transformer_units,\n","    mlp_head_units,\n","    num_classes,\n","    drop_out,\n","    reg,\n","    l2_weight,\n","    demographic=False,\n","):\n","    if reg:\n","        activation = None\n","    else:\n","        activation = 'sigmoid'\n","    inputs = Input(shape=input_shape)\n","    patch_size = input_shape[0] / num_patches\n","    if demographic:\n","        normalized_inputs = tfa.layers.InstanceNormalization(\n","            axis=-1,\n","            epsilon=1e-6,\n","            center=False,\n","            scale=False,\n","            beta_initializer=\"glorot_uniform\",\n","            gamma_initializer=\"glorot_uniform\"\n","        )(inputs[:,:,:-1])\n","        demo = inputs[:, :12, -1]\n","\n","    else:\n","        normalized_inputs = tfa.layers.InstanceNormalization(\n","            axis=-1,\n","            epsilon=1e-6,\n","            center=False,\n","            scale=False,\n","            beta_initializer=\"glorot_uniform\",\n","            gamma_initializer=\"glorot_uniform\",\n","        )(inputs)\n","\n","    # patches = Reshape((num_patches, -1))(normalized_inputs)\n","    patches = Patches(patch_size=patch_size)(normalized_inputs)\n","    encoded_patches = PatchEncoder(\n","        num_patches=num_patches,\n","        projection_dim=projection_dim,\n","        l2_weight=l2_weight\n","    )(patches)\n","\n","    for i in range(transformer_layers):\n","        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n","        attention_output = MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n","            bias_regularizer=L2(l2_weight))(x1, x1)\n","        x2 = Add()([attention_output, encoded_patches])\n","        x3 = LayerNormalization(epsilon=1e-6)(x2)\n","        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n","        encoded_patches = Add()([x3, x2])\n","\n","    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    x = GlobalAveragePooling1D()(x)\n","    #x = Concatenate()([x, demo])\n","    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n","\n","    logits = Dense(\n","        num_classes,\n","        kernel_regularizer=L2(l2_weight),\n","        bias_regularizer=L2(l2_weight),\n","        activation=activation\n","    )(features)\n","\n","    return tf.keras.Model(inputs=inputs, outputs=logits)\n","\n","\n","\n","def create_hybrid_transformer_model(input_shape):\n","    transformer_units =  [32,32]\n","    transformer_layers = 2\n","    num_heads = 4\n","    l2_weight = 0.001\n","    drop_out= 0.25\n","    mlp_head_units = [256, 128]\n","    num_patches=30\n","    projection_dim=  32\n","\n","    # Conv1D(32...\n","    input1 = Input(shape=input_shape)\n","    conv11 = Conv1D(16, 256)(input1) #13\n","    conv12 = Conv1D(16, 256)(input1) #13\n","    conv13 = Conv1D(16, 256)(input1) #13\n","\n","    pwconv1 = SeparableConvolution1D(32, 1)(input1)\n","    pwconv2 = SeparableConvolution1D(32, 1)(pwconv1)\n","\n","    conv21 = Conv1D(16, 256)(conv11) # 7\n","    conv22 = Conv1D(16, 256)(conv12) # 7\n","    conv23 = Conv1D(16, 256)(conv13) # 7\n","\n","    concat = keras.layers.concatenate([conv21, conv22, conv23], axis=-1)\n","    concat = Dense(64, activation=relu)(concat) #192\n","    concat = Dense(64, activation=sigmoid)(concat) #192\n","    concat = SeparableConvolution1D(32,1)(concat)\n","    concat = keras.layers.concatenate([concat, pwconv2], axis=1)\n","\n","    ####################################################################################################################\n","    patch_size = input_shape[0] / num_patches\n","\n","    normalized_inputs = tfa.layers.InstanceNormalization(\n","        axis=-1,\n","        epsilon=1e-6,\n","        center=False,\n","        scale=False,\n","        beta_initializer=\"glorot_uniform\",\n","        gamma_initializer=\"glorot_uniform\"\n","    )(concat)\n","\n","    # patches = Reshape((num_patches, -1))(normalized_inputs)\n","    patches = Patches(patch_size=patch_size)(normalized_inputs)\n","    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n","    for i in range(transformer_layers):\n","        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n","        attention_output = MultiHeadAttention(\n","            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n","            bias_regularizer=L2(l2_weight))(x1, x1)\n","        x2 = Add()([attention_output, encoded_patches])\n","        x3 = LayerNormalization(epsilon=1e-6)(x2)\n","        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n","        encoded_patches = Add()([x3, x2])\n","\n","    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    x = GlobalAveragePooling1D()(x)\n","    #x = Concatenate()([x, demo])\n","    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n","\n","    logits = Dense(1, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n","                   activation='sigmoid')(features)\n","\n","    ####################################################################################################################\n","\n","    model = Model(inputs=input1, outputs=logits)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Core model\n","\n","The following is from [`./models/models.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/models/models.py) in the repo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBdVZoTvsSFV"},"outputs":[],"source":["import keras\n","from keras import Input, Model\n","from keras.layers import Dense, Flatten, MaxPooling2D, Conv2D, BatchNormalization, LSTM, Bidirectional, Permute, \\\n","    Reshape, GRU, Conv1D, MaxPooling1D, Activation, Dropout, GlobalAveragePooling1D, multiply, MultiHeadAttention, Add, \\\n","    LayerNormalization, SeparableConvolution1D\n","from keras.models import Sequential\n","from keras.activations import relu, sigmoid\n","from keras.regularizers import l2\n","import tensorflow_addons as tfa\n","from .transformer import create_transformer_model, mlp, create_hybrid_transformer_model\n","\n","\n","\n","\n","def create_cnn_model(input_shape):\n","    model = Sequential()\n","    for i in range(5): # 10\n","        model.add(Conv1D(45, 32, padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Activation(relu))\n","        model.add(MaxPooling1D())\n","        model.add(Dropout(0.5))\n","\n","    model.add(Flatten())\n","    for i in range(2): #4\n","        model.add(Dense(512))\n","        model.add(BatchNormalization())\n","        model.add(Activation(relu))\n","        model.add(Dropout(0.5))\n","\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    return model\n","\n","\n","def create_cnnlstm_model(input_a_shape, weight=1e-3):\n","    cnn_filters = 32 # 128\n","    cnn_kernel_size = 4 # 4\n","    input1 = Input(shape=input_a_shape)\n","    input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n","                                              beta_initializer=\"glorot_uniform\",\n","                                              gamma_initializer=\"glorot_uniform\")(input1)\n","    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(input1)\n","    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n","    x1 = BatchNormalization()(x1)\n","    x1 = MaxPooling1D()(x1)\n","\n","    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n","    x1 = BatchNormalization()(x1)\n","    x1 = MaxPooling1D()(x1)\n","\n","    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n","    x1 = BatchNormalization()(x1)\n","    x1 = MaxPooling1D()(x1)\n","\n","    x1 = LSTM(32, return_sequences=True)(x1) #256\n","    x1 = LSTM(32, return_sequences=True)(x1) #256\n","    x1 = LSTM(32)(x1) #256\n","    x1 = Flatten()(x1)\n","\n","    x1 = Dense(32, activation='relu')(x1) #64\n","    x1 = Dense(32, activation='relu')(x1) #64\n","    outputs = Dense(1, activation='sigmoid')(x1)\n","\n","    model = Model(inputs=input1, outputs=outputs)\n","    return model\n","\n","\n","def create_semscnn_model(input_a_shape):\n","    input1 = Input(shape=input_a_shape)\n","    # input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n","    #                                           beta_initializer=\"glorot_uniform\",\n","    #                                           gamma_initializer=\"glorot_uniform\")(input1)\n","    x1 = Conv1D(45, 32, strides=1)(input1) #kernel_size=11\n","    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n","    x1 = BatchNormalization()(x1)\n","    x1 = Activation(relu)(x1)\n","    x1 = MaxPooling1D()(x1)\n","\n","    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n","    x1 = BatchNormalization()(x1)\n","    x1 = Activation(relu)(x1)\n","    x1 = MaxPooling1D()(x1)\n","\n","    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n","    x1 = BatchNormalization()(x1)\n","    x1 = Activation(relu)(x1)\n","    x1 = MaxPooling1D()(x1)\n","\n","    squeeze = Flatten()(x1)\n","    excitation = Dense(128, activation='relu')(squeeze)\n","    excitation = Dense(64, activation='relu')(excitation)\n","    logits = Dense(1, activation='sigmoid')(excitation)\n","    model = Model(inputs=input1, outputs=logits)\n","    return model\n","\n","model_dict = {\n","\n","    \"cnn\": create_cnn_model((60 * 32, 3)),\n","    \"sem-mscnn\": create_semscnn_model((60 * 32, 3)),\n","    \"cnn-lstm\": create_cnnlstm_model((60 * 32, 3)),\n","    \"hybrid\": create_hybrid_transformer_model((60 * 32, 3)),\n","}\n","\n","\n","def get_model(config):\n","    if config[\"model_name\"].split('_')[0] == \"Transformer\":\n","        return create_transformer_model(\n","            input_shape=(60 * 32, len(config[\"channels\"])),\n","            num_patches=config[\"num_patches\"],\n","            projection_dim=config[\"transformer_units\"],\n","            transformer_layers=config[\"transformer_layers\"],\n","            num_heads=config[\"num_heads\"],\n","            transformer_units=[\n","                config[\"transformer_units\"] * 2,\n","                config[\"transformer_units\"]\n","            ],\n","            mlp_head_units=[256, 128],\n","            num_classes=1,\n","            drop_out=config[\"drop_out_rate\"],\n","            reg=config[\"regression\"],\n","            l2_weight=config[\"regularization_weight\"]\n","        )\n","    else:\n","        return model_dict.get(config[\"model_name\"].split('_')[0])\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model training\n","\n","The below trains the model using the data prepared previously. This code taken from [`./main/train.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/train.py)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import keras\n","import keras.metrics\n","import numpy as np\n","from keras.callbacks import LearningRateScheduler, EarlyStopping\n","from keras.losses import BinaryCrossentropy\n","from sklearn.utils import shuffle\n","\n","from models.models import get_model\n","\n","THRESHOLD = 1\n","FOLD = 5\n","\n","\n","def lr_schedule(epoch, lr):\n","    if epoch > 50 and (epoch - 1) % 5 == 0:\n","        lr *= 0.5\n","    return lr\n","\n","\n","def train(config, fold=None):\n","    data = np.load(config[\"data_path\"], allow_pickle=True)\n","    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n","    y = y_apnea + y_hypopnea\n","    ########################################################################################\n","    for i in range(FOLD):\n","        x[i], y[i] = shuffle(x[i], y[i])\n","        x[i] = np.nan_to_num(x[i], nan=-1)\n","        if config[\"regression\"]:\n","            y[i] = np.sqrt(y[i])\n","            y[i][y[i] != 0] += 2\n","        else:\n","            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n","\n","        x[i] = x[i][:, :, config[\"channels\"]]  # CHANNEL SELECTION\n","\n","    ########################################################################################\n","    folds = range(FOLD) if fold is None else [fold]\n","    for fold in folds:\n","        first = True\n","        for i in range(5):\n","            if i != fold:\n","                if first:\n","                    x_train = x[i]\n","                    y_train = y[i]\n","                    first = False\n","                else:\n","                    x_train = np.concatenate((x_train, x[i]))\n","                    y_train = np.concatenate((y_train, y[i]))\n","\n","        model = get_model(config)\n","        if config[\"regression\"]:\n","            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n","            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","        else:\n","            model.compile(\n","                optimizer=\"adam\",\n","                loss=BinaryCrossentropy(),\n","                metrics=[\n","                    keras.metrics.Precision(),\n","                    keras.metrics.Recall()\n","                ],\n","            )\n","            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","        lr_scheduler = LearningRateScheduler(lr_schedule)\n","        model.fit(\n","            x=x_train,\n","            y=y_train,\n","            batch_size=512,\n","            epochs=config[\"epochs\"],\n","            validation_split=0.1,\n","            callbacks=[early_stopper, lr_scheduler]\n","        )\n","        ################################################################################################################\n","        model.save(config[\"model_path\"] + str(fold))\n","        keras.backend.clear_session()\n","\n","def train_age_seperated(config):\n","    data = np.load(config[\"data_path\"], allow_pickle=True)\n","    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n","    y = y_apnea + y_hypopnea\n","    ########################################################################################\n","    for i in range(10):\n","        x[i], y[i] = shuffle(x[i], y[i])\n","        x[i] = np.nan_to_num(x[i], nan=-1)\n","        if config[\"regression\"]:\n","            y[i] = np.sqrt(y[i])\n","            y[i][y[i] != 0] += 2\n","        else:\n","            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n","\n","        x[i] = x[i][:, :, config[\"channels\"]]  # CHANNEL SELECTION\n","\n","    ########################################################################################\n","    first = True\n","    for i in range(10):\n","        if first:\n","            x_train = x[i]\n","            y_train = y[i]\n","            first = False\n","        else:\n","            x_train = np.concatenate((x_train, x[i]))\n","            y_train = np.concatenate((y_train, y[i]))\n","\n","    model = get_model(config)\n","    if config[\"regression\"]:\n","        model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n","        early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","\n","    else:\n","        model.compile(\n","            optimizer=\"adam\",\n","            loss=BinaryCrossentropy(),\n","            metrics=[keras.metrics.Precision(), keras.metrics.Recall()]\n","        )\n","        early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","    lr_scheduler = LearningRateScheduler(lr_schedule)\n","    model.fit(\n","        x=x_train,\n","        y=y_train,\n","        batch_size=512,\n","        epochs=config[\"epochs\"],\n","        validation_split=0.1,\n","        callbacks=[early_stopper, lr_scheduler]\n","    )\n","    ################################################################################################################\n","    model.save(config[\"model_path\"] + str(0))\n","    keras.backend.clear_session()"]},{"cell_type":"markdown","metadata":{"id":"gX6bCcZNuxmz"},"source":["# Results\n","\n",">NOTE: We trained the model previously, now going to evaluate it\n","\n","In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n","\n","Please test and report results for all experiments that you run with:\n","\n","*   specific numbers (accuracy, AUC, RMSE, etc)\n","*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n","\n","See the following files for testing and evaluation:\n","\n","- https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/test.py\n","- https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/metrics.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LjW9bCkouv8O"},"outputs":[],"source":["# metrics to evaluate my model\n","\n","# plot figures to better show the results\n","\n","# it is better to save the numbers and figures for your presentation."]},{"cell_type":"markdown","metadata":{"id":"8EAWAy_LwHlV"},"source":["## Model comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOdhGrbwwG71"},"outputs":[],"source":["# compare you model with others\n","# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"]},{"cell_type":"markdown","metadata":{"id":"qH75TNU71eRH"},"source":["# Discussion\n","\n","In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n","  * Make assessment that the paper is reproducible or not.\n","  * Explain why it is not reproducible if your results are kind negative.\n","  * Describe What was easy and What was difficult during the reproduction.\n","  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n","  * What will you do in next phase.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E2VDXo5F4Frm"},"outputs":[],"source":["# no code is required for this section\n","'''\n","if you want to use an image outside this notebook for explanaition,\n","you can read and plot it here like the Scope of Reproducibility\n","'''"]},{"cell_type":"markdown","metadata":{"id":"SHMI2chl9omn"},"source":["# References\n","\n","1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xmVuzQ724HbO"},"source":["# Feel free to add new sections"]}],"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5","timestamp":1709153069464}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
