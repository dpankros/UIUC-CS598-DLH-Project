{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper reproduction: Bringing At-home Pediatric Sleep Apnea Tesing Closer to Reality: A Multi-Modal Transformer Approach\n",
    "\n",
    "_Aaron Schlesinger and Dave Pankros_\n",
    "\n",
    "This notebook is a draft submission of our paper reproduction for CS 598 - Deep Learning for healthcare. In it, we aim to reproduce the findings in the paper _Bringing At-home Pediatric Sleep Apnea Testing Closer to Reality: A Multi-Modal Transformer Approach_ $^1$.\n",
    "\n",
    "The code herein is functional, with some limitations around data, discussed below. The public GitHub repository in which we do our work and from which we derived this notebook is located at [github.com/arschles/UIUC-CS598-DLH-Project](https://github.com/arschles/UIUC-CS598-DLH-Project). The code therein is organized as a standard Python project, and the repository contains comprehensive instructions on running it.\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "Our work to get to this point has been extensive and varied. In the subsequent sections, you will see extensive prose and code detailing the work we've done, but below is a high-level summary, in bulleted form:\n",
    "\n",
    "- We have acquired a subset of the data used in the paper\n",
    "- We have completed data preprocessing and data loading\n",
    "- We have ensured the model trains sufficiently\n",
    "- We have done standard evaluations of our trained model\n",
    "\n",
    "From here, we plan to acquire as much additional data as we can (this is not under our control, as described below), clean up the code, perform ablation studies, and generate graphs and other visuals as necessary to support and illustrate our evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# License\n",
    "The data used by this dataset is subject to a license.  While we are not sharing the original data in its raw form, we are sharing derivations of that data like pre-trained model checkpoints. To facilitate the goals of this assignment and class and to honor the spirit of the [license](https://physionet.org/content/nch-sleep/view-license/3.1.0/), we require that by opening and running this notebook you agree that you:\n",
    "\n",
    "1. Will not attempt to reverse engineer the data into a form other than provided,\n",
    "2. Will not attempt to identify or de-anonymize any individual or institution in the dataset,\n",
    "3. Will not share or re-use the data in any form,\n",
    "4. Will not use the data for any purpose other than this assignment, and\n",
    "5. Maintain a up-to-date cerification in human research subject protection and HIPAA regulations.\n",
    "\n",
    "The data license is discussed in more detail [below](#data-licensing-note)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "Much effort went into making the [original paper repository](https://github.com/healthylaife/Pediatric-Apnea-Detection) runnable.  The repository was lacking any documentation including even python version and package versions.  It was built exclusively to be run on Windows and lacked any attempt at cross-platform support.  We ran into several cases where the code, as supplied, could never have been run in the provided state.  In one instance, for example, the arguments to a function were illegal and, after reaching out to the author and receiving no reply, we used our best judgment at a solution.\n",
    "\n",
    "With that in mind, we are attempting to reproduce results consistent with the original paper, but there may be differences due to these changes or other instances of errors or omissions that did not cause a code failure and that may have been too subtle to be caught in our initial passes over the code.  These differences will, inevitably, cause deviations between our results and the results presented in the paper.\n",
    "\n",
    "We have, however, undertaken in [our fork of the original code](https://github.com/arschles/UIUC-CS598-DLH-Project/tree/main/original) to provide more information to aid reproducibility (including a `requirements.txt` file and other detailed dependency information), made the code cross-platform where it was not, and provide for information about how to preprocess and run the code using standard tools like bash and make.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Sleep apnea in children is a major health problem that affects between one to five percent of US children, but differs from sleep apnea in adults in its clinical causes and characteristics.  Thus, previously-created methods for detecting adult sleep apnea may be ineffective at detecting pediatric sleep apnea.\n",
    "\n",
    "## Background\n",
    "While there are numerous testing tools and algorithmic methods for detecting adult sleep apnea, the same tools and methods are unavailable for pediatric sleep apnea (PSA) due to these differences. Detecting pediatric sleep apnea more quickly and easily can lead to earlier clinical intervention in the child's care and ultimately prevent the wide variety of health issues commonly caused by OSA.  Polysomnography (PSG) is the standard method for formal sleep apnea diagnosis, but is generally performed in a dedicated facility where a patient can be monitored overnight. Polysomnography involves collecting various continuous-time signals, including electroencephalogram (EEG), electrooculogram (EOG), electrocardiogram (ECG), pulse oximetry (SpO2), end-tidal carbon dioxide (ETCO2), respiratory inductance plethysmography (RIP), nasal airflow, and oral airflow.  While effective, PSG is, however, complex, costly and requires a dedicated sleep lab.  \n",
    "\n",
    "### State of the Art\n",
    "Current methods target adults and, for reasons stated earlier, are ineffective at diagnosing PSA in childen. Very little work has been done in the scope of pediatric sleep apnea. In general, full Polysomnography data is hard to find and thus, much research has focused on determining the Apnea-Hypopnea Index (AHI) from ECG and SpO2 signals.   \n",
    "\n",
    "While transformers are used commonly in general deep learning models, they are much less prevalent in the detection of sleep apnea.  Two studies described in this paper used transformers to determine sleep stages (one in adults, one in children), while another used a hybrid CNN/transformer model of obstructive sleep apnea (OSA) detection.  \n",
    "\n",
    "## Paper\n",
    "The paper proposes to study the gaps in Obstructive Sleep Apnea Hypopnea Syndrome (OSAHS) in children vis-a-vis adults. The paper then suggests a custom transformer-based method and data representation for PSA detection, and identifies the polysomography modalities that most closely correlate to OSAHS in children.\n",
    "\n",
    "The results presented in the paper portray state-of-the-art results. \n",
    "![Figure 2](./images/figure_2.png)\n",
    "\n",
    "If diagnosing pediatric sleep apnea can be done with low-cost, consumer hardware, then the costs of pediatric sleep apena diagnosis decrease and children's health improves.  Additionally, lowering the cost of dianoses would also enable access for underserved, including rural, populations with limited access to sleep labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility\n",
    "We will generally investigate whether we can beat state-of-the-art results from Polysomnography (PSG) studies in pediatric sleep apnea detection using the transformer-based model proposed in the paper and discussed above. More specifically, we will focus on testing the following hypotheses:\n",
    "\n",
    "1. Whether the proposed model can achieve results from signals more easily collected than PSG. As in the paper, we will focus on ECG and $SpO_2$ signals, and\n",
    "2. Whether the results support this method being effective as a dedicated method for in-lab sleep studies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "In this section, we detail how we acquire, process and load the relevant data, and how we train and evaluate the model given those data.  We will exlain the procedure and code in this notebook with the understanding that we did not process the data, train the model, or evaluate the model using this notebook.  True reproduction of our environment is explained in the Repository Setup and Evaluation section, immediately following.\n",
    "\n",
    "# Repository Setup and Evalution\n",
    "\n",
    "We undertook to make this repository as complete and easily setup as possible. Much of these instructions are listed in the Readme.md in [our repository](https://github.com/arschles/UIUC-CS598-DLH-project).  This assumes that the environment variable DATA_ROOT has been set and at least part of the raw  NCH dataset has been downloaded to `$DATA_ROOT`. \n",
    "1. Clone the repository from `https://github.com/arschles/UIUC-CS598-DLH-project`.  E.g. `git clone https://github.com/arschles/UIUC-CS598-DLH-project`\n",
    "2. Change to the code directory: `cd original`\n",
    "3. Install dependencies: `pip install -r requirements.txt`  (of course, if using conda, poetry, venv or other virtual environment there may be additional steps or a different procedure here)\n",
    "4. Run: `make train`.  If a different number of EPOCHS for training is desired or if the data directory should be changed, the following command can be used, `DLHPROJ_NUM_EPOCHS=200 DATA_ROOT=/a_new_data_dir make train`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "##  Data\n",
    "\n",
    "### Data Licensing Note\n",
    "\n",
    "**Due to the license required to access the data, we cannot provide the raw dataset with this notebook because doing so would be a violation of terms 3 and 4 of the [PhysioNetCredential Health Data License 1.5.0](https://physionet.org/content/nch-sleep/view-license/3.1.0/).**\n",
    "\n",
    "Because we cannot supply original raw data, we will discuss the processing of the data and the relevant processing code with example output, but the provided code **will not actually process data** within this notebook.  Since it is not subject to the aforementioned license, we have stored preprocessed data elsewhere and will load it in relevant sections of this notebook. From that point, further processing will be performed in this notebook.\n",
    "\n",
    ">Before we proceed, we want to stress that this preprocessed data -- and the model checkpoint data we'll discuss later -- is derived from licensed data for which one must pass a training course to access. This notebook includes instructions and code to access these resources. **By proceeding past this section, you must agree to adhere to the requirements outlined in the license section above**.\n",
    "\n",
    "### Source(s)\n",
    "The original paper used Nationwide Childrenâ€™s Hospital (NCH) Sleep Data Bank (Lee et al., 2022), and Childhood Adenotonsillectomy Trial (CHAT) dataset (Marcus et al., 2013; Redline et al., 2011).  Each of these datasets are collected from actual sleep studies, anonymized and made available for research.  At the time of this writing, we have been unable to gain approval to access the CHAT dataset so are unable to use it.  We currently have access to the [NCH dataset through physionet.org](https://physionet.org/content/nch-sleep/3.1.0/).  While we have been downloading this data for over a week, download speeds are capped at around 600KB/s.  We have currently downloaded around 400GB of 2.1TB total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset summary\n",
    "Since the paper provides extensive statistics on the data used in this study, we have not undertaken to perform our own calculations. The paper-provided measures are as follows:\n",
    "\n",
    "### Demographics of the Datasets\n",
    "|          |                         |     NCH      |    CHAT     |\n",
    "|---------:|:------------------------|:------------:|:-----------:|\n",
    "|          | Number of Patients      |       3673   | 453         |\n",
    "|          | Number of Sleep Studies |     3984     |     453     |\n",
    "|  **Sex** |                         |              |             |\n",
    "|          | Male                    |     2068     |     219     |\n",
    "|          | Female                  |     1604     |     234     |\n",
    "| **Race** |                         |              |             |\n",
    "|          | Asian                   |      93      |      8      |\n",
    "|          | Black                   |     738      |     252     |\n",
    "|          | White                   |     2433     |     161     |\n",
    "|          | Other                   |     409      |     32      |\n",
    "|          | **Age (years/mean)**    | \\[0-30\\]/8.8 | \\[5-9\\]/6.5 |\n",
    "\n",
    "\n",
    "### Data Statistics \n",
    "\n",
    "|                Event |   NCH   | CHAT  |\n",
    "|---------------------:|:-------:|:-----:|\n",
    "|  Oxygen Desaturation | 215280  | 65006 |\n",
    "|       Oximeter Event | 161641  | 9,864 |\n",
    "|          EEG arousal | 146052  |  --   |\n",
    "|   Respiratory Events |         |       |\n",
    "|             Hypopnea |  14522  | 15871 |\n",
    "| Obstructive Hypopnea |  42179  |  --   |\n",
    "|    Obstructive apnea |  15782  | 7075  |\n",
    "|        Central apnea |  6938   | 3656  |\n",
    "|          Mixed apnea |  2650   |  --   |\n",
    "|         Sleep Stages |         |       |\n",
    "|                 Wake | 665676  | 10282 |\n",
    "|                   N1 | 128410  | 13578 |\n",
    "|                   N2 | 1383765 | 19985 |\n",
    "|                   N3 | 875486  | 9981  |\n",
    "|                  REM | 611320  | 3283  |    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "### Data Normalization: Interpolation, Resampling and Tokenization\n",
    "The raw data consist of 1) regular time-series, 2) irregular time-teries, and 3) tabular data.  Additionally, the time-series data may be provided in various frequencies.  To merge all the different data types into a coherent dataset suitable for training and testing, the following steps must be performed:\n",
    "    \n",
    "1. Each irregular time series must be interpolated to convert it into a regular time series.\n",
    "1. All the time series data must be resampled into a uniform frequency, $f_{sampling}$, for all sleep studies and modalities.\n",
    "1. The tabular data is added to the time series as a constant signal (i.e. repeated tokens)\n",
    "1. The combined data is split into $i$ equal-length tokens of time $S$, where each modality consists of $S * f_{sampling}$ data points\n",
    "    \n",
    "This data can then be split and passed to the model for training and testing.\n",
    "\n",
    "### Splitting\n",
    "\n",
    "This paper utilizes a custom stratified $k$-fold cross validation to ensure 1) an equal number of patients are assigned to each fold, and to 2) normalize the number of positive samples in each fold.  Pseudocode of this method is:\n",
    "\n",
    "![Algorithm](./images/algorithm_1.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Apnea-Hypopnea index\n",
    "\n",
    "The [Apnea-Hypopnea index (AHI)](https://www.sleepfoundation.org/sleep-apnea/ahi) is an important quantification of the severity of sleep apnea. Its derivation for a single night of sleep is as follows:\n",
    "\n",
    "$$\n",
    "\\frac {N_{apneic} + N_{hypopneic}} {H}\n",
    "$$\n",
    "\n",
    "Where $N_{apneic}$ is the number of apneic events (instances where the person stops breathing), $N_{hypopneic}$ is the number of hypopneic events (instances where the airflow is blocked and the person's breathing becomes more shallow), and $H$ is the total number of hours of sleep for the night.\n",
    "\n",
    "Since it can be derived from an entire night of sleep, the AHI is a very useful measure to summarize, with relatively modest loss of information, a person's apnea/hypopnea activity in a given night of sleep.\n",
    "\n",
    "Below is the code to create a TSV (tab-separated file) file containing AHI measures for a given sleep study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating AHI file from data at /root/data/physionet.org, outputting to /root/data/physionet.org/AHI.csv\n",
      "creating AHI from 188 sleep studies in /root/data/physionet.org/files/nch-sleep/3.1.0/Sleep_Data and outputting the AHI results to /root/data/physionet.org/AHI.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "APNEA_EVENT_DICT = {\n",
    "    \"Obstructive Apnea\": 2,\n",
    "    \"Central Apnea\": 2,\n",
    "    \"Mixed Apnea\": 2,\n",
    "    \"apnea\": 2,\n",
    "    \"obstructive apnea\": 2,\n",
    "    \"central apnea\": 2,\n",
    "    \"apnea\": 2,\n",
    "    \"Apnea\": 2,\n",
    "}\n",
    "\n",
    "HYPOPNEA_EVENT_DICT = {\n",
    "    \"Obstructive Hypopnea\": 1,\n",
    "    \"Hypopnea\": 1,\n",
    "    \"hypopnea\": 1,\n",
    "    \"Mixed Hypopnea\": 1,\n",
    "    \"Central Hypopnea\": 1,\n",
    "}\n",
    "\n",
    "def _num_sleep_hours(\n",
    "    sleep_study_metadata: pd.DataFrame,\n",
    "    pat_id: int,\n",
    "    study_id: int,\n",
    ") -> int:\n",
    "    ssm = sleep_study_metadata\n",
    "    sleep_duration_df = ssm.loc[\n",
    "        (ssm[\"STUDY_PAT_ID\"] == pat_id) &\n",
    "        (ssm[\"SLEEP_STUDY_ID\"] == study_id)\n",
    "    ]\n",
    "    assert len(sleep_duration_df) == 1, (\n",
    "        f'expected just 1 study with patient {pat_id} and study '\n",
    "        f'{study_id}, but got {len(sleep_duration_df)} instead'\n",
    "    )\n",
    "    sleep_duration_datetime = datetime.strptime(\n",
    "        str(\n",
    "            sleep_duration_df[\n",
    "                \"SLEEP_STUDY_DURATION_DATETIME\"\n",
    "            ].iloc[0]\n",
    "        ).strip(),\n",
    "        \"%H:%M:%S\"\n",
    "    )\n",
    "    return sleep_duration_datetime.hour\n",
    "\n",
    "\n",
    "def _ahi_for_study(\n",
    "    sleep_study_metadata: pd.DataFrame,\n",
    "    sleep_study: pd.DataFrame,\n",
    "    pat_id: int,\n",
    "    study_id: int,\n",
    ") -> float:\n",
    "    '''\n",
    "    Calculate the apnea-hypopnea index (AHI) for a given sleep study.\n",
    "    All apnea and hypopnea events will be counted from the sleep_study\n",
    "    DataFrame, and then divided by the total sleep duration, which\n",
    "    will be gotten from the sleep_study_metadata DataFrame. The result will\n",
    "    be returned as a float\n",
    "\n",
    "    For more on AHI, see the following link:\n",
    "\n",
    "    https://www.sleepfoundation.org/sleep-apnea/ahi\n",
    "\n",
    "    :param sleep_study_metadata\n",
    "        the DataFrame that has at least the following columns in order\n",
    "        from left to right:\n",
    "        STUDY_PAT_ID,\n",
    "        SLEEP_STUDY_ID,\n",
    "        SLEEP_STUDY_START_DATETIME,\n",
    "        SLEEP_STUDY_DURATION_DATETIME\n",
    "    :param sleep_study\n",
    "        the data from the sleep study in which we're interested\n",
    "    :param pat_id\n",
    "        the ID of the patient on whom the given study was done\n",
    "    :param study_id\n",
    "        the ID of the study\n",
    "    \n",
    "    :return\n",
    "        the AHI for the given study, as a float value\n",
    "    '''\n",
    "\n",
    "    # example tsv file:\n",
    "    # onset duration description\n",
    "    # 29766.7421875\t11.0546875\tObstructive Hypopnea\n",
    "\n",
    "    df = sleep_study\n",
    "    hypopnea_keys = set(HYPOPNEA_EVENT_DICT.keys())\n",
    "    apnea_keys = set(APNEA_EVENT_DICT.keys())\n",
    "\n",
    "    hypopnea_events = df.loc[df[\"description\"].isin(hypopnea_keys)]\n",
    "    apnea_events = df.loc[df[\"description\"].isin(apnea_keys)]\n",
    "    total_num_events = len(hypopnea_events) + len(apnea_events)\n",
    "    sleep_hours = float(_num_sleep_hours(\n",
    "        sleep_study_metadata,\n",
    "        pat_id,\n",
    "        study_id,\n",
    "    ))\n",
    "    return float(total_num_events) / sleep_hours\n",
    "\n",
    "\n",
    "def _parse_ss_tsv_filename(filename: str) -> tuple[int, int]:\n",
    "    '''\n",
    "    given a sleep study filename like `10048_24622.tsv`, that represents\n",
    "    <patient_id>_<sleep_study_id>.tsv, return a 2-tuple containing\n",
    "    the patient ID in element 1 and sleep study ID in element 2\n",
    "    '''\n",
    "    if not filename.endswith(\".tsv\"):\n",
    "        raise FileNotFoundError(\n",
    "            f\"expected {filename} to end with .tsv but it didn't\"\n",
    "        )\n",
    "\n",
    "    underscore_spl = filename.split(\"/\")[-1][:-4].split(\"_\")\n",
    "    if len(underscore_spl) != 2:\n",
    "        raise FileNotFoundError(f'malformed filename {filename}')\n",
    "    [pat_id, study_id] = underscore_spl\n",
    "    return (int(pat_id), int(study_id))\n",
    "\n",
    "\n",
    "def _write_tsv(out_filename: str, data: list[tuple[str, str, float]]):\n",
    "    with open(out_filename, 'w', newline='') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter=',')\n",
    "        # in ./preprocessing.py, we need to have at least 'Study'\n",
    "        # and 'AHI'. Since they chose PascalCase, I extended that usage\n",
    "        # to patient ID.\n",
    "        writer.writerow((\"PatID\", \"Study\", \"AHI\"))\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def calculate_ahi(\n",
    "    sleep_study_metadata_file: str,\n",
    "    sleep_study_root: str,\n",
    "    out_file: str,\n",
    ") -> None:\n",
    "    '''\n",
    "    Calculate the AHI values from a sleep study's metadata file and all the \n",
    "    sleep measurements in a given directory. See the _ahi_for_study function\n",
    "    for an overview of how the metadata file and sleep measurement files should\n",
    "    be structured.\n",
    "\n",
    "    :param sleep_study_metadata_file - the metadata file summarizing the sleep\n",
    "        study\n",
    "    :param sleep_study_root - the root directory containing all the individual\n",
    "        sleep study measures\n",
    "    \n",
    "    :return out_file - the name of the file to which to write the AHI values\n",
    "        in TSV format\n",
    "\n",
    "    '''\n",
    "    metadata_df = pd.read_csv(\n",
    "        sleep_study_metadata_file,\n",
    "        sep=\",\"\n",
    "    )\n",
    "\n",
    "    tsv_files = [\n",
    "        f for f in os.listdir(sleep_study_root)\n",
    "        if f.endswith(\".tsv\")\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        f\"creating AHI from {len(tsv_files)} sleep studies in \"\n",
    "        f\"{sleep_study_root} and outputting the AHI results to {out_file}\"\n",
    "    )\n",
    "\n",
    "    # each tuple is (patient_id, study_id, AHI)\n",
    "    results: list[tuple[str, str, float]] = []\n",
    "    for tsv_file in tsv_files:\n",
    "        filename = os.path.join(sleep_study_root, tsv_file)\n",
    "        pat_id, study_id = _parse_ss_tsv_filename(filename)\n",
    "        sleep_study_df = pd.read_csv(\n",
    "            filename,\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "        ahi = _ahi_for_study(\n",
    "            metadata_df,\n",
    "            sleep_study_df,\n",
    "            pat_id,\n",
    "            study_id,\n",
    "        )\n",
    "        results.append((pat_id, study_id, ahi))\n",
    "    _write_tsv(out_file, results)\n",
    "\n",
    "\n",
    "def generate_ahi_file(data_root: str, out_file: str) -> None:\n",
    "    sleep_study_metadata_file = os.path.join(\n",
    "        data_root,\n",
    "        \"files\",\n",
    "        \"nch-sleep\",\n",
    "        \"3.1.0\",\n",
    "        \"Health_Data\",\n",
    "        \"SLEEP_STUDY.csv\"\n",
    "    )\n",
    "    sleep_study_root = os.path.join(\n",
    "        data_root,\n",
    "        \"files\",\n",
    "        \"nch-sleep\",\n",
    "        \"3.1.0\",\n",
    "        \"Sleep_Data\"\n",
    "    )\n",
    "\n",
    "    calculate_ahi(\n",
    "        sleep_study_metadata_file,\n",
    "        sleep_study_root,\n",
    "        out_file=out_file,\n",
    "    )\n",
    "\n",
    "# by default, we do not do preprocessing since raw data is licensed. if you \n",
    "# have access to raw data, download it to your machine, set this variable to \n",
    "# True and set PHYSIONET_ROOT to the location on disk of your data's\n",
    "# physionet.org directory\n",
    "SHOULD_PREPROCESS = True\n",
    "PHYSIONET_ROOT=\"/root/data/physionet.org\"\n",
    "if SHOULD_PREPROCESS:\n",
    "    ahi_out = f'{PHYSIONET_ROOT}/AHI.csv'\n",
    "    print(f\"generating AHI file from data at {PHYSIONET_ROOT}, outputting to {ahi_out}\")\n",
    "    generate_ahi_file(PHYSIONET_ROOT, ahi_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing code\n",
    "\n",
    "With the AHI values calculated and saved to a TSV file, we can move onto data preprocessing. Roughly speaking, the goals of preprocessing are as follows for each sleep study:\n",
    "\n",
    "- Ignore the study if its AHI is lower than a threshold\n",
    "- Collate and pad as necessary the relevant sleep events in the study\n",
    "- Resample samples from raw data as necessary\n",
    "    - The motivation behind, and method for Resampling was discussed above\n",
    "- Write results to a [compressed numpy representation](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os.path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import mne\n",
    "import numpy as np\n",
    "from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n",
    "from biosppy.signals import tools as st\n",
    "from mne import make_fixed_length_events\n",
    "from scipy.interpolate import splev, splrep\n",
    "from itertools import compress\n",
    "\n",
    "mne.set_log_file('log.txt', overwrite=False)\n",
    "\n",
    "CHUNK_DURATION = 30.0\n",
    "FREQ = 64.0\n",
    "\n",
    "POS_EVENT_DICT: dict[str, int] = {\n",
    "    \"Obstructive Hypopnea\": 1,\n",
    "    \"Hypopnea\": 1,\n",
    "    \"hypopnea\": 1,\n",
    "    \"Mixed Hypopnea\": 1,\n",
    "    \"Central Hypopnea\": 1,\n",
    "\n",
    "    \"Obstructive Apnea\": 2,\n",
    "    \"Central Apnea\": 2,\n",
    "    \"Mixed Apnea\": 2,\n",
    "    \"apnea\": 2,\n",
    "    \"obstructive apnea\": 2,\n",
    "    \"central apnea\": 2,\n",
    "    \"Apnea\": 2,\n",
    "}\n",
    "\n",
    "WAKE_DICT: dict[str, int] = {\n",
    "    \"Sleep stage W\": 10\n",
    "}\n",
    "\n",
    "\n",
    "########################################## Annotation Modifier functions ##########################################\n",
    "def identity(df):\n",
    "    return df\n",
    "\n",
    "\n",
    "def apnea2bad(df):\n",
    "    df = df.replace(r'.*pnea.*', 'badevent', regex=True)\n",
    "    print(\"bad replaced!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def wake2bad(df):\n",
    "    return df.replace(\"Sleep stage W\", 'badevent')\n",
    "\n",
    "\n",
    "def change_duration(df, label_dict=POS_EVENT_DICT, duration=CHUNK_DURATION):\n",
    "    for key in label_dict:\n",
    "        df.loc[df.description == key, 'duration'] = duration\n",
    "    print(\"change duration!\")\n",
    "    return df\n",
    "\n",
    "def preprocess(i, annotation_modifier, out_dir, ahi_dict):\n",
    "    is_apnea_available, is_hypopnea_available = True, True\n",
    "    study = ss.data.study_list[i]\n",
    "\n",
    "    # print(f\"loading study {study}\")\n",
    "    raw = ss.data.load_study(study, annotation_modifier, verbose=True)\n",
    "\n",
    "    ########################################   CHECK CRITERIA FOR SS   #################################################\n",
    "    if not all([name in raw.ch_names for name in channels]):\n",
    "        print(\"study \" + str(study) + \" skipped since insufficient channels\", file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "    ahi_value = ahi_dict.get(study,None)\n",
    "    if ahi_value is None:\n",
    "        print(ahi_dict)\n",
    "        print(\"study \" + str(study) + \" skipped since AHI is MISSING.  Is AHI.csv out of date?\", file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "    if ahi_value < THRESHOLD:\n",
    "        print(\"study \" + str(study) + \" skipped since low AHI ---  AHI = \" + str(ahi_value), file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        apnea_events, event_ids = mne.events_from_annotations(\n",
    "            raw,\n",
    "            event_id=POS_EVENT_DICT,\n",
    "            chunk_duration=1.0,\n",
    "            verbose=None\n",
    "        )\n",
    "        # print('|')\n",
    "    except ValueError:\n",
    "        print(\"No Chunk found!\", file=sys.stderr)\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "    ########################################   CHECK CRITERIA FOR SS   #################################################\n",
    "    print(str(i) + \"---\" + str(datetime.now().time().strftime(\"%H:%M:%S\")) + ' --- Processing %d' % i)\n",
    "\n",
    "    try:\n",
    "        apnea_events, event_ids = mne.events_from_annotations(\n",
    "            raw,\n",
    "            event_id=APNEA_EVENT_DICT,\n",
    "            chunk_duration=1.0,\n",
    "            verbose=None\n",
    "        )\n",
    "    except ValueError:\n",
    "        is_apnea_available = False\n",
    "\n",
    "    try:\n",
    "        hypopnea_events, event_ids = mne.events_from_annotations(\n",
    "            raw,\n",
    "            event_id=HYPOPNEA_EVENT_DICT,\n",
    "            chunk_duration=1.0,\n",
    "            verbose=None\n",
    "        )\n",
    "    except ValueError:\n",
    "        is_hypopnea_available = False\n",
    "\n",
    "    wake_events, event_ids = mne.events_from_annotations(\n",
    "        raw,\n",
    "        event_id=WAKE_DICT,\n",
    "        chunk_duration=1.0,\n",
    "        verbose=None\n",
    "    )\n",
    "    ####################################################################################################################\n",
    "    sfreq = raw.info['sfreq']\n",
    "    tmax = CHUNK_DURATION - 1. / sfreq\n",
    "\n",
    "    raw = raw.pick_channels(channels, ordered=True)\n",
    "    fixed_events = make_fixed_length_events(\n",
    "        raw,\n",
    "        id=0,\n",
    "        duration=CHUNK_DURATION,\n",
    "        overlap=0.\n",
    "    )\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        fixed_events,\n",
    "        event_id=[0],\n",
    "        tmin=0,\n",
    "        tmax=tmax,\n",
    "        baseline=None,\n",
    "        preload=True,\n",
    "        proj=False,\n",
    "        verbose=None\n",
    "    )\n",
    "    epochs.load_data()\n",
    "    if sfreq != FREQ:\n",
    "        epochs = epochs.resample(FREQ, npad='auto', n_jobs=4, verbose=None)\n",
    "    data = epochs.get_data()\n",
    "    ####################################################################################################################\n",
    "    if is_apnea_available:\n",
    "        apnea_events_set = set((apnea_events[:, 0] / sfreq).astype(int))\n",
    "    if is_hypopnea_available:\n",
    "        hypopnea_events_set = set((hypopnea_events[:, 0] / sfreq).astype(int))\n",
    "    wake_events_set = set((wake_events[:, 0] / sfreq).astype(int))\n",
    "\n",
    "    starts = (epochs.events[:, 0] / sfreq).astype(int)\n",
    "\n",
    "    labels_apnea = []\n",
    "    labels_hypopnea = []\n",
    "    labels_wake = []\n",
    "    total_apnea_event_second = 0\n",
    "    total_hypopnea_event_second = 0\n",
    "\n",
    "    for seq in range(data.shape[0]):\n",
    "        epoch_set = set(range(starts[seq], starts[seq] + int(CHUNK_DURATION)))\n",
    "        if is_apnea_available:\n",
    "            apnea_seconds = len(apnea_events_set.intersection(epoch_set))\n",
    "            total_apnea_event_second += apnea_seconds\n",
    "            labels_apnea.append(apnea_seconds)\n",
    "        else:\n",
    "            labels_apnea.append(0)\n",
    "\n",
    "        if is_hypopnea_available:\n",
    "            hypopnea_seconds = len(hypopnea_events_set.intersection(epoch_set))\n",
    "            total_hypopnea_event_second += hypopnea_seconds\n",
    "            labels_hypopnea.append(hypopnea_seconds)\n",
    "        else:\n",
    "            labels_hypopnea.append(0)\n",
    "\n",
    "        labels_wake.append(len(wake_events_set.intersection(epoch_set)) == 0)\n",
    "    ####################################################################################################################\n",
    "    print(study + \"    HAMED    \" + str(len(labels_wake) - sum(labels_wake)))\n",
    "    data = data[labels_wake, :, :]\n",
    "    labels_apnea = list(compress(labels_apnea, labels_wake))\n",
    "    labels_hypopnea = list(compress(labels_hypopnea, labels_wake))\n",
    "\n",
    "    out_name = study + \"_\" + str(total_apnea_event_second) + \"_\" + str(total_hypopnea_event_second)\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    # print(f\"Saving {study} to {out_path}.npz\")\n",
    "    np.savez_compressed(out_path, data=data, labels_apnea=labels_apnea, labels_hypopnea=labels_hypopnea)\n",
    "\n",
    "    return data.shape[0]\n",
    "\n",
    "def preprocess(\n",
    "    ahi_path: str,\n",
    "    out_folder: str,\n",
    "    n_studies: int = 3984,\n",
    "    n_workers: int = 3\n",
    "):\n",
    "    if not os.path.exists(ahi_path):\n",
    "        return FileNotFoundError(f'AHI file {ahi_path} was not found!')\n",
    "\n",
    "    ahi = pd.read_csv(ahi_path)\n",
    "    # filename is <patient_id>_<study>\n",
    "    filenames = ahi['PatID'].astype(str) + '_' + ahi['Study'].astype(str)\n",
    "    # ahi_dict = dict(zip(ahi.Study, ahi.AHI))\n",
    "    ahi_dict = dict(zip(filenames, ahi['AHI']))\n",
    "    ss.__init__()\n",
    "\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "\n",
    "    if n_workers < 2:\n",
    "        for idx in range(n_studies):\n",
    "            preprocess(\n",
    "                ahi_path=idx,\n",
    "                out_folder=identity,\n",
    "                n_studies=out_folder,\n",
    "                n_workers=ahi_dict\n",
    "            )\n",
    "    else:\n",
    "        with concurrent.futures.ThreadPoolExecutor(\n",
    "            max_workers=n_workers\n",
    "        ) as executor:\n",
    "            executor.map(\n",
    "                preprocess,\n",
    "                range(n_studies),\n",
    "                [identity] * n_studies,\n",
    "                [out_folder] * n_studies,\n",
    "                [ahi_dict] * n_studies\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "After data are preprocessed, we are left with a compressed numpy array file, also called an `npz` file. At this point, we are ready to take the final step before model training - data loading.\n",
    "\n",
    "The data loader code requires familiar collation and padding logic shown below. Notably, we make use of [TensorFlow's ragged tensors](https://www.tensorflow.org/guide/ragged_tensor) to simply and easily handle the task of padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import tensorflow as tf\n",
    "\n",
    "def max_dimensions(\n",
    "        lst: list[Any],\n",
    "        level: int=0,\n",
    "        max_dims: list[int] | None = None\n",
    ") -> tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Finds the maximum dimension for each level of a nested list structure\n",
    "\n",
    "    :param lst: The list for which to get dimensions\n",
    "    :param level: INTERNAL USE ONLY (the dimension we are processing)\n",
    "    :param max_dims: INTERNAL USE ONLY (the current array of maximums)\n",
    "    :return: a tuple of sizes, similar to torch.Tensor.shape()\n",
    "    \"\"\"\n",
    "    if max_dims is None:\n",
    "        max_dims = []\n",
    "\n",
    "    # Extend the max_dims list if this is the deepest level we've encountered so far\n",
    "    if level >= len(max_dims):\n",
    "        max_dims.append(len(lst))\n",
    "    else:\n",
    "        max_dims[level] = max(max_dims[level], len(lst))\n",
    "\n",
    "    for item in lst:\n",
    "        if isinstance(item, list):\n",
    "            # Recursively process each sublist\n",
    "            max_dimensions(item, level + 1, max_dims)\n",
    "\n",
    "    return tuple(max_dims)\n",
    "\n",
    "def pad_lists(\n",
    "    lst: list[Any],\n",
    "    pad_with: int = 0\n",
    ") -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    Given a ragged nested list structure `lst` (i.e. where the length\n",
    "    in each dimension is not uniform), return a new list with all\n",
    "    levels of the list padded to the maximal length of any list in that \n",
    "    dimension. Padding elements will have the same value as given in \n",
    "    `pad_with`\n",
    "\n",
    "    For example, the return value of `pad_lists([[1], [1, 2]], 0)` will\n",
    "    be `[[1, 0], [1, 2]]`\n",
    "\n",
    "    :param lst: the list to pad, if it is ragged. if it's not, this function\n",
    "        is a no-op\n",
    "    :param pad_with: the value to use for padding\n",
    "    \"\"\"\n",
    "    # max_dims[0] is the number of elements (either lists or ints) we \n",
    "    # need in this dimension\n",
    "    return tf.ragged.constant(lst).to_tensor(pad_with).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, with padding and collating handled, data loading code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import resample\n",
    "from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n",
    "from biosppy.signals import tools as st\n",
    "from scipy.interpolate import splev, splrep\n",
    "from collate import max_dimensions, pad_lists\n",
    "\n",
    "# \"EOG LOC-M2\",  # 0\n",
    "# \"EOG ROC-M1\",  # 1\n",
    "# \"EEG C3-M2\",  # 2\n",
    "# \"EEG C4-M1\",  # 3\n",
    "# \"ECG EKG2-EKG\",  # 4\n",
    "#\n",
    "# \"RESP PTAF\",  # 5\n",
    "# \"RESP AIRFLOW\",  # 6\n",
    "# \"RESP THORACIC\",  # 7\n",
    "# \"RESP ABDOMINAL\",  # 8\n",
    "# \"SPO2\",  # 9\n",
    "# \"CAPNO\",  # 10\n",
    "\n",
    "######### ADDED IN THIS STEP #########\n",
    "# RRI #11\n",
    "# Ramp #12\n",
    "# Demo #13\n",
    "\n",
    "\n",
    "SIGS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "s_count = len(SIGS)\n",
    "\n",
    "THRESHOLD = 3\n",
    "FREQ = 64\n",
    "EPOCH_DURATION = 30\n",
    "ECG_SIG = 4\n",
    "\n",
    "# PATH = \"D:\\\\nch_30x64\\\\\"\n",
    "# OUT_PATH = \"D:\\\\nch_30x64\"\n",
    "\n",
    "_data_root = os.getenv(\n",
    "    \"DLHPROJ_DATA_ROOT\",\n",
    "    '/mnt/e/data/physionet.org'\n",
    ")\n",
    "AHI_PATH = os.path.join(_data_root,\"AHI.csv\")\n",
    "OUT_PATH = os.path.join(_data_root,\"nch_30x64.npz\")\n",
    "PATH = os.path.join(_data_root, \"nch_30x64\")\n",
    "\n",
    "\n",
    "def extract_rri(signal, ir, CHUNK_DURATION):\n",
    "    tm = np.arange(0, CHUNK_DURATION, step=1 / float(ir))  # TIME METRIC FOR INTERPOLATION\n",
    "\n",
    "    # print('filtering', signal, FREQ)\n",
    "    # TODO: Temporarily bypassed until we know how we want to handle this\n",
    "    # filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\", order=int(0.3 * FREQ),\n",
    "    #                                   frequency=[3, 45], sampling_rate=FREQ, )\n",
    "    filtered, _, _ = st.filter_signal(signal=signal, ftype=\"FIR\", band=\"bandpass\", order=int(0.3 * FREQ),\n",
    "                                      frequency=[3, 30], sampling_rate=FREQ, )\n",
    "    (rpeaks,) = hamilton_segmenter(signal=filtered, sampling_rate=FREQ)\n",
    "    (rpeaks,) = correct_rpeaks(signal=filtered, rpeaks=rpeaks, sampling_rate=FREQ, tol=0.05)\n",
    "\n",
    "    if 4 < len(rpeaks) < 200:  # and np.max(signal) < 0.0015 and np.min(signal) > -0.0015:\n",
    "        rri_tm, rri_signal = rpeaks[1:] / float(FREQ), np.diff(rpeaks) / float(FREQ)\n",
    "        ampl_tm, ampl_signal = rpeaks / float(FREQ), signal[rpeaks]\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, rri_signal, k=3), ext=1)\n",
    "        amp_interp_signal = splev(tm, splrep(ampl_tm, ampl_signal, k=3), ext=1)\n",
    "\n",
    "        return np.clip(rri_interp_signal, 0, 2), np.clip(amp_interp_signal, -0.001, 0.002)\n",
    "    else:\n",
    "        return np.zeros((FREQ * EPOCH_DURATION)), np.zeros((FREQ * EPOCH_DURATION))\n",
    "\n",
    "\n",
    "def load_data(path) -> tuple[list[Any], list[Any], list[Any]]:\n",
    "    # demo = pd.read_csv(\"../misc/result.csv\") # TODO\n",
    "\n",
    "    ahi = pd.read_csv(AHI_PATH)\n",
    "    filename = ahi.PatID.astype(str) + '_' + ahi.Study.astype(str)\n",
    "    ahi_dict = dict(zip(filename, ahi.AHI))\n",
    "    root_dir = os.path.expanduser(path)\n",
    "    file_list = os.listdir(root_dir)\n",
    "    length = len(file_list)\n",
    "\n",
    "    print(f\"Using AHI from {AHI_PATH}\")\n",
    "    print(f\"Using npz files from {root_dir}\")\n",
    "    print(f\"Files {file_list}\")\n",
    "\n",
    "    study_event_counts = {}\n",
    "    apnea_event_counts = {}\n",
    "    hypopnea_event_counts = {}\n",
    "    ######################################## Count the respiratory events ###########################################\n",
    "    for i in range(length):\n",
    "        # skip directories\n",
    "        if os.path.isdir(file_list[i]):\n",
    "            continue\n",
    "\n",
    "        # print(f\"Processing {file_list[i]}\")\n",
    "        try:\n",
    "            # parts = file_list[i].split(\"_\")\n",
    "            # parts[0] should be nch\n",
    "\n",
    "            patient_id = (file_list[i].split(\"_\")[0])\n",
    "            study_id = (file_list[i].split(\"_\")[1])\n",
    "            apnea_count = int((file_list[i].split(\"_\")[2]))\n",
    "            hypopnea_count = int((file_list[i].split(\"_\")[3]).split(\".\")[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Filename mismatch. Skipping {file_list[i]} ({e})\", file=sys.stderr)\n",
    "            continue\n",
    "        filename = f\"{patient_id}_{study_id}\"\n",
    "        ahi_value = ahi_dict.get(filename, None)\n",
    "        if ahi_value is None:\n",
    "            print(f\"Sleep study {filename} is not found in AHI.csv.  Skipping {file_list[i]}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if ahi_value > THRESHOLD:\n",
    "                apnea_event_counts[patient_id] = apnea_event_counts.get(patient_id, 0) + apnea_count\n",
    "                hypopnea_event_counts[patient_id] = hypopnea_event_counts.get(patient_id, 0) + hypopnea_count\n",
    "                study_event_counts[patient_id] = study_event_counts.get(patient_id, 0) + apnea_count + hypopnea_count\n",
    "        except Exception as e:\n",
    "            print(f\"File structure problem.  Skipping {file_list[i]} ({e})\", file=sys.stderr)\n",
    "            continue\n",
    "\n",
    "    apnea_event_counts = sorted(apnea_event_counts.items(), key=lambda item: item[1])\n",
    "    hypopnea_event_counts = sorted(hypopnea_event_counts.items(), key=lambda item: item[1])\n",
    "    study_event_counts = sorted(study_event_counts.items(), key=lambda item: item[1])\n",
    "\n",
    "    ################################### Fold the data based on number of respiratory events #########################\n",
    "    folds = []\n",
    "    for i in range(5):\n",
    "        folds.append(study_event_counts[i::5])\n",
    "\n",
    "    # print('FOLDS:', folds)\n",
    "\n",
    "    x = []\n",
    "    y_apnea = []\n",
    "    y_hypopnea = []\n",
    "    counter = 0\n",
    "    for idx, fold in enumerate(folds):\n",
    "        first = True\n",
    "        aggregated_data = None\n",
    "        aggregated_label_apnea = None\n",
    "        aggregated_label_hypopnea = None\n",
    "        for patient in fold:\n",
    "            counter += 1\n",
    "            # print(counter)\n",
    "            glob_path = os.path.join(PATH, patient[0] + \"_*\")\n",
    "            # print(\"glob path\", glob_path)\n",
    "            for study in glob.glob(glob_path):\n",
    "                study_data = np.load(study)\n",
    "\n",
    "                signals = study_data['data']\n",
    "                labels_apnea = study_data['labels_apnea']\n",
    "                labels_hypopnea = study_data['labels_hypopnea']\n",
    "\n",
    "                identifier = study.split(os.path.sep)[-1].split('_')[0] + \"_\" + study.split(os.path.sep)[-1].split('_')[1]\n",
    "                # print(identifier)\n",
    "                # demo_arr = demo[demo['id'] == identifier].drop(columns=['id']).to_numpy().squeeze() # TODO\n",
    "\n",
    "                y_c = labels_apnea + labels_hypopnea\n",
    "                neg_samples = np.where(y_c == 0)[0]\n",
    "                pos_samples = list(np.where(y_c > 0)[0])\n",
    "                ratio = len(pos_samples) / len(neg_samples)\n",
    "                neg_survived = []\n",
    "                for s in range(len(neg_samples)):\n",
    "                    if random.random() < ratio:\n",
    "                        neg_survived.append(neg_samples[s])\n",
    "                samples = neg_survived + pos_samples\n",
    "                signals = signals[samples, :, :]\n",
    "                labels_apnea = labels_apnea[samples]\n",
    "                labels_hypopnea = labels_hypopnea[samples]\n",
    "\n",
    "                data = np.zeros((signals.shape[0], EPOCH_DURATION * FREQ, s_count + 3))\n",
    "                for i in range(signals.shape[0]):  # for each epoch\n",
    "                    # data[i, :len(demo_arr), -1] = demo_arr TODO\n",
    "                    data[i, :, -2], data[i, :, -3] = extract_rri(signals[i, ECG_SIG, :], FREQ, float(EPOCH_DURATION))\n",
    "                    for j in range(s_count):  # for each signal\n",
    "                        data[i, :, j] = resample(signals[i, SIGS[j], :], EPOCH_DURATION * FREQ)\n",
    "\n",
    "                if first:\n",
    "                    aggregated_data = data\n",
    "                    aggregated_label_apnea = labels_apnea\n",
    "                    aggregated_label_hypopnea = labels_hypopnea\n",
    "                    first = False\n",
    "                else:\n",
    "                    aggregated_data = np.concatenate((aggregated_data, data), axis=0)\n",
    "                    aggregated_label_apnea = np.concatenate((aggregated_label_apnea, labels_apnea), axis=0)\n",
    "                    aggregated_label_hypopnea = np.concatenate((aggregated_label_hypopnea, labels_hypopnea), axis=0)\n",
    "\n",
    "        if aggregated_data is not None:\n",
    "            x.append(aggregated_data.tolist())\n",
    "        if aggregated_label_apnea is not None:\n",
    "            y_apnea.append(aggregated_label_apnea.tolist())\n",
    "        if aggregated_label_hypopnea is not None:\n",
    "            y_hypopnea.append(aggregated_label_hypopnea.tolist())\n",
    "\n",
    "    return x, y_apnea, y_hypopnea\n",
    "\n",
    "def list_lengths(lst):\n",
    "    \"\"\"\n",
    "    Gets all the individual lengths of a list\n",
    "    :param lst:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(lst, list):\n",
    "        # For each item in the list, recursively process it if it's a list\n",
    "        # Otherwise, the item itself is not counted and is represented as None for non-list items\n",
    "        sublengths = [list_lengths(item) for item in lst]\n",
    "        if len([*filter(lambda v: v is not None, sublengths)]) == 0:\n",
    "            return len(lst)\n",
    "        # Instead of returning None for non-list items, you could choose to omit them or handle differently\n",
    "        return len(lst), sublengths  # Return the length of the current list and the structure\n",
    "    # Return None or some indication for non-list items, if needed\n",
    "    return None\n",
    "\n",
    "def load_data():\n",
    "    x, y_apnea, y_hypopnea = load_data(PATH)\n",
    "\n",
    "    # these output the maximum size for dimension.\n",
    "    # If we're going to make this a consistent size without truncating,\n",
    "    # this is the size to make it\n",
    "    print(f\"Padded X.shape:{max_dimensions(x)}\")\n",
    "    x_norm = pad_lists(x, 0)\n",
    "\n",
    "    print(f\"Padded Y_a shape: {max_dimensions(y_apnea)}\")\n",
    "    y_apnea_norm = pad_lists(y_apnea, 0)\n",
    "\n",
    "    print(f\"Padded Y_h.shape:{max_dimensions(y_hypopnea)}\")\n",
    "    y_hypopnea_norm = pad_lists(y_hypopnea, 0)\n",
    "\n",
    "    print(f\"Saving to {OUT_PATH}\")\n",
    "    np.savez_compressed(\n",
    "        OUT_PATH,\n",
    "        x=x_norm,\n",
    "        y_apnea=y_apnea_norm,\n",
    "        y_hypopnea=y_hypopnea_norm,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "#   Model\n",
    "As discussed above, the model this paper proposes is based on the [transformer architecture](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)). While this architecture has led to excellent results in NLP and other applications, it has, to our knowledge, not been studied in the context of sleep apnea.\n",
    "\n",
    "Like many other transformer-based architectures, this model has several other components, illustrated below.\n",
    "\n",
    "![model architecture](./images/model_arch.png)\n",
    "\n",
    "As seen in this architecture, inputs, which are primarily pre-processed signals data, are first fed in parallel through a variety of 1-D convolutional layers, partially concatenated to fewer parallel \"tracks\", and then those results are passed through an activation layer. Next, all tracks are concatenated prior to being passed into the transformer, which includes at least one multi-headed attention mechanism and one dense layer. Finally, the output of the transformer is passed through a normalization layer, pooling layer, fully-connected layer (labeled \"Multi-layer perceptron\" in the illustration) and a final activation function.\n",
    "\n",
    "As mentioned in previous sections, we provide a pre-trained model because we are unable to share raw, preprocessed, or loaded data in any form due to licensing issues. Model code is shown in subsequent sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer layers\n",
    "\n",
    "Since model code is extensive, we split it up into two sections. We show the transformer model code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from keras import Model\n",
    "from keras.activations import sigmoid, relu\n",
    "from keras.layers import Dense, Dropout, Reshape, LayerNormalization, MultiHeadAttention, Add, Flatten, Input, Layer, \\\n",
    "    GlobalAveragePooling1D, AveragePooling1D, Concatenate, SeparableConvolution1D, Conv1D\n",
    "from keras.regularizers import L2\n",
    "\n",
    "\n",
    "\n",
    "class Patches(Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, input):\n",
    "        input = input[:, tf.newaxis, :, :]\n",
    "        batch_size = tf.shape(input)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=input,\n",
    "            sizes=[1, 1, self.patch_size, 1],\n",
    "            strides=[1, 1, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches,\n",
    "                             [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "\n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches, projection_dim, l2_weight):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.l2_weight = l2_weight\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(units=projection_dim, kernel_regularizer=L2(l2_weight),\n",
    "                                bias_regularizer=L2(l2_weight))\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) # + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate, l2_weight):\n",
    "    for _, units in enumerate(hidden_units):\n",
    "        x = Dense(units, activation=None, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight))(x)\n",
    "        x = tf.nn.gelu(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_transformer_model(input_shape, num_patches,\n",
    "                             projection_dim, transformer_layers,\n",
    "                             num_heads, transformer_units, mlp_head_units,\n",
    "                             num_classes, drop_out, reg, l2_weight, demographic=False):\n",
    "    if reg:\n",
    "        activation = None\n",
    "    else:\n",
    "        activation = 'sigmoid'\n",
    "    inputs = Input(shape=input_shape)\n",
    "    patch_size = input_shape[0] / num_patches\n",
    "    if demographic:\n",
    "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                                             beta_initializer=\"glorot_uniform\",\n",
    "                                                             gamma_initializer=\"glorot_uniform\")(inputs[:,:,:-1])\n",
    "        demo = inputs[:, :12, -1]\n",
    "\n",
    "    else:\n",
    "        normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                                             beta_initializer=\"glorot_uniform\",\n",
    "                                                             gamma_initializer=\"glorot_uniform\")(inputs)\n",
    "\n",
    "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
    "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
    "    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n",
    "            bias_regularizer=L2(l2_weight))(x1, x1)\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    #x = Concatenate()([x, demo])\n",
    "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
    "\n",
    "    logits = Dense(num_classes, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n",
    "                   activation=activation)(features)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "\n",
    "\n",
    "def create_hybrid_transformer_model(input_shape):\n",
    "    transformer_units =  [32,32]\n",
    "    transformer_layers = 2\n",
    "    num_heads = 4\n",
    "    l2_weight = 0.001\n",
    "    drop_out= 0.25\n",
    "    mlp_head_units = [256, 128]\n",
    "    num_patches=30\n",
    "    projection_dim=  32\n",
    "\n",
    "    # Conv1D(32...\n",
    "    input1 = Input(shape=input_shape)\n",
    "    conv11 = Conv1D(16, 256)(input1) #13\n",
    "    conv12 = Conv1D(16, 256)(input1) #13\n",
    "    conv13 = Conv1D(16, 256)(input1) #13\n",
    "\n",
    "    pwconv1 = SeparableConvolution1D(32, 1)(input1)\n",
    "    pwconv2 = SeparableConvolution1D(32, 1)(pwconv1)\n",
    "\n",
    "    conv21 = Conv1D(16, 256)(conv11) # 7\n",
    "    conv22 = Conv1D(16, 256)(conv12) # 7\n",
    "    conv23 = Conv1D(16, 256)(conv13) # 7\n",
    "\n",
    "    concat = keras.layers.concatenate([conv21, conv22, conv23], axis=-1)\n",
    "    concat = Dense(64, activation=relu)(concat) #192\n",
    "    concat = Dense(64, activation=sigmoid)(concat) #192\n",
    "    concat = SeparableConvolution1D(32,1)(concat)\n",
    "    concat = keras.layers.concatenate([concat, pwconv2], axis=1)\n",
    "\n",
    "    ####################################################################################################################\n",
    "    patch_size = input_shape[0] / num_patches\n",
    "\n",
    "    normalized_inputs = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                                             beta_initializer=\"glorot_uniform\",\n",
    "                                                             gamma_initializer=\"glorot_uniform\")(concat)\n",
    "\n",
    "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
    "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
    "    encoded_patches = PatchEncoder(num_patches=num_patches, projection_dim=projection_dim, l2_weight=l2_weight)(patches)\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = encoded_patches # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=drop_out, kernel_regularizer=L2(l2_weight),  # i *\n",
    "            bias_regularizer=L2(l2_weight))(x1, x1)\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(x3, transformer_units, drop_out, l2_weight)  # i *\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    #x = Concatenate()([x, demo])\n",
    "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
    "\n",
    "    logits = Dense(1, kernel_regularizer=L2(l2_weight), bias_regularizer=L2(l2_weight),\n",
    "                   activation='sigmoid')(features)\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    model = Model(inputs=input1, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting layers\n",
    "\n",
    "There are several layers prior to, and after the transformer. As discussed above, these include several 1-D convolutions, activations, concatenations, normalizations, and fully-connected layers. Code for these layers is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Input, Model\n",
    "from keras.layers import Dense, Flatten, MaxPooling2D, Conv2D, BatchNormalization, LSTM, Bidirectional, Permute, \\\n",
    "    Reshape, GRU, Conv1D, MaxPooling1D, Activation, Dropout, GlobalAveragePooling1D, multiply, MultiHeadAttention, Add, \\\n",
    "    LayerNormalization, SeparableConvolution1D\n",
    "from keras.models import Sequential\n",
    "from keras.activations import relu, sigmoid\n",
    "from keras.regularizers import l2\n",
    "import tensorflow_addons as tfa\n",
    "from .transformer import create_transformer_model, mlp, create_hybrid_transformer_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    for i in range(5): # 10\n",
    "        model.add(Conv1D(45, 32, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(relu))\n",
    "        model.add(MaxPooling1D())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    for i in range(2): #4\n",
    "        model.add(Dense(512))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(relu))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnnlstm_model(input_a_shape, weight=1e-3):\n",
    "    cnn_filters = 32 # 128\n",
    "    cnn_kernel_size = 4 # 4\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "                                              beta_initializer=\"glorot_uniform\",\n",
    "                                              gamma_initializer=\"glorot_uniform\")(input1)\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(input1)\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(cnn_filters, cnn_kernel_size, activation='relu')(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = LSTM(32, return_sequences=True)(x1) #256\n",
    "    x1 = LSTM(32, return_sequences=True)(x1) #256\n",
    "    x1 = LSTM(32)(x1) #256\n",
    "    x1 = Flatten()(x1)\n",
    "\n",
    "    x1 = Dense(32, activation='relu')(x1) #64\n",
    "    x1 = Dense(32, activation='relu')(x1) #64\n",
    "    outputs = Dense(1, activation='sigmoid')(x1)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_semscnn_model(input_a_shape):\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    # input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "    #                                           beta_initializer=\"glorot_uniform\",\n",
    "    #                                           gamma_initializer=\"glorot_uniform\")(input1)\n",
    "    x1 = Conv1D(45, 32, strides=1)(input1) #kernel_size=11\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1) #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    squeeze = Flatten()(x1)\n",
    "    excitation = Dense(128, activation='relu')(squeeze)\n",
    "    excitation = Dense(64, activation='relu')(excitation)\n",
    "    logits = Dense(1, activation='sigmoid')(excitation)\n",
    "    model = Model(inputs=input1, outputs=logits)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "\n",
    "    \"cnn\": create_cnn_model((60 * 32, 3)),\n",
    "    \"sem-mscnn\": create_semscnn_model((60 * 32, 3)),\n",
    "    \"cnn-lstm\": create_cnnlstm_model((60 * 32, 3)),\n",
    "    \"hybrid\": create_hybrid_transformer_model((60 * 32, 3)),\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(config):\n",
    "    if config[\"model_name\"].split('_')[0] == \"Transformer\":\n",
    "        return create_transformer_model(input_shape=(60 * 32, len(config[\"channels\"])),\n",
    "                                        num_patches=config[\"num_patches\"], projection_dim=config[\"transformer_units\"],\n",
    "                                        transformer_layers=config[\"transformer_layers\"], num_heads=config[\"num_heads\"],\n",
    "                                        transformer_units=[config[\"transformer_units\"] * 2,\n",
    "                                                           config[\"transformer_units\"]],\n",
    "                                        mlp_head_units=[256, 128], num_classes=1, drop_out=config[\"drop_out_rate\"],\n",
    "                                        reg=config[\"regression\"], l2_weight=config[\"regularization_weight\"])\n",
    "    else:\n",
    "        return model_dict.get(config[\"model_name\"].split('_')[0])\n",
    "\n",
    "def run_model():\n",
    "    config = {\n",
    "        \"model_name\": \"hybrid\",\n",
    "        \"regression\": False,\n",
    "\n",
    "        \"transformer_layers\": 4,  # best 5\n",
    "        \"drop_out_rate\": 0.25,\n",
    "        \"num_patches\": 20,  # best\n",
    "        \"transformer_units\": 32,  # best 32\n",
    "        \"regularization_weight\": 0.001,  # best 0.001\n",
    "        \"num_heads\": 4,\n",
    "        \"epochs\": 100,  # best\n",
    "        \"channels\": [14, 18, 19, 20],\n",
    "    }\n",
    "    model = get_model(config)\n",
    "    model.build(input_shape=(1, 60 * 32, 10))\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "From data preprocessing and model architecture, we can train the model. As discussed previously, we do not provide raw data for licensing reasons. Instead, we provide preprocessed data on a limited basis, and also provide pre-trained model weights. To minimize resource requirements and runtime when running this notebook in this section, we will show training on a limited subset of the preprocessed data and limited number of epochs. We will also take steps to minimize resource requirements and runtime in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import keras.metrics\n",
    "import numpy as np\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from sklearn.utils import shuffle\n",
    "import numpy.typing as npt\n",
    "\n",
    "from models.models import get_model\n",
    "from channels import transform_for_channels\n",
    "from folds import concat_all_folds\n",
    "\n",
    "THRESHOLD = 1\n",
    "FOLD = 5\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 50 and (epoch - 1) % 5 == 0:\n",
    "        lr *= 0.5\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train(\n",
    "        config,\n",
    "        fold: int | None = None,\n",
    "        force_retrain: bool = False\n",
    "):\n",
    "    print(f'training with config {config}, fold={fold}')\n",
    "\n",
    "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
    "\n",
    "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
    "    print(\n",
    "        f'x={x.shape}, y_apnea={y_apnea.shape}, y_hypopnea={y_hypopnea.shape}'\n",
    "    )\n",
    "    y = y_apnea + y_hypopnea\n",
    "    ########################################################################################\n",
    "    # Channel selection\n",
    "    \n",
    "    chans = config[\"channels\"]\n",
    "    x_transform = transform_for_channels(x=x, channels=chans)\n",
    "    print(f'Extracting channels {chans}')\n",
    "    max_fold = min(FOLD, x_transform.shape[0])\n",
    "    if max_fold < x_transform.shape[0]:\n",
    "        print(\n",
    "            f'WARNING: only looking at the first {max_fold} of '\n",
    "            f'{x_transform.shape[0]} total folds in X'\n",
    "        )\n",
    "    # for i in range(FOLD):\n",
    "    for i in range(max_fold):\n",
    "        x_transform[i], y[i] = shuffle(x_transform[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        if config[\"regression\"]:\n",
    "            y[i] = np.sqrt(y[i])\n",
    "            y[i][y[i] != 0] += 2\n",
    "        else:\n",
    "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "        \n",
    "        replace = x[i][:, :, chans]\n",
    "        \n",
    "        x_transform[i] = replace  # CHANNEL SELECTION\n",
    "\n",
    "    ########################################################################################\n",
    "    #\n",
    "    # The original code for this is taken from the following link:\n",
    "    # \n",
    "    # https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/6dc5ec87ef17810c461d4738dd4f46240816999c/train.py#L39-L48\n",
    "    # \n",
    "    # I (Aaron) think that in the inner loop, they're just trying to create\n",
    "    # one big NDArray with the concatenation of all the folds except for the \n",
    "    # one on which they're currently on in the outer loop.\n",
    "    # \n",
    "    # Then, they train on the concatenated array. In other words, the outer\n",
    "    # loop behaves similarly to epochs, with a small twist.\n",
    "    # \n",
    "    # They used to have the logic to do this inside the outer loop,\n",
    "    # but I pulled it out.\n",
    "    # \n",
    "    # also note, the folds selection (commented below) didn't work because \n",
    "    # they pass fold=0 into this function, which results in no training \n",
    "    # whatsoever.\n",
    "    folds = range(max_fold)\n",
    "    # folds = range(FOLD) if fold is None else range(fold)\n",
    "    print(f'iterating over {folds} fold(s)')\n",
    "    for fold in folds:\n",
    "        base_model_path = config[\"model_path\"]\n",
    "        model_path = f\"{base_model_path}/{str(fold)}\"\n",
    "        if (\n",
    "            os.path.exists(model_path) and \n",
    "            not force_retrain\n",
    "        ):\n",
    "            print(\n",
    "                f'Training fold {fold}: force_retrain==False and '\n",
    "                f'{model_path} already exists, skipping.'\n",
    "            )\n",
    "            continue\n",
    "        x_train = concat_all_folds(orig=x_transform, except_fold=fold)\n",
    "        y_train = concat_all_folds(orig=y, except_fold=fold)\n",
    "\n",
    "        model = get_model(config)\n",
    "        if config[\"regression\"]:\n",
    "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
    "            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        else:\n",
    "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy(),\n",
    "                          metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "            early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        model.fit(x=x_train, y=y_train, batch_size=512, epochs=config[\"epochs\"], validation_split=0.1,\n",
    "                  callbacks=[early_stopper, lr_scheduler])\n",
    "        ################################################################################################################\n",
    "        print(f\"saving model for fold {fold} to {model_path}\")\n",
    "        model.save(model_path)\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "\n",
    "def train_age_seperated(config):\n",
    "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
    "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
    "    y = y_apnea + y_hypopnea\n",
    "    ########################################################################################\n",
    "    for i in range(10):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        if config[\"regression\"]:\n",
    "            y[i] = np.sqrt(y[i])\n",
    "            y[i][y[i] != 0] += 2\n",
    "        else:\n",
    "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]  # CHANNEL SELECTION\n",
    "\n",
    "    ########################################################################################\n",
    "    first = True\n",
    "    for i in range(10):\n",
    "        if first:\n",
    "            x_train = x[i]\n",
    "            y_train = y[i]\n",
    "            first = False\n",
    "        else:\n",
    "            x_train = np.concatenate((x_train, x[i]))\n",
    "            y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "    model = get_model(config)\n",
    "    if config[\"regression\"]:\n",
    "        model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
    "        early_stopper = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model.compile(optimizer=\"adam\", loss=BinaryCrossentropy(),\n",
    "                      metrics=[keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "        early_stopper = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=512,\n",
    "        epochs=config[\"epochs\"],\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopper, lr_scheduler]\n",
    "    )\n",
    "    ################################################################################################################\n",
    "    model.save(config[\"model_path\"] + str(0))\n",
    "    keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results and evaluations\n",
    "\n",
    "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
    "\n",
    "Please test and report results for all experiments that you run with:\n",
    "\n",
    "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
    "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n",
    "\n",
    "See the following files for testing and evaluation:\n",
    "\n",
    "- https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/test.py\n",
    "- https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/metrics.py\n",
    "\n",
    "---\n",
    "\n",
    "In a normal machine learning pipeline, we would train a model and then immediately evaluate it on a test data set. As with the last section, however, we take steps to minimize resource requirements and runtime. To that end, we have run training on the full dataset and saved the resulting model's weights (which you can find in the [`original/weights`](https://github.com/arschles/UIUC-CS598-DLH-Project/blob/main/weights/README.md) directory). In this section, we will instantiate our model from those weights, then evaluate it. The results of this evaluation determine how well we were able to reproduce the results claimed in the original paper.\n",
    "\n",
    "The code shown in this section tests our preloaded model. As with the previous section, testing and evaluation code is extensive, so we split it into two separate sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics reporting and calculation\n",
    "\n",
    "Below, we have utility code for metrics collection and reporting. This code will be used by subsequent evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score, average_precision_score, roc_auc_score\n",
    "\n",
    "\n",
    "class FromLogitsMixin:\n",
    "    def __init__(self, from_logits=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if self.from_logits:\n",
    "            y_pred = tf.nn.sigmoid(y_pred)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class AUC(FromLogitsMixin, tf.metrics.AUC):\n",
    "    ...\n",
    "\n",
    "\n",
    "class BinaryAccuracy(FromLogitsMixin, tf.metrics.BinaryAccuracy):\n",
    "    ...\n",
    "\n",
    "\n",
    "class TruePositives(FromLogitsMixin, tf.metrics.TruePositives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class FalsePositives(FromLogitsMixin, tf.metrics.FalsePositives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class TrueNegatives(FromLogitsMixin, tf.metrics.TrueNegatives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class FalseNegatives(FromLogitsMixin, tf.metrics.FalseNegatives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Precision(FromLogitsMixin, tf.metrics.Precision):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Recall(FromLogitsMixin, tf.metrics.Recall):\n",
    "    ...\n",
    "\n",
    "\n",
    "class F1Score(FromLogitsMixin, tfa.metrics.F1Score):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Result:\n",
    "    def __init__(self):\n",
    "        self.accuracy_list = []\n",
    "        self.sensitivity_list = []\n",
    "        self.specificity_list = []\n",
    "        self.f1_list = []\n",
    "        self.auroc_list = []\n",
    "        self.auprc_list = []\n",
    "        self.precision_list = []\n",
    "\n",
    "    def add(self, y_test, y_predict, y_score):\n",
    "        C = confusion_matrix(y_test, y_predict, labels=(1, 0))\n",
    "        TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "\n",
    "        acc, sn, sp, pr = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP), 1. * TP / (\n",
    "                TP + FP)\n",
    "        acc = 1. * (TP + TN) / (TP + TN + FP + FN)\n",
    "        sn = 1. * TP / (TP + FN)\n",
    "        sp = 1. * TN / (TN + FP)\n",
    "        pr = 1. * TP / (TP + FP) if TP + FP != 0 else 0 # define precision to be zeero if there are NO positive predictions\n",
    "        f1 = f1_score(y_test, y_predict)\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "        auprc = average_precision_score(y_test, y_score)\n",
    "\n",
    "        self.accuracy_list.append(acc * 100)\n",
    "        self.precision_list.append(pr * 100)\n",
    "        self.sensitivity_list.append(sn * 100)\n",
    "        self.specificity_list.append(sp * 100)\n",
    "        self.f1_list.append(f1 * 100)\n",
    "        self.auroc_list.append(auc * 100)\n",
    "        self.auprc_list.append(auprc * 100)\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        out_str = \"=========================================================================== \\n\"\n",
    "        out_str += str(self.accuracy_list) + \" \\n\"\n",
    "        out_str += str(self.precision_list) + \" \\n\"\n",
    "        out_str += str(self.sensitivity_list) + \" \\n\"\n",
    "        out_str += str(self.specificity_list) + \" \\n\"\n",
    "        out_str += str(self.f1_list) + \" \\n\"\n",
    "        out_str += str(self.auroc_list) + \" \\n\"\n",
    "        out_str += str(self.auprc_list) + \" \\n\"\n",
    "        out_str += str(\"Accuracy: %.2f -+ %.3f\" % (np.mean(self.accuracy_list), np.std(self.accuracy_list))) + \" \\n\"\n",
    "        out_str += str(\"Precision: %.2f -+ %.3f\" % (np.mean(self.precision_list), np.std(self.precision_list))) + \" \\n\"\n",
    "        out_str += str(\n",
    "            \"Recall: %.2f -+ %.3f\" % (np.mean(self.sensitivity_list), np.std(self.sensitivity_list))) + \" \\n\"\n",
    "        out_str += str(\n",
    "            \"Specifity: %.2f -+ %.3f\" % (np.mean(self.specificity_list), np.std(self.specificity_list))) + \" \\n\"\n",
    "        out_str += str(\"F1: %.2f -+ %.3f\" % (np.mean(self.f1_list), np.std(self.f1_list))) + \" \\n\"\n",
    "        out_str += str(\"AUROC: %.2f -+ %.3f\" % (np.mean(self.auroc_list), np.std(self.auroc_list))) + \" \\n\"\n",
    "        out_str += str(\"AUPRC: %.2f -+ %.3f\" % (np.mean(self.auprc_list), np.std(self.auprc_list))) + \" \\n\"\n",
    "\n",
    "        out_str += str(\"$ %.1f \\pm %.1f$\" % (np.mean(self.accuracy_list), np.std(self.accuracy_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.precision_list), np.std(self.precision_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.sensitivity_list), np.std(self.sensitivity_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.f1_list), np.std(self.f1_list))) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (np.mean(self.auroc_list), np.std(self.auroc_list))) + \"& \"\n",
    "\n",
    "        return out_str\n",
    "\n",
    "    def print(self):\n",
    "        print(self.get())\n",
    "\n",
    "    def save(self, path, config):\n",
    "        file = open(path, \"w+\")\n",
    "        file.write(str(config))\n",
    "        file.write(\"\\n\")\n",
    "        file.write(self.get())\n",
    "        file.flush()\n",
    "        file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics calculation\n",
    "\n",
    "The below code evaluates the model and reports it using the above reporting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from metrics import Result\n",
    "from data.noise_util import add_noise_to_data\n",
    "\n",
    "from channels import transform_for_channels\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "THRESHOLD = 1\n",
    "FOLD = 5\n",
    "\n",
    "\n",
    "def test(config: dict[str, str], fold=None):\n",
    "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
    "    ############################################################################\n",
    "    x, y_apnea, y_hypopnea = data[\"x\"], data[\"y_apnea\"], data[\"y_hypopnea\"]\n",
    "    x_transform = transform_for_channels(x=x, channels=config[\"channels\"])\n",
    "    y = y_apnea + y_hypopnea\n",
    "\n",
    "    max_fold = min(FOLD, x_transform.shape[0])\n",
    "    if max_fold < x_transform.shape[0]:\n",
    "        print(\n",
    "            f'WARNING: only looking at the first {max_fold} of '\n",
    "            f'{x_transform.shape[0]} total folds in X'\n",
    "        )\n",
    "\n",
    "    # for i in range(FOLD):\n",
    "    for i in range(max_fold):\n",
    "        x_transform[i], y[i] = shuffle(x_transform[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "        x_transform[i] = x[i][:, :, config[\"channels\"]]\n",
    "    ############################################################################\n",
    "    result = Result()\n",
    "    # folds = range(FOLD) if fold is None else [fold]\n",
    "    folds = range(max_fold)\n",
    "    for fold in folds:\n",
    "        base_model_path = config[\"model_path\"]\n",
    "        model_path = f\"{base_model_path}/{str(fold)}\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(\n",
    "                f\"WARNING: model path {model_path} does not exist, skipping!\"\n",
    "            )\n",
    "            continue\n",
    "        x_test = x_transform[fold]\n",
    "        # NOTE: this config key is not set in both `main_chat.py` and\n",
    "        # `main_nch.py`. if it were, the code under this `if` would fail\n",
    "        # because there is no `add_noise_to_data` function in this repository.\n",
    "        if config.get(\"test_noise_snr\"):\n",
    "           x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
    "\n",
    "        y_test = y[\n",
    "            fold\n",
    "        ]  # For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\n",
    "        model = tf.keras.models.load_model(model_path, compile=False)\n",
    "\n",
    "        predict = model.predict(x_test)\n",
    "        y_score = predict\n",
    "        y_predict = np.where(\n",
    "            predict > 0.5, 1, 0\n",
    "        )  # For MultiClass np.argmax(y_score, axis=-1)\n",
    "\n",
    "        result.add(y_test, y_predict, y_score)\n",
    "\n",
    "    print(\n",
    "        '\\n----------\\n'\n",
    "        'results:\\n'\n",
    "    )\n",
    "    result.print()\n",
    "    model_name = config[\"model_name\"]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\") # Format the date and time as a string \"YYYYMMDD-HH:mm\"\n",
    "    results_file = os.path.join('results', f\"{model_name}-{timestamp}.txt\")\n",
    "    print(\n",
    "        f'done, saving to {results_file}\\n'\n",
    "        '----------\\n'\n",
    "    )\n",
    "\n",
    "    result.save(path=results_file, config=config)\n",
    "\n",
    "    del data, x_test, y_test, model, predict, y_score, y_predict\n",
    "\n",
    "\n",
    "def test_age_seperated(config):\n",
    "    x = []\n",
    "    y_apnea = []\n",
    "    y_hypopnea = []\n",
    "    for i in range(10):\n",
    "        data = np.load(config[\"data_path\"] + str(i) + \".npz\", allow_pickle=True)\n",
    "        x.append(data[\"x\"])\n",
    "        y_apnea.append(data[\"y_apnea\"])\n",
    "        y_hypopnea.append(data[\"y_hypopnea\"])\n",
    "    ############################################################################\n",
    "    y = np.array(y_apnea) + np.array(y_hypopnea)\n",
    "    for i in range(10):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]\n",
    "    ############################################################################\n",
    "    result = Result()\n",
    "\n",
    "    for fold in range(10):\n",
    "        x_test = x[fold]\n",
    "        if config.get(\"test_noise_snr\"):\n",
    "            x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
    "\n",
    "        y_test = y[\n",
    "            fold\n",
    "        ]  # For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\n",
    "\n",
    "        model = tf.keras.models.load_model(config[\"model_path\"] + str(0), compile=False)\n",
    "\n",
    "        predict = model.predict(x_test)\n",
    "        y_score = predict\n",
    "        y_predict = np.where(\n",
    "            predict > 0.5, 1, 0\n",
    "        )  # For MultiClass np.argmax(y_score, axis=-1)\n",
    "\n",
    "        result.add(y_test, y_predict, y_score)\n",
    "\n",
    "    result.print()\n",
    "    result.save(\"./results/\" + config[\"model_name\"] + \".txt\", config)\n",
    "\n",
    "    del data, x_test, y_test, model, predict, y_score, y_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "outputs": [],
   "source": [
    "# compare you model with others\n",
    "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "Since we were unable to acquire all the data used in the paper, it's difficult to determine whether the paper is reproducible. The results we have with our current dataset are positive, however, so we believe the paper is reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with paper reproduction\n",
    "\n",
    "Given we encountered many significant issues during our paper reproduction efforts to date, we believe this paper is very difficult to reproduce. These issues have been discussed previously, but are summarized as follows:\n",
    "\n",
    "- Data that are either impossible or very difficult, practically speaking, to acquire in a reasonable amount of time\n",
    "- Open source code that is either very low quality or simply does not work\n",
    "- Open source code that does not match exactly the architecture discussed in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for improvement\n",
    "\n",
    "We estimate that data acquisition problems are outside the authors' control, so we focus our suggestions for improvement on code quality. Below are the highest-impact tasks we would suggest to the authors:\n",
    "\n",
    "- Add clear documentation to each function, including descriptions of parameters and return values\n",
    "- Add [Python type hints](https://docs.python.org/3/library/typing.html) to at least function parameters and return values\n",
    "- Add complete information about the expected runtime environment, including required Python version(s) and dependency versions (including transitive dependencies)\n",
    "- Do an audit to ensure that model architecture and evaluations match those described in the paper\n",
    "- Add at least minimal testing to ensure all code runs successfully. Ideas for tests include:\n",
    "    - Construct the model, load it from pretrained weights, and do several inferences, just to make sure it runs and can properly run inferences\n",
    "    - Train the model for a small number of epochs to make sure loss is established and begins decreasing\n",
    "    - Preprocess and load one sleep study to ensure preprocessing code runs properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plans for improvement\n",
    "\n",
    "Between the draft due date and the final project due date, we plan to improve this notebook along several axes:\n",
    "\n",
    "- Include more evaluations and visual aids to illustrate them (i.e. charts, graphs, etc...)\n",
    "- Run at least one of the ablation studies from the paper and determine whether we reach the same conclusion as in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1. Fayyaz H, Strang A, Beheshti R. Bringing At-home Pediatric Sleep Apnea Testing Closer to Reality: A Multi-modal Transformer Approach. Proc Mach Learn Res. 2023;219:167-185.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5",
     "timestamp": 1709153069464
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
