{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01aH0PR4Sg-"
   },
   "source": [
    "# Before you use this template\n",
    "\n",
    "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
    "\n",
    "---\n",
    "\n",
    "# FAQ and Attentions\n",
    "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
    "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
    "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
    "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
    "must be within 8 min, otherwise, you may get penalty on the grade.\n",
    "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
    "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
    "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
    "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
    "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
    "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# License\n",
    "The data used by this dataset is subject to a license.  While we are not sharing the original data in its raw form, to facilitate the goals of the class and to honor the spirit of the [license](https://physionet.org/content/nch-sleep/view-license/3.1.0/), we require that by opening and running this notebook you agree that you:\n",
    "1. Will not attempt to reverse engineer the data into a form other than provided,\n",
    "2. Will not attempt to identify or de-anonymize any individual or institution in the dataset,\n",
    "3. Will not share or re-use the data in any form,\n",
    "4. Will not use the data for any purpose other than this assignment, and\n",
    "5. Maintain a up-to-date cerification in human research subject protection and HIPAA regulations.\n",
    "\n",
    "The data license is discussed in more detail [below](#data-licensing-note).\n",
    "\n",
    "# Preface\n",
    "Much effort went into making the [original repository](https://github.com/healthylaife/Pediatric-Apnea-Detection) runnable.  The repository was lacking any documentation including even python version and package versions.  It was built exclusively to be run on Windows and lacked any attempt at cross-platform support.  We ran into several cases where the code, as supplied, could never have been run in the provided state.  In one instance, for example, the arguments to a function were illegal and, after reaching out to the author and receiving no reply, we used our best judgment at a solution.\n",
    "\n",
    "With that in mind, we are attempting to reproduce results consistent with the original paper, but there may be differences due to these changes or other instances of errors or omissions that did not cause a code failure and that may have been too subtle to be caught in our initial passes over the code.  These differences will, inevitably, cause deviations between our results and the results presented in the paper.\n",
    "\n",
    "We have, however, undetaken in [our fork of the original code](https://github.com/arschles/UIUC-CS598-DLH-Project/tree/main/original) to provide more information to aid reproducibility (including requirements.txt), made the code cross-platform where it was not, and provide for information about how to preprocess and run the code using standard tools like bash and make.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlv6knX04FiY"
   },
   "source": [
    "# Mount Notebook to Google Drive\n",
    "\n",
    "**TODO: remove this section??**\n",
    "\n",
    "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
    "\n",
    "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
    "\n",
    "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
    "\n",
    "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sfk8Zrul_E8V"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "> ***This is in temporarily.  We should remove it before submission***\n",
    "\n",
    "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
    "\n",
    "*   ~~Background of the problem~~\n",
    "  * ~~what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc~~\n",
    "  * ~~what is the importance/meaning of solving the problem~~\n",
    "  * what is the difficulty of the problem\n",
    "  * ~~the state of the art methods and effectiveness.~~\n",
    "*   Paper explanation\n",
    "  * ~~what did the paper propose~~\n",
    "  * ~~what is the innovations of the method~~\n",
    "  * ~~how well the proposed method work (in its own metrics)~~\n",
    "  * ~~what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).~~\n",
    "\n",
    "---\n",
    "## Background\n",
    "Sleep apnea in children is a major health problem that affects between one to five percent of US children, but differs from sleep apnea in adults in its clinical causes and characteristics.  Thus, previously-created methods for detecting adult sleep apnea may be ineffective at detecting pediatric sleep apnea.\n",
    "\n",
    "While there are numerous testing tools and algorithmic methods for detecting adult sleep apnea, the same tools and methods are unavailable for pediatric sleep apnea (PSA) due to these differences. Detecting pediatric sleep apnea more easily can lead to earlier clinical intervention in the child's care and ultimately ____(need to ask Julie about effects of PSA)___.  Polysomnography is the standard method for formal sleep apnea diagnosis, but is generally performed in a dedicated facility where a patient can be monitored overnight. Polysomnography involves collecting various continuous-time signals, including electroencephalogram (EEG), electrooculogram (EOG), electrocardiogram (ECG), pulse oximetry (SpO2), end-tidal carbon dioxide (ETCO2), respiratory inductance plethysmography (RIP), nasal airflow, and oral airflow.  While effective, Polysomnography is, however, complex, costly and requires a dedicated sleep lab.  \n",
    "\n",
    "### State of the Art\n",
    "Current methods target adults and, for reasons stated earlier, are ineffective at diagnosing PSA. Very little work has been done in the scope of pediatric sleep apnea.\n",
    "\n",
    "In general, full Polysomnography data is hard to find and thus, much research has focused on determining the Apnea-Hypopnea Index (AHI) from ECG and SpO2 signals.   \n",
    "\n",
    "While transformers are used commonly in general deep learning models, they are less prevalent in the detection of sleep apnea.  Two studies used transformers to determine sleep stages (one in adults, one in children), while another used a hybrid CNN/transformer model of obstructive sleep apnea (OSA) detection.  \n",
    "\n",
    "## Paper\n",
    "The paper proposes to study the gaps in Obstructive Sleep Apnea Hypopnea Syndrome (OSAHS) in children vis-a-vis adults. The paper then suggests a custom transformer-based method and data representation for PSA detection, and identifies the polysomography modalities that most closely correlate to OSAHS in children.\n",
    "\n",
    "The results presented in the paper portray state-of-the-art results. \n",
    "![Figure 2](./images/figure_2.png)\n",
    "\n",
    "If diagnosis of pediatric sleep apnea can be performed with low-cost, consumer hardware, then costs of pediatric sleep apena diagnosis decrease and chilren's health improves.  Additionally, this also enables access for underserved, including rural, populations with limited access to sleep labs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ABD4VhFZbehA"
   },
   "outputs": [],
   "source": [
    "# code comment is used as inline annotations for your coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "~~List hypotheses from the paper you will test and the corresponding experiments you will run.~~\n",
    "\n",
    "\n",
    "~~1.   Hypothesis 1: xxxxxxx~~\n",
    "~~2.   Hypothesis 2: xxxxxxx~~\n",
    "\n",
    "~~You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:~~\n",
    "\n",
    "~~![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)~~\n",
    "\n",
    "---\n",
    "\n",
    "We generally will test whether we can beat state-of-the-art results from Polysomnography (PSG) studies in pediatric sleep apnea detection using the transformer-based model proposed in the paper and discussed above. More specifically, we will focus on testing the following hypotheses:\n",
    "\n",
    "1. Whether the proposed model can achieve results from signals more easily collected than PSG. As in the paper, we will focus on ECG and $SpO_2$ signals, and\n",
    "2. whether the results support this method being effective as a dedicated method for in-lab sleep studies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM4WUjz64C3B"
   },
   "source": [
    "\n",
    "You can also use code to display images, see the code below.\n",
    "\n",
    "The images must be saved in Google Drive first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rRksCB1vbYwJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif you want to use an image outside this notebook for explanaition,\\nyou can upload it to your google drive and show it with OpenCV or matplotlib\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanaition,\n",
    "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
    "'''\n",
    "# mount this notebook to your google drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# define dirs to workspace and data\n",
    "# img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
    "\n",
    "# import cv2\n",
    "# img = cv2.imread(img_dir)\n",
    "# cv2.imshow(\"Title\", img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "~~This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the experiment you executed for testing the hypotheses.~~\n",
    "\n",
    "~~The methodology at least contains two subsections **data** and **model** in your experiment.~~\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we detail how we acquire, process and load the relevant data, and how we train and evaluate the model given those data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "#  Data\n",
    "\n",
    "~~Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).~~\n",
    "  * ~~Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.~~\n",
    "  * ~~Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.~~\n",
    "  * ~~Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.~~\n",
    "  * Illustration: printing results, plotting figures for illustration.\n",
    "  * CANNOT DO THIS: ~~You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset.~~\n",
    "---\n",
    "## Data Licensing Note\n",
    "\n",
    "**Due to the license required to access the data, we cannot provide the raw dataset with this notebook because doing so would be a violation of terms 3 and 4 of the [PhysioNetCredential Health Data License 1.5.0](https://physionet.org/content/nch-sleep/view-license/3.1.0/).**\n",
    "\n",
    "Because we cannot supply original raw data, we will discuss the processing of the data and the relevant processing code with example output, but the provided code **will not actually process data** within this notebook.  We will load preprocessed data later in this notebook that is not subject to the aforementioned license and, at that point, further processing will be performed in this notebook.\n",
    "\n",
    "## Source(s)\n",
    "The original paper used Nationwide Childrenâ€™s Hospital (NCH) Sleep Data Bank (Lee et al., 2022), and Childhood Adenotonsillectomy Trial (CHAT) dataset (Marcus et al., 2013; Redline et al., 2011).  Each of these datasets are collected from actual sleep studies, anonymized and made available for research.  At the time of this writing, we have been unable to gain approval to access the CHAT dataset so we are unable to use it.  We currently have access to the [NCH dataset through physionet.org](https://physionet.org/content/nch-sleep/3.1.0/).  While we have been downloading this data for over a week, download speeds are capped at around 600KB/s.  We have currently downloaded around 400GB of 2.1TB total.  \n",
    "\n",
    "## Dataset Statistics\n",
    "Since the paper provides extensive statistics on the data used in this study, we have not undertaken to perform our own calculations. The paper-provided measures are as follows:\n",
    "\n",
    "### Demographics of the Datasets\n",
    "|          |                         |     NCH      |    CHAT     |\n",
    "|---------:|:------------------------|:------------:|:-----------:|\n",
    "|          | Number of Patients      |       3673   | 453         |\n",
    "|          | Number of Sleep Studies |     3984     |     453     |\n",
    "|  **Sex** |                         |              |             |\n",
    "|          | Male                    |     2068     |     219     |\n",
    "|          | Female                  |     1604     |     234     |\n",
    "| **Race** |                         |              |             |\n",
    "|          | Asian                   |      93      |      8      |\n",
    "|          | Black                   |     738      |     252     |\n",
    "|          | White                   |     2433     |     161     |\n",
    "|          | Other                   |     409      |     32      |\n",
    "|          | **Age (years/mean)**    | \\[0-30\\]/8.8 | \\[5-9\\]/6.5 |\n",
    "\n",
    "\n",
    "### Data Statistics \n",
    "\n",
    "|                Event |   NCH   | CHAT  |\n",
    "|---------------------:|:-------:|:-----:|\n",
    "|  Oxygen Desaturation | 215280  | 65006 |\n",
    "|       Oximeter Event | 161641  | 9,864 |\n",
    "|          EEG arousal | 146052  |  --   |\n",
    "|   Respiratory Events |         |       |\n",
    "|             Hypopnea |  14522  | 15871 |\n",
    "| Obstructive Hypopnea |  42179  |  --   |\n",
    "|    Obstructive apnea |  15782  | 7075  |\n",
    "|        Central apnea |  6938   | 3656  |\n",
    "|          Mixed apnea |  2650   |  --   |\n",
    "|         Sleep Stages |         |       |\n",
    "|                 Wake | 665676  | 10282 |\n",
    "|                   N1 | 128410  | 13578 |\n",
    "|                   N2 | 1383765 | 19985 |\n",
    "|                   N3 | 875486  | 9981  |\n",
    "|                  REM | 611320  | 3283  |    \n",
    "     \n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "### Data Normalization: Interpolation, Resampling and Tokenization\n",
    "The data consists of 1) regular time-series, 2) irregular time-teries, and 3) tabular data.  Additionally, the time-series data may be provided in various frequencies.  To merge the data into a coherent dataset suitable for training and testing, the following steps must be performed:\n",
    "    \n",
    "1. Each irregular time series must be interpolated to convert it into a regular time series.\n",
    "1. All the time series data must be resampled into a uniform frequency, $f_{sampling}$, for all sleep studies and modalities.\n",
    "1. The tabular data is added to the time series as a constant signal (i.e. repeated tokens)\n",
    "1. The combined data is split into $i$ equal-length tokens of time $S$, where each modality consists of $S * f_{sampling}$ data points\n",
    "    \n",
    "This data can then be split and passed to the model for training and testing.\n",
    "\n",
    "### Splitting\n",
    "\n",
    "This paper utilizes a custom stratified $k$-fold cross validation to ensure 1) an equal number of patients are assigned to each fold, and to 2) normalize the number of positive samples in each fold.  Pseudocode of this method is:\n",
    "\n",
    "![Algorithm](./images/algorithm_1.png)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Apnea-Hypopnea index\n",
    "\n",
    "The [Apnea-Hypopnea index (AHI)](https://www.sleepfoundation.org/sleep-apnea/ahi) is an important quantification of the severity of sleep apnea. Its derivation for a single night of sleep is as follows:\n",
    "\n",
    "$$\n",
    "\\frac {N_{apneic} + N_{hypopneic}} {H}\n",
    "$$\n",
    "\n",
    "Where $N_{apneic}$ is the number of apneic events (instances where the person stops breathing), $N_{hypopneic}$ is the number of hypopneic events (instances where the airflow is blocked and the person's breathing becomes more shallow), and $H$ is the total number of hours of sleep for the night.\n",
    "\n",
    "Since it can be derived from an entire night of sleep, the AHI is a very useful measure to summarize, with relatively modest loss of information, a person's apnea/hypopnea activity in a given night of sleep.\n",
    "\n",
    "Below is the code to create a TSV (tab-separated file) file containing AHI measures for a given sleep study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "APNEA_EVENT_DICT = {\n",
    "    \"Obstructive Apnea\": 2,\n",
    "    \"Central Apnea\": 2,\n",
    "    \"Mixed Apnea\": 2,\n",
    "    \"apnea\": 2,\n",
    "    \"obstructive apnea\": 2,\n",
    "    \"central apnea\": 2,\n",
    "    \"apnea\": 2,\n",
    "    \"Apnea\": 2,\n",
    "}\n",
    "\n",
    "HYPOPNEA_EVENT_DICT = {\n",
    "    \"Obstructive Hypopnea\": 1,\n",
    "    \"Hypopnea\": 1,\n",
    "    \"hypopnea\": 1,\n",
    "    \"Mixed Hypopnea\": 1,\n",
    "    \"Central Hypopnea\": 1,\n",
    "}\n",
    "\n",
    "def _num_sleep_hours(\n",
    "    sleep_study_metadata: pd.DataFrame,\n",
    "    pat_id: int,\n",
    "    study_id: int,\n",
    ") -> int:\n",
    "    ssm = sleep_study_metadata\n",
    "    sleep_duration_df = ssm.loc[\n",
    "        (ssm[\"STUDY_PAT_ID\"] == pat_id) &\n",
    "        (ssm[\"SLEEP_STUDY_ID\"] == study_id)\n",
    "    ]\n",
    "    assert len(sleep_duration_df) == 1, (\n",
    "        f'expected just 1 study with patient {pat_id} and study '\n",
    "        f'{study_id}, but got {len(sleep_duration_df)} instead'\n",
    "    )\n",
    "    sleep_duration_datetime = datetime.strptime(\n",
    "        str(\n",
    "            sleep_duration_df[\n",
    "                \"SLEEP_STUDY_DURATION_DATETIME\"\n",
    "            ].iloc[0]\n",
    "        ).strip(),\n",
    "        \"%H:%M:%S\"\n",
    "    )\n",
    "    return sleep_duration_datetime.hour\n",
    "\n",
    "\n",
    "def _ahi_for_study(\n",
    "    sleep_study_metadata: pd.DataFrame,\n",
    "    sleep_study: pd.DataFrame,\n",
    "    pat_id: int,\n",
    "    study_id: int,\n",
    ") -> float:\n",
    "    '''\n",
    "    Calculate the apnea-hypopnea index (AHI) for a given sleep study.\n",
    "    All apnea and hypopnea events will be counted from the sleep_study\n",
    "    DataFrame, and then divided by the total sleep duration, which\n",
    "    will be gotten from the sleep_study_metadata DataFrame. The result will\n",
    "    be returned as a float\n",
    "\n",
    "    For more on AHI, see the following link:\n",
    "\n",
    "    https://www.sleepfoundation.org/sleep-apnea/ahi\n",
    "\n",
    "    :param sleep_study_metadata\n",
    "        the DataFrame that has at least the following columns in order\n",
    "        from left to right:\n",
    "        STUDY_PAT_ID,\n",
    "        SLEEP_STUDY_ID,\n",
    "        SLEEP_STUDY_START_DATETIME,\n",
    "        SLEEP_STUDY_DURATION_DATETIME\n",
    "    :param sleep_study\n",
    "        the data from the sleep study in which we're interested\n",
    "    :param pat_id\n",
    "        the ID of the patient on whom the given study was done\n",
    "    :param study_id\n",
    "        the ID of the study\n",
    "    \n",
    "    :return\n",
    "        the AHI for the given study, as a float value\n",
    "    '''\n",
    "\n",
    "    # example tsv file:\n",
    "    # onset duration description\n",
    "    # 29766.7421875\t11.0546875\tObstructive Hypopnea\n",
    "\n",
    "    df = sleep_study\n",
    "    hypopnea_keys = set(HYPOPNEA_EVENT_DICT.keys())\n",
    "    apnea_keys = set(APNEA_EVENT_DICT.keys())\n",
    "\n",
    "    hypopnea_events = df.loc[df[\"description\"].isin(hypopnea_keys)]\n",
    "    apnea_events = df.loc[df[\"description\"].isin(apnea_keys)]\n",
    "    total_num_events = len(hypopnea_events) + len(apnea_events)\n",
    "    sleep_hours = float(_num_sleep_hours(\n",
    "        sleep_study_metadata,\n",
    "        pat_id,\n",
    "        study_id,\n",
    "    ))\n",
    "    return float(total_num_events) / sleep_hours\n",
    "\n",
    "\n",
    "def _parse_ss_tsv_filename(filename: str) -> tuple[int, int]:\n",
    "    '''\n",
    "    given a sleep study filename like `10048_24622.tsv`, that represents\n",
    "    <patient_id>_<sleep_study_id>.tsv, return a 2-tuple containing\n",
    "    the patient ID in element 1 and sleep study ID in element 2\n",
    "    '''\n",
    "    if not filename.endswith(\".tsv\"):\n",
    "        raise FileNotFoundError(\n",
    "            f\"expected {filename} to end with .tsv but it didn't\"\n",
    "        )\n",
    "\n",
    "    underscore_spl = filename.split(\"/\")[-1][:-4].split(\"_\")\n",
    "    if len(underscore_spl) != 2:\n",
    "        raise FileNotFoundError(f'malformed filename {filename}')\n",
    "    [pat_id, study_id] = underscore_spl\n",
    "    return (int(pat_id), int(study_id))\n",
    "\n",
    "\n",
    "def _write_tsv(out_filename: str, data: list[tuple[str, str, float]]):\n",
    "    with open(out_filename, 'w', newline='') as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter=',')\n",
    "        # in ./preprocessing.py, we need to have at least 'Study'\n",
    "        # and 'AHI'. Since they chose PascalCase, I extended that usage\n",
    "        # to patient ID.\n",
    "        writer.writerow((\"PatID\", \"Study\", \"AHI\"))\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def calculate_ahi(\n",
    "    sleep_study_metadata_file: str,\n",
    "    sleep_study_root: str,\n",
    "    out_file: str,\n",
    ") -> None:\n",
    "    '''\n",
    "    Calculate the AHI values from a sleep study's metadata file and all the \n",
    "    sleep measurements in a given directory. See the _ahi_for_study function\n",
    "    for an overview of how the metadata file and sleep measurement files should\n",
    "    be structured.\n",
    "\n",
    "    :param sleep_study_metadata_file - the metadata file summarizing the sleep\n",
    "        study\n",
    "    :param sleep_study_root - the root directory containing all the individual\n",
    "        sleep study measures\n",
    "    \n",
    "    :return out_file - the name of the file to which to write the AHI values\n",
    "        in TSV format\n",
    "\n",
    "    '''\n",
    "    metadata_df = pd.read_csv(\n",
    "        sleep_study_metadata_file,\n",
    "        sep=\",\"\n",
    "    )\n",
    "\n",
    "    tsv_files = [\n",
    "        f for f in os.listdir(sleep_study_root)\n",
    "        if f.endswith(\".tsv\")\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        f\"creating AHI from {len(tsv_files)} sleep studies in \"\n",
    "        f\"{sleep_study_root} and outputting the AHI results to {out_file}\"\n",
    "    )\n",
    "\n",
    "    # each tuple is (patient_id, study_id, AHI)\n",
    "    results: list[tuple[str, str, float]] = []\n",
    "    for tsv_file in tsv_files:\n",
    "        filename = os.path.join(sleep_study_root, tsv_file)\n",
    "        pat_id, study_id = _parse_ss_tsv_filename(filename)\n",
    "        sleep_study_df = pd.read_csv(\n",
    "            filename,\n",
    "            sep=\"\\t\",\n",
    "        )\n",
    "        ahi = _ahi_for_study(\n",
    "            metadata_df,\n",
    "            sleep_study_df,\n",
    "            pat_id,\n",
    "            study_id,\n",
    "        )\n",
    "        results.append((pat_id, study_id, ahi))\n",
    "    _write_tsv(out_file, results)\n",
    "\n",
    "\n",
    "def generate_ahi_file(data_root: str, out_file: str) -> None:\n",
    "    sleep_study_metadata_file = os.path.join(\n",
    "        data_root,\n",
    "        \"files\",\n",
    "        \"nch-sleep\",\n",
    "        \"3.1.0\",\n",
    "        \"Health_Data\",\n",
    "        \"SLEEP_STUDY.csv\"\n",
    "    )\n",
    "    sleep_study_root = os.path.join(\n",
    "        data_root,\n",
    "        \"files\",\n",
    "        \"nch-sleep\",\n",
    "        \"3.1.0\",\n",
    "        \"Sleep_Data\"\n",
    "    )\n",
    "\n",
    "    calculate_ahi(\n",
    "        sleep_study_metadata_file,\n",
    "        sleep_study_root,\n",
    "        out_file=sys.argv[2],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "With the AHI values calculated and saved to a TSV file, we can move onto data preprocessing. Roughly speaking, the goals of preprocessing are as follows for each sleep study:\n",
    "\n",
    "- Ignore the study if its AHI is lower than a threshold\n",
    "- Collate and pad as necessary the relevant sleep events in the study\n",
    "- Resample samples from raw data as necessary\n",
    "    - The motivation behind, and method for Resampling was discussed above\n",
    "- Write results to a [compressed numpy representation](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import os.path\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import mne\n",
    "import numpy as np\n",
    "from biosppy.signals.ecg import hamilton_segmenter, correct_rpeaks\n",
    "from biosppy.signals import tools as st\n",
    "from mne import make_fixed_length_events\n",
    "from scipy.interpolate import splev, splrep\n",
    "from itertools import compress\n",
    "\n",
    "mne.set_log_file('log.txt', overwrite=False)\n",
    "\n",
    "########################################## Annotation Modifier functions ##########################################\n",
    "def identity(df):\n",
    "    return df\n",
    "\n",
    "\n",
    "def apnea2bad(df):\n",
    "    df = df.replace(r'.*pnea.*', 'badevent', regex=True)\n",
    "    print(\"bad replaced!\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def wake2bad(df):\n",
    "    return df.replace(\"Sleep stage W\", 'badevent')\n",
    "\n",
    "\n",
    "def change_duration(df, label_dict=POS_EVENT_DICT, duration=CHUNK_DURATION):\n",
    "    for key in label_dict:\n",
    "        df.loc[df.description == key, 'duration'] = duration\n",
    "    print(\"change duration!\")\n",
    "    return df\n",
    "\n",
    "def preprocess(i, annotation_modifier, out_dir, ahi_dict):\n",
    "    is_apnea_available, is_hypopnea_available = True, True\n",
    "    study = ss.data.study_list[i]\n",
    "\n",
    "    # print(f\"loading study {study}\")\n",
    "    raw = ss.data.load_study(study, annotation_modifier, verbose=True)\n",
    "\n",
    "    ########################################   CHECK CRITERIA FOR SS   #################################################\n",
    "    if not all([name in raw.ch_names for name in channels]):\n",
    "        print(\"study \" + str(study) + \" skipped since insufficient channels\", file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "    ahi_value = ahi_dict.get(study,None)\n",
    "    if ahi_value is None:\n",
    "        print(ahi_dict)\n",
    "        print(\"study \" + str(study) + \" skipped since AHI is MISSING.  Is AHI.csv out of date?\", file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "    if ahi_value < THRESHOLD:\n",
    "        print(\"study \" + str(study) + \" skipped since low AHI ---  AHI = \" + str(ahi_value), file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=POS_EVENT_DICT, chunk_duration=1.0,\n",
    "                                                              verbose=None)\n",
    "        # print('|')\n",
    "    except ValueError:\n",
    "        print(\"No Chunk found!\", file=sys.stderr)\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "    ########################################   CHECK CRITERIA FOR SS   #################################################\n",
    "    print(str(i) + \"---\" + str(datetime.now().time().strftime(\"%H:%M:%S\")) + ' --- Processing %d' % i)\n",
    "\n",
    "    try:\n",
    "        apnea_events, event_ids = mne.events_from_annotations(raw, event_id=APNEA_EVENT_DICT, chunk_duration=1.0,\n",
    "                                                              verbose=None)\n",
    "    except ValueError:\n",
    "        is_apnea_available = False\n",
    "\n",
    "    try:\n",
    "        hypopnea_events, event_ids = mne.events_from_annotations(raw, event_id=HYPOPNEA_EVENT_DICT, chunk_duration=1.0,\n",
    "                                                                 verbose=None)\n",
    "    except ValueError:\n",
    "        is_hypopnea_available = False\n",
    "\n",
    "    wake_events, event_ids = mne.events_from_annotations(raw, event_id=WAKE_DICT, chunk_duration=1.0, verbose=None)\n",
    "    ####################################################################################################################\n",
    "    sfreq = raw.info['sfreq']\n",
    "    tmax = CHUNK_DURATION - 1. / sfreq\n",
    "\n",
    "    raw = raw.pick_channels(channels, ordered=True)\n",
    "    fixed_events = make_fixed_length_events(raw, id=0, duration=CHUNK_DURATION, overlap=0.)\n",
    "    epochs = mne.Epochs(raw, fixed_events, event_id=[0], tmin=0, tmax=tmax, baseline=None, preload=True, proj=False, verbose=None)\n",
    "    epochs.load_data()\n",
    "    if sfreq != FREQ:\n",
    "        epochs = epochs.resample(FREQ, npad='auto', n_jobs=4, verbose=None)\n",
    "    data = epochs.get_data()\n",
    "    ####################################################################################################################\n",
    "    if is_apnea_available:\n",
    "        apnea_events_set = set((apnea_events[:, 0] / sfreq).astype(int))\n",
    "    if is_hypopnea_available:\n",
    "        hypopnea_events_set = set((hypopnea_events[:, 0] / sfreq).astype(int))\n",
    "    wake_events_set = set((wake_events[:, 0] / sfreq).astype(int))\n",
    "\n",
    "    starts = (epochs.events[:, 0] / sfreq).astype(int)\n",
    "\n",
    "    labels_apnea = []\n",
    "    labels_hypopnea = []\n",
    "    labels_wake = []\n",
    "    total_apnea_event_second = 0\n",
    "    total_hypopnea_event_second = 0\n",
    "\n",
    "    for seq in range(data.shape[0]):\n",
    "        epoch_set = set(range(starts[seq], starts[seq] + int(CHUNK_DURATION)))\n",
    "        if is_apnea_available:\n",
    "            apnea_seconds = len(apnea_events_set.intersection(epoch_set))\n",
    "            total_apnea_event_second += apnea_seconds\n",
    "            labels_apnea.append(apnea_seconds)\n",
    "        else:\n",
    "            labels_apnea.append(0)\n",
    "\n",
    "        if is_hypopnea_available:\n",
    "            hypopnea_seconds = len(hypopnea_events_set.intersection(epoch_set))\n",
    "            total_hypopnea_event_second += hypopnea_seconds\n",
    "            labels_hypopnea.append(hypopnea_seconds)\n",
    "        else:\n",
    "            labels_hypopnea.append(0)\n",
    "\n",
    "        labels_wake.append(len(wake_events_set.intersection(epoch_set)) == 0)\n",
    "    ####################################################################################################################\n",
    "    print(study + \"    HAMED    \" + str(len(labels_wake) - sum(labels_wake)))\n",
    "    data = data[labels_wake, :, :]\n",
    "    labels_apnea = list(compress(labels_apnea, labels_wake))\n",
    "    labels_hypopnea = list(compress(labels_hypopnea, labels_wake))\n",
    "\n",
    "    out_name = study + \"_\" + str(total_apnea_event_second) + \"_\" + str(total_hypopnea_event_second)\n",
    "    out_path = os.path.join(out_dir, out_name)\n",
    "    # print(f\"Saving {study} to {out_path}.npz\")\n",
    "    np.savez_compressed(out_path, data=data, labels_apnea=labels_apnea, labels_hypopnea=labels_hypopnea)\n",
    "\n",
    "    return data.shape[0]\n",
    "\n",
    "def preprocess(\n",
    "    ahi_path: str,\n",
    "    out_folder: str,\n",
    "    n_studies: int = 3984,\n",
    "    n_workers: int = 3\n",
    "):\n",
    "    ahi = pd.read_csv(ahi_path)\n",
    "    # filename is <patient_id>_<study>\n",
    "    filenames = ahi['PatID'].astype(str) + '_' + ahi['Study'].astype(str)\n",
    "    # ahi_dict = dict(zip(ahi.Study, ahi.AHI))\n",
    "    ahi_dict = dict(zip(filenames, ahi['AHI']))\n",
    "    ss.__init__()\n",
    "\n",
    "    if not os.path.exists(out_folder):\n",
    "        os.mkdir(out_folder)\n",
    "\n",
    "    if n_workers < 2:\n",
    "        for idx in range(n_studies):\n",
    "            preprocess(\n",
    "                ahi_path=idx,\n",
    "                out_folder=identity,\n",
    "                n_studies=out_folder,\n",
    "                n_workers=ahi_dict\n",
    "            )\n",
    "    else:\n",
    "        with concurrent.futures.ThreadPoolExecutor(\n",
    "            max_workers=n_workers\n",
    "        ) as executor:\n",
    "            executor.map(\n",
    "                preprocess,\n",
    "                range(n_studies),\n",
    "                [identity] * n_studies,\n",
    "                [out_folder] * n_studies,\n",
    "                [ahi_dict] * n_studies\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock code\n",
    "\n",
    "The following code already existed in the notebook for loading data. It should be adapted to work with both CHAT and NCH datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZScZNbROw-N"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import sys\n",
    "from sys import path\n",
    "\n",
    "# # dir and function to load raw data\n",
    "# raw_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
    "# \n",
    "# \n",
    "# def load_raw_data(raw_data_dir):\n",
    "#     # implement this function to load raw data to dataframe/numpy array/tensor\n",
    "#     return None\n",
    "# \n",
    "# \n",
    "# raw_data = load_raw_data(raw_data_dir)\n",
    "\n",
    "\n",
    "# calculate statistics\n",
    "def calculate_stats(raw_data):\n",
    "    # implement this function to calculate the statistics\n",
    "    # it is encouraged to print out the results\n",
    "    return None\n",
    "\n",
    "\n",
    "# process raw data\n",
    "def process_data(raw_data):\n",
    "    # implement this function to process the data as you need\n",
    "    return None\n",
    "\n",
    "# this doesn't work\n",
    "# processed_data = process_data(raw_data)\n",
    "# processed_data_url = 'https://drive.google.com/file/d/1ZtjpMSfFneHzNakfwEw_XlU1IQijrMqJ/view?usp=share_link'\n",
    "# processed_data_path = './nch_30x64.npz'\n",
    "# def load_processed_data(path):\n",
    "#     if path.\n",
    "#     files.download(processed_data_url)\n",
    "''' you can load the processed data directly\n",
    "processed_data_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-raw-data>'\n",
    "def load_processed_data(raw_data_dir):\n",
    "  pass\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "#   Model\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "  * Model architecture: layer number/size/type, activation function, etc\n",
    "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
    "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
    "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
    "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "The below is from [`./models/transformer.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/models/transformer.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _tf_keras\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/_tf_keras/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/_tf_keras/keras/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/activations/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/activations/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gelu\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/activations/activations.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/backend/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m any_symbolic_tensors\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_keras_tensor\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/backend/common/keras_tensor.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.KerasTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKerasTensor\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dlh/lib/python3.12/site-packages/keras/src/utils/tree.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_structures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[1;32m     14\u001b[0m     optree\u001b[38;5;241m.\u001b[39mregister_pytree_node(\n\u001b[1;32m     15\u001b[0m         ListWrapper,\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: (x, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m metadata, children: ListWrapper(\u001b[38;5;28mlist\u001b[39m(children)),\n\u001b[1;32m     18\u001b[0m         namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.tree.is_nested\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_nested\u001b[39m(structure):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "class Patches(Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, input):\n",
    "        input = input[:, tf.newaxis, :, :]\n",
    "        batch_size = tf.shape(input)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=input,\n",
    "            sizes=[1, 1, self.patch_size, 1],\n",
    "            strides=[1, 1, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(\n",
    "            patches,\n",
    "            [batch_size, -1, patch_dims]\n",
    "        )\n",
    "        return patches\n",
    "\n",
    "\n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches, projection_dim, l2_weight):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.projection_dim = projection_dim\n",
    "        self.l2_weight = l2_weight\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(\n",
    "            units=projection_dim,\n",
    "            kernel_regularizer=L2(l2_weight),\n",
    "            bias_regularizer=L2(l2_weight)\n",
    "        )\n",
    "        self.position_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch)  # + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate, l2_weight):\n",
    "    for _, units in enumerate(hidden_units):\n",
    "        x = Dense(\n",
    "            units=units,\n",
    "            activation=None,\n",
    "            kernel_regularizer=L2(l2_weight),\n",
    "            bias_regularizer=L2(l2_weight)\n",
    "        )(x)\n",
    "        x = tf.nn.gelu(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_transformer_model(\n",
    "        input_shape,\n",
    "        num_patches,\n",
    "        projection_dim,\n",
    "        transformer_layers: int,\n",
    "        num_heads,\n",
    "        transformer_units,\n",
    "        mlp_head_units,\n",
    "        num_classes,\n",
    "        drop_out,\n",
    "        reg,\n",
    "        l2_weight,\n",
    "        demographic=False,\n",
    "):\n",
    "    if reg:\n",
    "        activation = None\n",
    "    else:\n",
    "        activation = 'sigmoid'\n",
    "    inputs = Input(shape=input_shape)\n",
    "    patch_size = input_shape[0] / num_patches\n",
    "    if demographic:\n",
    "        normalized_inputs = tfa.layers.InstanceNormalization(\n",
    "            axis=-1,\n",
    "            epsilon=1e-6,\n",
    "            center=False,\n",
    "            scale=False,\n",
    "            beta_initializer=\"glorot_uniform\",\n",
    "            gamma_initializer=\"glorot_uniform\"\n",
    "        )(inputs[:, :, :-1])\n",
    "        demo = inputs[:, :12, -1]\n",
    "\n",
    "    else:\n",
    "        normalized_inputs = tfa.layers.InstanceNormalization(\n",
    "            axis=-1,\n",
    "            epsilon=1e-6,\n",
    "            center=False,\n",
    "            scale=False,\n",
    "            beta_initializer=\"glorot_uniform\",\n",
    "            gamma_initializer=\"glorot_uniform\",\n",
    "        )(inputs)\n",
    "\n",
    "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
    "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
    "    encoded_patches = PatchEncoder(\n",
    "        num_patches=num_patches,\n",
    "        projection_dim=projection_dim,\n",
    "        l2_weight=l2_weight\n",
    "    )(patches)\n",
    "\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = encoded_patches  # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=projection_dim,\n",
    "            dropout=drop_out,\n",
    "            kernel_regularizer=L2(l2_weight),  # i *\n",
    "            bias_regularizer=L2(l2_weight)\n",
    "        )(x1, x1)\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(\n",
    "            x3,\n",
    "            transformer_units,\n",
    "            drop_out,\n",
    "            l2_weight\n",
    "        )  # i *\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    #x = Concatenate()([x, demo])\n",
    "    features = mlp(\n",
    "        x,\n",
    "        mlp_head_units,\n",
    "        0.0,\n",
    "        l2_weight\n",
    "    )\n",
    "\n",
    "    logits = Dense(\n",
    "        num_classes,\n",
    "        kernel_regularizer=L2(l2_weight),\n",
    "        bias_regularizer=L2(l2_weight),\n",
    "        activation=activation\n",
    "    )(features)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "\n",
    "def create_hybrid_transformer_model(input_shape):\n",
    "    transformer_units = [32, 32]\n",
    "    transformer_layers = 2\n",
    "    num_heads = 4\n",
    "    l2_weight = 0.001\n",
    "    drop_out = 0.25\n",
    "    mlp_head_units = [256, 128]\n",
    "    num_patches = 30\n",
    "    projection_dim = 32\n",
    "\n",
    "    # Conv1D(32...\n",
    "    input1 = Input(shape=input_shape)\n",
    "    conv11 = Conv1D(16, 256)(input1)  #13\n",
    "    conv12 = Conv1D(16, 256)(input1)  #13\n",
    "    conv13 = Conv1D(16, 256)(input1)  #13\n",
    "\n",
    "    pwconv1 = SeparableConvolution1D(32, 1)(input1)\n",
    "    pwconv2 = SeparableConvolution1D(32, 1)(pwconv1)\n",
    "\n",
    "    conv21 = Conv1D(16, 256)(conv11)  # 7\n",
    "    conv22 = Conv1D(16, 256)(conv12)  # 7\n",
    "    conv23 = Conv1D(16, 256)(conv13)  # 7\n",
    "\n",
    "    concat = keras.layers.concatenate([conv21, conv22, conv23], axis=-1)\n",
    "    concat = Dense(64, activation=relu)(concat)  #192\n",
    "    concat = Dense(64, activation=sigmoid)(concat)  #192\n",
    "    concat = SeparableConvolution1D(32, 1)(concat)\n",
    "    concat = keras.layers.concatenate([concat, pwconv2], axis=1)\n",
    "\n",
    "    ####################################################################################################################\n",
    "    patch_size = input_shape[0] / num_patches\n",
    "\n",
    "    normalized_inputs = tfa.layers.InstanceNormalization(\n",
    "        axis=-1,\n",
    "        epsilon=1e-6,\n",
    "        center=False,\n",
    "        scale=False,\n",
    "        beta_initializer=\"glorot_uniform\",\n",
    "        gamma_initializer=\"glorot_uniform\"\n",
    "    )(concat)\n",
    "\n",
    "    # patches = Reshape((num_patches, -1))(normalized_inputs)\n",
    "    patches = Patches(patch_size=patch_size)(normalized_inputs)\n",
    "    encoded_patches = PatchEncoder(\n",
    "        num_patches=num_patches,\n",
    "        projection_dim=projection_dim,\n",
    "        l2_weight=l2_weight\n",
    "    )(patches)\n",
    "    for i in range(transformer_layers):\n",
    "        x1 = encoded_patches  # LayerNormalization(epsilon=1e-6)(encoded_patches) # TODO\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=projection_dim,\n",
    "            dropout=drop_out,\n",
    "            kernel_regularizer=L2(l2_weight),  # i *\n",
    "            bias_regularizer=L2(l2_weight)\n",
    "        )(x1, x1)\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        x3 = mlp(\n",
    "            x3,\n",
    "            transformer_units,\n",
    "            drop_out,\n",
    "            l2_weight\n",
    "        )  # i *\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    #x = Concatenate()([x, demo])\n",
    "    features = mlp(x, mlp_head_units, 0.0, l2_weight)\n",
    "\n",
    "    logits = Dense(\n",
    "        1,\n",
    "        kernel_regularizer=L2(l2_weight),\n",
    "        bias_regularizer=L2(l2_weight),\n",
    "        activation='sigmoid'\n",
    "    )(features)\n",
    "\n",
    "    ####################################################################################################################\n",
    "\n",
    "    model = Model(inputs=input1, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core model\n",
    "\n",
    "The following is from [`./models/models.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/models/models.py) in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    for i in range(5):  # 10\n",
    "        model.add(Conv1D(45, 32, padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(relu))\n",
    "        model.add(MaxPooling1D())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    for i in range(2):  #4\n",
    "        model.add(Dense(512))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(relu))\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnnlstm_model(input_a_shape, weight=1e-3):\n",
    "    cnn_filters = 32  # 128\n",
    "    cnn_kernel_size = 4  # 4\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    input1 = tfa.layers.InstanceNormalization(\n",
    "        axis=-1,\n",
    "        epsilon=1e-6,\n",
    "        center=False,\n",
    "        scale=False,\n",
    "        beta_initializer=\"glorot_uniform\",\n",
    "        gamma_initializer=\"glorot_uniform\"\n",
    "    )(input1)\n",
    "    x1 = Conv1D(\n",
    "        cnn_filters,\n",
    "        cnn_kernel_size,\n",
    "        activation='relu'\n",
    "    )(input1)\n",
    "    x1 = Conv1D(\n",
    "        cnn_filters,\n",
    "        cnn_kernel_size,\n",
    "        activation='relu'\n",
    "    )(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(\n",
    "        cnn_filters,\n",
    "        cnn_kernel_size,\n",
    "        activation='relu'\n",
    "    )(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(\n",
    "        cnn_filters,\n",
    "        cnn_kernel_size,\n",
    "        activation='relu'\n",
    "    )(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = LSTM(32, return_sequences=True)(x1)  #256\n",
    "    x1 = LSTM(32, return_sequences=True)(x1)  #256\n",
    "    x1 = LSTM(32)(x1)  #256\n",
    "    x1 = Flatten()(x1)\n",
    "\n",
    "    x1 = Dense(32, activation='relu')(x1)  #64\n",
    "    x1 = Dense(32, activation='relu')(x1)  #64\n",
    "    outputs = Dense(1, activation='sigmoid')(x1)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_semscnn_model(input_a_shape):\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    # input1 = tfa.layers.InstanceNormalization(axis=-1, epsilon=1e-6, center=False, scale=False,\n",
    "    #                                           beta_initializer=\"glorot_uniform\",\n",
    "    #                                           gamma_initializer=\"glorot_uniform\")(input1)\n",
    "    x1 = Conv1D(45, 32, strides=1)(input1)  #kernel_size=11\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1)  #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1)  #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    x1 = Conv1D(45, 32, strides=2)(x1)  #64 kernel_size=11\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = Activation(relu)(x1)\n",
    "    x1 = MaxPooling1D()(x1)\n",
    "\n",
    "    squeeze = Flatten()(x1)\n",
    "    excitation = Dense(128, activation='relu')(squeeze)\n",
    "    excitation = Dense(64, activation='relu')(excitation)\n",
    "    logits = Dense(1, activation='sigmoid')(excitation)\n",
    "    model = Model(inputs=input1, outputs=logits)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "\n",
    "    \"cnn\": create_cnn_model((60 * 32, 3)),\n",
    "    \"sem-mscnn\": create_semscnn_model((60 * 32, 3)),\n",
    "    \"cnn-lstm\": create_cnnlstm_model((60 * 32, 3)),\n",
    "    \"hybrid\": create_hybrid_transformer_model((60 * 32, 3)),\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(config):\n",
    "    if config[\"model_name\"].split('_')[0] == \"Transformer\":\n",
    "        return create_transformer_model(\n",
    "            input_shape=(60 * 32, len(config[\"channels\"])),\n",
    "            num_patches=config[\"num_patches\"],\n",
    "            projection_dim=config[\"transformer_units\"],\n",
    "            transformer_layers=config[\"transformer_layers\"],\n",
    "            num_heads=config[\"num_heads\"],\n",
    "            transformer_units=[\n",
    "                config[\"transformer_units\"] * 2,\n",
    "                config[\"transformer_units\"]\n",
    "            ],\n",
    "            mlp_head_units=[256, 128],\n",
    "            num_classes=1,\n",
    "            drop_out=config[\"drop_out_rate\"],\n",
    "            reg=config[\"regression\"],\n",
    "            l2_weight=config[\"regularization_weight\"]\n",
    "        )\n",
    "    else:\n",
    "        return model_dict.get(config[\"model_name\"].split('_')[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "The below trains the model using the data prepared previously. This code taken from [`./main/train.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 1\n",
    "FOLD = 5\n",
    "\n",
    "\n",
    "def lr_schedule(epoch: int, lr: float) -> float:\n",
    "    if epoch > 50 and (epoch - 1) % 5 == 0:\n",
    "        lr *= 0.5\n",
    "    return lr\n",
    "\n",
    "\n",
    "def train(config, fold=None):\n",
    "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
    "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
    "    y = y_apnea + y_hypopnea\n",
    "    ########################################################################################\n",
    "    for i in range(FOLD):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        if config[\"regression\"]:\n",
    "            y[i] = np.sqrt(y[i])\n",
    "            y[i][y[i] != 0] += 2\n",
    "        else:\n",
    "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]  # CHANNEL SELECTION\n",
    "\n",
    "    ########################################################################################\n",
    "    folds = range(FOLD) if fold is None else [fold]\n",
    "    for fold in folds:\n",
    "        first = True\n",
    "        for i in range(5):\n",
    "            if i != fold:\n",
    "                if first:\n",
    "                    x_train = x[i]\n",
    "                    y_train = y[i]\n",
    "                    first = False\n",
    "                else:\n",
    "                    x_train = np.concatenate((x_train, x[i]))\n",
    "                    y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "        model = get_model(config)\n",
    "        if config[\"regression\"]:\n",
    "            model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
    "            early_stopper = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            model.compile(\n",
    "                optimizer=\"adam\",\n",
    "                loss=BinaryCrossentropy(),\n",
    "                metrics=[\n",
    "                    keras.metrics.Precision(),\n",
    "                    keras.metrics.Recall()\n",
    "                ],\n",
    "            )\n",
    "            early_stopper = EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "        lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=512,\n",
    "            epochs=config[\"epochs\"],\n",
    "            validation_split=0.1,\n",
    "            callbacks=[early_stopper, lr_scheduler]\n",
    "        )\n",
    "        ################################################################################################################\n",
    "        model.save(config[\"model_path\"] + str(fold))\n",
    "        keras.backend.clear_session()\n",
    "\n",
    "\n",
    "def train_age_seperated(config):\n",
    "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
    "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
    "    y = y_apnea + y_hypopnea\n",
    "    ########################################################################################\n",
    "    for i in range(10):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        if config[\"regression\"]:\n",
    "            y[i] = np.sqrt(y[i])\n",
    "            y[i][y[i] != 0] += 2\n",
    "        else:\n",
    "            y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]  # CHANNEL SELECTION\n",
    "\n",
    "    ########################################################################################\n",
    "    first = True\n",
    "    for i in range(10):\n",
    "        if first:\n",
    "            x_train = x[i]\n",
    "            y_train = y[i]\n",
    "            first = False\n",
    "        else:\n",
    "            x_train = np.concatenate((x_train, x[i]))\n",
    "            y_train = np.concatenate((y_train, y[i]))\n",
    "\n",
    "    model = get_model(config)\n",
    "    if config[\"regression\"]:\n",
    "        model.compile(optimizer=\"adam\", loss=BinaryCrossentropy())\n",
    "        early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    else:\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=BinaryCrossentropy(),\n",
    "            metrics=[keras.metrics.Precision(), keras.metrics.Recall()]\n",
    "        )\n",
    "        early_stopper = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=512,\n",
    "        epochs=config[\"epochs\"],\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopper, lr_scheduler]\n",
    "    )\n",
    "    ################################################################################################################\n",
    "    model.save(config[\"model_path\"] + str(0))\n",
    "    keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results\n",
    "\n",
    ">NOTE: We trained the model previously, now going to evaluate it\n",
    "\n",
    "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
    "\n",
    "Please test and report results for all experiments that you run with:\n",
    "\n",
    "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
    "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n",
    "\n",
    "See the following files for testing and evaluation:\n",
    "\n",
    "- https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/test.py\n",
    "- https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "The below is from [`./test.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/test.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "THRESHOLD = 1\n",
    "FOLD = 5\n",
    "\n",
    "\n",
    "def test(config, fold=None):\n",
    "    data = np.load(config[\"data_path\"], allow_pickle=True)\n",
    "    ############################################################################\n",
    "    x, y_apnea, y_hypopnea = data['x'], data['y_apnea'], data['y_hypopnea']\n",
    "    y = y_apnea + y_hypopnea\n",
    "    for i in range(FOLD):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]\n",
    "    ############################################################################\n",
    "    result = Result()\n",
    "    folds = range(FOLD) if fold is None else [fold]\n",
    "    for fold in folds:\n",
    "        x_test = x[fold]\n",
    "        if config.get(\"test_noise_snr\"):\n",
    "            x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
    "\n",
    "        y_test = y[fold]  # For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\n",
    "\n",
    "        model = tf.keras.models.load_model(\n",
    "            config[\"model_path\"] + str(fold),\n",
    "            compile=False\n",
    "        )\n",
    "\n",
    "        predict = model.predict(x_test)\n",
    "        y_score = predict\n",
    "        y_predict = np.where(predict > 0.5, 1, 0)  # For MultiClass np.argmax(y_score, axis=-1)\n",
    "\n",
    "        result.add(y_test, y_predict, y_score)\n",
    "\n",
    "    result.print()\n",
    "    result.save(\"./results/\" + config[\"model_name\"] + \".txt\", config)\n",
    "\n",
    "    del data, x_test, y_test, model, predict, y_score, y_predict\n",
    "\n",
    "\n",
    "def test_age_seperated(config):\n",
    "    x = []\n",
    "    y_apnea = []\n",
    "    y_hypopnea = []\n",
    "    for i in range(10):\n",
    "        data = np.load(\n",
    "            config[\"data_path\"] + str(i) + \".npz\",\n",
    "            allow_pickle=True\n",
    "        )\n",
    "        x.append(data['x'])\n",
    "        y_apnea.append(data['y_apnea'])\n",
    "        y_hypopnea.append(data['y_hypopnea'])\n",
    "    ############################################################################\n",
    "    y = np.array(y_apnea) + np.array(y_hypopnea)\n",
    "    for i in range(10):\n",
    "        x[i], y[i] = shuffle(x[i], y[i])\n",
    "        x[i] = np.nan_to_num(x[i], nan=-1)\n",
    "        y[i] = np.where(y[i] >= THRESHOLD, 1, 0)\n",
    "        x[i] = x[i][:, :, config[\"channels\"]]\n",
    "    ############################################################################\n",
    "    result = Result()\n",
    "\n",
    "    for fold in range(10):\n",
    "        x_test = x[fold]\n",
    "        if config.get(\"test_noise_snr\"):\n",
    "            x_test = add_noise_to_data(x_test, config[\"test_noise_snr\"])\n",
    "\n",
    "        y_test = y[fold]  # For MultiClass keras.utils.to_categorical(y[fold], num_classes=2)\n",
    "\n",
    "        model = tf.keras.models.load_model(\n",
    "            config[\"model_path\"] + str(0),\n",
    "            compile=False\n",
    "        )\n",
    "\n",
    "        predict = model.predict(x_test)\n",
    "        y_score = predict\n",
    "        y_predict = np.where(predict > 0.5, 1, 0)  # For MultiClass np.argmax(y_score, axis=-1)\n",
    "\n",
    "        result.add(y_test, y_predict, y_score)\n",
    "\n",
    "    result.print()\n",
    "    result.save(\"./results/\" + config[\"model_name\"] + \".txt\", config)\n",
    "\n",
    "    del data, x_test, y_test, model, predict, y_score, y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "The below is from [`./metrics.py`](https://github.com/healthylaife/Pediatric-Apnea-Detection/blob/main/metrics.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FromLogitsMixin:\n",
    "    def __init__(self, from_logits=False, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        if self.from_logits:\n",
    "            y_pred = tf.nn.sigmoid(y_pred)\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "\n",
    "class AUC(FromLogitsMixin, tf.metrics.AUC):\n",
    "    ...\n",
    "\n",
    "\n",
    "class BinaryAccuracy(FromLogitsMixin, tf.metrics.BinaryAccuracy):\n",
    "    ...\n",
    "\n",
    "\n",
    "class TruePositives(FromLogitsMixin, tf.metrics.TruePositives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class FalsePositives(FromLogitsMixin, tf.metrics.FalsePositives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class TrueNegatives(FromLogitsMixin, tf.metrics.TrueNegatives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class FalseNegatives(FromLogitsMixin, tf.metrics.FalseNegatives):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Precision(FromLogitsMixin, tf.metrics.Precision):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Recall(FromLogitsMixin, tf.metrics.Recall):\n",
    "    ...\n",
    "\n",
    "\n",
    "class F1Score(FromLogitsMixin, tfa.metrics.F1Score):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Result:\n",
    "    def __init__(self):\n",
    "        self.accuracy_list = []\n",
    "        self.sensitivity_list = []\n",
    "        self.specificity_list = []\n",
    "        self.f1_list = []\n",
    "        self.auroc_list = []\n",
    "        self.auprc_list = []\n",
    "        self.precision_list = []\n",
    "\n",
    "    def add(self, y_test, y_predict, y_score):\n",
    "        C = confusion_matrix(y_test, y_predict, labels=(1, 0))\n",
    "        TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "\n",
    "        acc, sn, sp, pr = (\n",
    "            1. * (TP + TN) / (TP + TN + FP + FN),\n",
    "            1. * TP / (TP + FN),\n",
    "            1. * TN / (TN + FP),\n",
    "            1. * TP / (TP + FP)\n",
    "        )\n",
    "        f1 = f1_score(y_test, y_predict)\n",
    "        auc = roc_auc_score(y_test, y_score)\n",
    "        auprc = average_precision_score(y_test, y_score)\n",
    "\n",
    "        self.accuracy_list.append(acc * 100)\n",
    "        self.precision_list.append(pr * 100)\n",
    "        self.sensitivity_list.append(sn * 100)\n",
    "        self.specificity_list.append(sp * 100)\n",
    "        self.f1_list.append(f1 * 100)\n",
    "        self.auroc_list.append(auc * 100)\n",
    "        self.auprc_list.append(auprc * 100)\n",
    "\n",
    "    def get(self):\n",
    "        out_str = \"=========================================================================== \\n\"\n",
    "        out_str += str(self.accuracy_list) + \" \\n\"\n",
    "        out_str += str(self.precision_list) + \" \\n\"\n",
    "        out_str += str(self.sensitivity_list) + \" \\n\"\n",
    "        out_str += str(self.specificity_list) + \" \\n\"\n",
    "        out_str += str(self.f1_list) + \" \\n\"\n",
    "        out_str += str(self.auroc_list) + \" \\n\"\n",
    "        out_str += str(self.auprc_list) + \" \\n\"\n",
    "        out_str += str(\"Accuracy: %.2f -+ %.3f\" % (\n",
    "            np.mean(self.accuracy_list),\n",
    "            np.std(self.accuracy_list)\n",
    "        )) + \" \\n\"\n",
    "        out_str += str(\"Precision: %.2f -+ %.3f\" % (\n",
    "            np.mean(self.precision_list),\n",
    "            np.std(self.precision_list)\n",
    "        )) + \" \\n\"\n",
    "        out_str += str(\n",
    "            \"Recall: %.2f -+ %.3f\" % (\n",
    "                np.mean(self.sensitivity_list),\n",
    "                np.std(self.sensitivity_list)\n",
    "            )\n",
    "        ) + \" \\n\"\n",
    "        out_str += str(\n",
    "            \"Specifity: %.2f -+ %.3f\" % (\n",
    "                np.mean(self.specificity_list),\n",
    "                np.std(self.specificity_list)\n",
    "            )\n",
    "        ) + \" \\n\"\n",
    "        out_str += str(\"F1: %.2f -+ %.3f\" % (\n",
    "            np.mean(self.f1_list),\n",
    "            np.std(self.f1_list)\n",
    "        )) + \" \\n\"\n",
    "        out_str += str(\"AUROC: %.2f -+ %.3f\" % (\n",
    "            np.mean(self.auroc_list),\n",
    "            np.std(self.auroc_list)\n",
    "        )) + \" \\n\"\n",
    "        out_str += str(\"AUPRC: %.2f -+ %.3f\" % (\n",
    "            np.mean(self.auprc_list),\n",
    "            np.std(self.auprc_list)\n",
    "        )) + \" \\n\"\n",
    "\n",
    "        out_str += str(\"$ %.1f \\pm %.1f$\" % (\n",
    "            np.mean(self.accuracy_list),\n",
    "            np.std(self.accuracy_list)\n",
    "        )) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (\n",
    "            np.mean(self.precision_list),\n",
    "            np.std(self.precision_list)\n",
    "        )) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (\n",
    "            np.mean(self.sensitivity_list),\n",
    "            np.std(self.sensitivity_list)\n",
    "        )) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (\n",
    "            np.mean(self.f1_list),\n",
    "            np.std(self.f1_list)\n",
    "        )) + \"& \"\n",
    "        out_str += str(\"$%.1f \\pm %.1f$\" % (\n",
    "            np.mean(self.auroc_list)\n",
    "            np.std(self.auroc_list)\n",
    "                       )) + \"& \"\n",
    "\n",
    "        return out_str\n",
    "\n",
    "    def print(self):\n",
    "        print(self.get())\n",
    "\n",
    "    def save(self, path, config):\n",
    "        file = open(path, \"w+\")\n",
    "        file.write(str(config))\n",
    "        file.write(\"\\n\")\n",
    "        file.write(self.get())\n",
    "        file.flush()\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "outputs": [],
   "source": [
    "# metrics to evaluate my model\n",
    "\n",
    "# plot figures to better show the results\n",
    "\n",
    "# it is better to save the numbers and figures for your presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "outputs": [],
   "source": [
    "# compare you model with others\n",
    "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
    "  * Make assessment that the paper is reproducible or not.\n",
    "  * Explain why it is not reproducible if your results are kind negative.\n",
    "  * Describe â€œWhat was easyâ€ and â€œWhat was difficultâ€ during the reproduction.\n",
    "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
    "  * What will you do in next phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2VDXo5F4Frm"
   },
   "outputs": [],
   "source": [
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanation,\n",
    "you can read and plot it here like the Scope of Reproducibility\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmVuzQ724HbO"
   },
   "source": [
    "# Feel free to add new sections"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1oAKqszNlwEZwPa_BjHPqfcoWlikYBpi5",
     "timestamp": 1709153069464
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
